{
  "cells": [
    {
      "metadata": {
        "id": "84973f3b9d53059"
      },
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 13: LangChain\n",
        "\n",
        "LangChain es un _framework_ de código abierto diseñado para facilitar el desarrollo de aplicaciones que combinan modelos de lenguaje LLMs con datos, herramientas externas y memoria. Está especialmente pensado para construir aplicaciones complejas basadas en IA, como sistemas _Retrieval-Augmented Generation_, asistentes conversacionales inteligentes, agentes autónomos y sistemas con razonamiento compuesto."
      ],
      "id": "84973f3b9d53059"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nombre: Ramirez Mishel"
      ],
      "metadata": {
        "id": "0iy2OdA0Xt0O"
      },
      "id": "0iy2OdA0Xt0O"
    },
    {
      "metadata": {
        "id": "17e0e6d4008e622"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 1: Carga y preprocesamiento del corpus"
      ],
      "id": "17e0e6d4008e622"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv kaggle pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suBeoOeRMAuF",
        "outputId": "dc5767d3-c394-403e-fe5e-663c7739cb6d"
      },
      "id": "suBeoOeRMAuF",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mTqFhvQXG9G",
        "outputId": "979f4edf-f3e8-491f-881f-8ac381a85bf0"
      },
      "id": "6mTqFhvQXG9G",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.70)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI0tAbGOj63f",
        "outputId": "3873ebce-ef2e-4481-b80f-2ce5e8b4f1c5"
      },
      "id": "DI0tAbGOj63f",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d839I6Thj59-",
        "outputId": "246e3896-6167-4fd6-9152-da5435c2cf28"
      },
      "id": "d839I6Thj59-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.70)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "775ff0fb0fd1407f9c6b9cd8d4e103d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload the kaggle.json file\")\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "CIw710OCUdnB",
        "outputId": "b9cd5566-224a-4a40-e43b-546fbccbede8"
      },
      "id": "CIw710OCUdnB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the kaggle.json file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-abfc2fd9-e0f5-480c-802e-e93d86078c42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-abfc2fd9-e0f5-480c-802e-e93d86078c42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\\r\\n    \"KAGGLE_USERNAME\":\"mishelramirez19\",\\r\\n    \"KAGGLE_KEY\":\"1fbbe5ea869de9c07b70b52449076f88\",\\r\\n    \"GOOGLE_API_KEY\":\"AIzaSyBIer5mRMuoV1Du9bDJzmSxkKghnHO9Yf0\"       \\r\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Crear el directorio para la autenticación de Kaggle\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Mover el archivo `kaggle.json` al directorio adecuado\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "\n",
        "# Permisos del archivo\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "k3cHsbktP2Sx"
      },
      "id": "k3cHsbktP2Sx",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7779ec483e8993e1",
        "outputId": "3412c707-7fcc-49d7-8c6d-7d06f7b19475"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import kaggle\n",
        "import pandas as pd\n",
        "\n",
        "# Leer las credenciales desde el archivo\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"r\") as f:\n",
        "    kaggle_creds = json.load(f)\n",
        "\n",
        "# Establecer las variables de entorno\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_creds[\"KAGGLE_USERNAME\"]\n",
        "os.environ[\"KAGGLE_KEY\"] = kaggle_creds[\"KAGGLE_KEY\"]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = kaggle_creds[\"GOOGLE_API_KEY\"]\n",
        "\n",
        "# Descargar el dataset\n",
        "dataset = \"rajneesh231/lex-fridman-podcast-transcript\"\n",
        "path = \"../data/13langchain\"\n",
        "\n",
        "kaggle.api.dataset_download_files(dataset, path=path, unzip=True)"
      ],
      "id": "7779ec483e8993e1",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-23T16:15:07.098935Z",
          "start_time": "2025-07-23T16:15:06.650753Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "d386c15c68427b6e",
        "outputId": "a61b10e4-552f-4133-ddd4-4e7c45a6780d"
      },
      "cell_type": "code",
      "source": [
        "# Cargar el dataset CSV\n",
        "file_path = os.path.join(path, \"podcastdata_dataset.csv\")\n",
        "df = pd.read_csv(file_path)\n",
        "df"
      ],
      "id": "d386c15c68427b6e",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id             guest                                              title  \\\n",
              "0      1       Max Tegmark                                           Life 3.0   \n",
              "1      2     Christof Koch                                      Consciousness   \n",
              "2      3     Steven Pinker                            AI in the Age of Reason   \n",
              "3      4     Yoshua Bengio                                      Deep Learning   \n",
              "4      5   Vladimir Vapnik                               Statistical Learning   \n",
              "..   ...               ...                                                ...   \n",
              "314  321      Ray Kurzweil    Singularity, Superintelligence, and Immortality   \n",
              "315  322  Rana el Kaliouby   Emotion AI, Social Robots, and Self-Driving Cars   \n",
              "316  323        Will Sasso  Comedy, MADtv, AI, Friendship, Madness, and Pr...   \n",
              "317  324   Daniel Negreanu                                              Poker   \n",
              "318  325     Michael Levin  Biology, Life, Aliens, Evolution, Embryogenesi...   \n",
              "\n",
              "                                                  text  \n",
              "0    As part of MIT course 6S099, Artificial Genera...  \n",
              "1    As part of MIT course 6S099 on artificial gene...  \n",
              "2    You've studied the human mind, cognition, lang...  \n",
              "3    What difference between biological neural netw...  \n",
              "4    The following is a conversation with Vladimir ...  \n",
              "..                                                 ...  \n",
              "314  By the time he gets to 2045, we'll be able to ...  \n",
              "315  there's a broader question here, right? As we ...  \n",
              "316  Once this whole thing falls apart and we are c...  \n",
              "317  you could be the seventh best player in the wh...  \n",
              "318  turns out that if you train a planarian and th...  \n",
              "\n",
              "[319 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a3a69e4-4b1c-418a-9096-33eb57477f7f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>guest</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Max Tegmark</td>\n",
              "      <td>Life 3.0</td>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Christof Koch</td>\n",
              "      <td>Consciousness</td>\n",
              "      <td>As part of MIT course 6S099 on artificial gene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Steven Pinker</td>\n",
              "      <td>AI in the Age of Reason</td>\n",
              "      <td>You've studied the human mind, cognition, lang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Yoshua Bengio</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>What difference between biological neural netw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Vladimir Vapnik</td>\n",
              "      <td>Statistical Learning</td>\n",
              "      <td>The following is a conversation with Vladimir ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>321</td>\n",
              "      <td>Ray Kurzweil</td>\n",
              "      <td>Singularity, Superintelligence, and Immortality</td>\n",
              "      <td>By the time he gets to 2045, we'll be able to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>322</td>\n",
              "      <td>Rana el Kaliouby</td>\n",
              "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
              "      <td>there's a broader question here, right? As we ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>323</td>\n",
              "      <td>Will Sasso</td>\n",
              "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
              "      <td>Once this whole thing falls apart and we are c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>324</td>\n",
              "      <td>Daniel Negreanu</td>\n",
              "      <td>Poker</td>\n",
              "      <td>you could be the seventh best player in the wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>325</td>\n",
              "      <td>Michael Levin</td>\n",
              "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
              "      <td>turns out that if you train a planarian and th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>319 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a3a69e4-4b1c-418a-9096-33eb57477f7f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a3a69e4-4b1c-418a-9096-33eb57477f7f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a3a69e4-4b1c-418a-9096-33eb57477f7f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fa67d365-fffe-4b59-a3b9-1405005ddd9e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa67d365-fffe-4b59-a3b9-1405005ddd9e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fa67d365-fffe-4b59-a3b9-1405005ddd9e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5489e6ff-51d3-4478-a163-58e0154ef85d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5489e6ff-51d3-4478-a163-58e0154ef85d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 319,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 93,\n        \"min\": 1,\n        \"max\": 325,\n        \"num_unique_values\": 318,\n        \"samples\": [\n          74,\n          281,\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"guest\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 281,\n        \"samples\": [\n          \"Keoki Jackson\",\n          \"Sergey Nazarov\",\n          \"Dan Reynolds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 317,\n        \"samples\": [\n          \"Deep Learning, Education, and Real-World AI\",\n          \"Bad Vegan\",\n          \"Thousand Brains Theory of Intelligence\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 318,\n        \"samples\": [\n          \"The following is a conversation with Michael I. Jordan, a professor at Berkeley and one of the most influential people in the history of machine learning, statistics, and artificial intelligence. He has been cited over 170,000 times and he has mentored many of the world class researchers defining the field of AI today, including Andrew Ng, Zubin Garamani, Ben Taskar, and Yoshua Bengio. All this, to me, is as impressive as the over 32,000 points in the six NBA championships of the Michael J. Jordan of basketball fame. There's a nonzero probability that I talked to the other Michael Jordan given my connection to and love of the Chicago Bulls of the 90s, but if I had to pick one, I'm going with the Michael Jordan of statistics and computer science, or as Yann LeCun calls him, the Miles Davis of machine learning. In his blog post titled Artificial Intelligence, the Revolution Hasn't Happened Yet, Michael argues for broadening the scope of the artificial intelligence field. In many ways, the underlying spirit of this podcast is the same, to see artificial intelligence as a deeply human endeavor, to not only engineer algorithms and robots, but to understand and empower human beings at all levels of abstraction, from the individual to our civilization as a whole. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe and YouTube, give it five stars at Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEX PODCAST. Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Since Cash App does fractional share trading, let me mention that the order execution algorithm that worked behind the scenes to create the abstraction of the fractional orders is to me an algorithmic marvel. Great props for the Cash App engineers for solving a hard problem that in the end provides an easy interface that takes a step up to the next layer of abstraction over the stock market, making trading more accessible for new investors and diversification much easier. So once again, if you get Cash App from the App Store or Google Play and use the code LEX PODCAST, you'll get $10 and Cash App will also donate $10 to First, one of my favorite organizations that is helping to advance robotics and STEM education for young people around the world. And now, here's my conversation with Michael I. Jordan. Given that you're one of the greats in the field of AI, machine learning, computer science, and so on, you're trivially called the Michael Jordan of machine learning, although as you know, you were born first, so technically MJ is the Michael I. Jordan of basketball. But anyway, my favorite is Yann LeCun calling you the Miles Davis of machine learning because as he says, you reinvent yourself periodically and sometimes leave fans scratching their heads after you change direction. So can you put at first your historian hat on and give a history of computer science and AI as you saw it, as you experienced it, including the four generations of AI successes that I've seen you talk about? Sure. Yeah, first of all, I much prefer Yann's metaphor. Miles Davis was a real explorer in jazz and he had a coherent story. So I think I have one, but it's not just the one you lived, it's the one you think about later. What the historian does is they look back and they revisit. I think what's happening right now is not AI, that was an intellectual aspiration that's still alive today as an aspiration. But I think this is akin to the development of chemical engineering from chemistry or electrical engineering from electromagnetism. So if you go back to the 30s or 40s, there wasn't yet chemical engineering. There was chemistry, there was fluid flow, there was mechanics and so on. But people pretty clearly viewed interesting goals to try to build factories that make chemicals products and do it viably, safely, make good ones, do it at scale. So people started to try to do that, of course, and some factories worked, some didn't, some were not viable, some exploded, but in parallel, developed a whole field called chemical engineering. Electrical engineering is a field, it's no bones about it, it has theoretical aspects to it, it has practical aspects. It's not just engineering, quote unquote, it's the real thing, real concepts are needed. Same thing with electrical engineering. There was Maxwell's equations, which in some sense were everything you know about electromagnetism, but you needed to figure out how to build circuits, how to build modules, how to put them together, how to bring electricity from one point to another safely and so on and so forth. So a whole field that developed called electrical engineering. I think that's what's happening right now, is that we have a proto field, which is statistics, more of the theoretical side of it, algorithmic side of computer science, that was enough to start to build things, but what things? Systems that bring value to human beings and use human data and mix in human decisions. The engineering side of that is all ad hoc. That's what's emerging. In fact, if you wanna call machine learning a field, I think that's what it is, that it's a proto form of engineering based on statistical and computational ideas of previous generations. But do you think there's something deeper about AI in his dreams and aspirations as compared to chemical engineering and electrical engineering? Well the dreams and aspirations maybe, but those are 500 years from now. I think that that's like the Greeks sitting there and saying, it would be neat to get to the moon someday. I think we have no clue how the brain does computation. We're just a clueless. We're even worse than the Greeks on most anything interesting scientifically of our era. Can you linger on that just for a moment because you stand not completely unique, but a little bit unique in the clarity of that. Can you elaborate your intuition of why we're, like where we stand in our understanding of the human brain? And a lot of people say, you know, scientists say we're not very far in understanding human brain, but you're like, you're saying we're in the dark here. Well, I know I'm not unique. I don't even think in the clarity, but if you talk to real neuroscientists that really study real synapses or real neurons, they agree, they agree. It's a hundreds of year task and they're building it up slowly and surely. What the signal is there is not clear. We think we have all of our metaphors. We think it's electrical, maybe it's chemical, it's a whole soup, it's ions and proteins and it's a cell. And that's even around like a single synapse. If you look at a electron micrograph of a single synapse, it's a city of its own. And that's one little thing on a dendritic tree, which is extremely complicated electrochemical thing. And it's doing these spikes and voltages are flying around and then proteins are taking that and taking it down into the DNA and who knows what. So it is the problem of the next few centuries. It is fantastic. But we have our metaphors about it. Is it an economic device? Is it like the immune system or is it like a layered set of, you know, arithmetic computations? We have all these metaphors and they're fun. But that's not real science per se. There is neuroscience. That's not neuroscience. All right. That's like the Greek speculating about how to get to the moon, fun, right? And I think that I like to say this fairly strongly because I think a lot of young people think we're on the verge because a lot of people who don't talk about it clearly let it be understood that, yes, we kind of, this is a brain inspired, we're kind of close, you know, breakthroughs are on the horizon. And that's scrupulous people sometimes who need money for their labs. That's what I'm saying, scrupulous, but people will oversell, I need money for my lab, I'm studying computational neuroscience, I'm going to oversell it. And so there's been too much of that. So I'll step into the gray area between metaphor and engineering with, I'm not sure if you're familiar with brain computer interfaces. So a company like Elon Musk has Neuralink that's working on putting electrodes into the brain and trying to be able to read, both read and send electrical signals. Just as you said, even the basic mechanism of communication in the brain is not something we understand. But do you hope without understanding the fundamental principles of how the brain works, we'll be able to do something interesting at that gray area of metaphor? It's not my area. So I hope in the sense, like anybody else hopes for some interesting things to happen from research, I would expect more something like Alzheimer's will get figured out from modern neuroscience. There's a lot of human suffering based on brain disease and we throw things like lithium at the brain, it kind of works, no one has a clue why. That's not quite true, but mostly we don't know. And that's even just about the biochemistry of the brain and how it leads to mood swings and so on. How thought emerges from that, we were really, really completely dim. So that you might want to hook up electrodes and try to do some signal processing on that and try to find patterns, fine, by all means, go for it. It's just not scientific at this point. So it's like kind of sitting in a satellite and watching the emissions from a city and trying to infer things about the microeconomy, even though you don't have microeconomic concepts. It's really that kind of thing. And so yes, can you find some signals that do something interesting or useful? Can you control a cursor or mouse with your brain? Yeah, absolutely, and then I can imagine business models based on that and even medical applications of that. But from there to understanding algorithms that allow us to really tie in deeply from the brain to computer, I just, no, I don't agree with Elon Musk. I don't think that's even, that's not for our generations, not even for the century. So just in hopes of getting you to dream, you've mentioned Kolmogorov and Turing might pop up, do you think that there might be breakthroughs that will get you to sit back in five, 10 years and say, wow? Oh, I'm sure there will be, but I don't think that there'll be demos that impress me. I don't think that having a computer call a restaurant and pretend to be a human is a breakthrough. Right. And people, you know, some people present it as such. It's imitating human intelligence. It's even putting coughs in the thing to make a bit of a PR stunt. And so fine that the world runs on those things too. And I don't want to diminish all the hard work and engineering that goes behind things like that and the ultimate value to the human race. But that's not scientific understanding. And I know the people that work on these things, they are after scientific understanding. In the meantime, they've got to kind of, you know, the trains got to run and they got mouths to feed and they got things to do and there's nothing wrong with all that. I would call that though, just engineering. And I want to distinguish that between an engineering field, like electrical engineering and chemical engineering that originally emerged, that had real principles and you really know what you're doing and you had a little scientific understanding, maybe not even complete. So it became more predictable and it really gave value to human life because it was understood. And so we don't want to muddle too much these waters of, you know, what we're able to do versus what we really can't do in a way that's going to impress the next. So I don't need to be wowed, but I think that someone comes along in 20 years, a younger person who's absorbed all the technology and for them to be wowed, I think they have to be more deeply impressed. A young Kolmogorov would not be wowed by some of the stunts that you see right now coming from the big companies. The demos, but do you think the breakthroughs from Kolmogorov would be, and give this question a chance, do you think there'll be in the scientific fundamental principles arena or do you think it's possible to have fundamental breakthroughs in engineering? Meaning, you know, I would say some of the things that Elon Musk is working with SpaceX and then others sort of trying to revolutionize the fundamentals of engineering, of manufacturing, of saying, here's a problem we know how to do a demo of and actually taking it to scale. Yeah. So there's going to be all kinds of breakthroughs. I just don't like that terminology. I'm a scientist and I work on things day in and day out and things move along and eventually you say, wow, something happened, but I don't like that language very much. Also I don't like to prize theoretical breakthroughs over practical ones. I tend to be more of a theoretician and I think there's lots to do in that arena right now. And so I wouldn't point to the Kolmogorovs, I might point to the Edisons of the era and maybe Musk is a bit more like that. But you know, Musk, God bless him, also will say things about AI that he knows very little about and he leads people astray when he talks about things he doesn't know anything about. Trying to program a computer to understand natural language, to be involved in a dialogue we're having right now, that ain't going to happen in our lifetime. You could fake it, you can mimic, sort of take old sentences that humans use and retread them, but the deep understanding of language, no, it's not going to happen. And so from that, I hope you can perceive that the deeper, yet deeper kind of aspects and intelligence are not going to happen. Now will there be breakthroughs? No, I think that Google was a breakthrough, I think Amazon is a breakthrough, you know, I think Uber is a breakthrough, you know, that bring value to human beings at scale in new, brand new ways based on data flows and so on. A lot of these things are slightly broken because there's not kind of an engineering field that takes economic value in context of data and, you know, planetary scale and worries about all the externalities, the privacy, you know, we don't have that field so we don't think these things through very well. I see that as emerging and that will be, you know, looking back from 100 years, that will be a constituted breakthrough in this era, just like electrical engineering was a breakthrough in the early part of the last century and chemical engineering was a breakthrough. So the scale, the markets that you talk about and we'll get to will be seen as sort of breakthrough and we're in the very early days of really doing interesting stuff there and we'll get to that, but just taking a quick step back, can you give, kind of throw off the historian hat. I mean, you briefly said that the history of AI kind of mimics the history of chemical engineering, but... I keep saying machine learning. You keep wanting to say AI, just to let you know, I don't, you know, I resist that. I don't think this is about AI really was John McCarthy as almost a philosopher saying, wouldn't it be cool if we could put thought in a computer? If we could mimic the human capability to think or put intelligence in, in some sense into a computer. That's an interesting philosophical question and he wanted to make it more than philosophy. He wanted to actually write down a logical formula and algorithms that would do that. And that is a perfectly valid, reasonable thing to do. That's not what's happening in this era. So the reason I keep saying AI actually, and I'd love to hear what you think about it. Machine learning has a very particular set of methods and tools. Maybe your version of it is that mine doesn't, it's very, very open. It does optimization, it does sampling, it does... So systems that learn is what machine learning is. Systems that learn and make decisions. And make decisions. So it's not just pattern recognition and, you know, finding patterns, it's all about making decisions in real worlds and having close feedback loops. So something like symbolic AI, expert systems, reasoning systems, knowledge based representation, all of those kinds of things, search, does that neighbor fit into what you think of as machine learning? So I don't even like the word machine learning, I think that what the field you're talking about is all about making large collections of decisions under uncertainty by large collections of entities. Right? And there are principles for that, at that scale. You don't have to say the principles are for a single entity that's making decisions, single agent or single human. It really immediately goes to the network of decisions. Is a good word for that or no? No, there's no good words for any of this. That's kind of part of the problem. So we can continue the conversation to use AI for all that. I just want to kind of raise the flag here that this is not about, we don't know what intelligence is and real intelligence. We don't know much about abstraction and reasoning at the level of humans. We don't have a clue. We're not trying to build that because we don't have a clue. Eventually it may emerge. They'll make, I don't know if there'll be breakthroughs, but eventually we'll start to get glimmers of that. It's not what's happening right now. Okay. We're taking data. We're trying to make good decisions based on that. We're trying to scale. We're trying to economically viably, we're trying to build markets. We're trying to keep value at that scale and aspects of this will look intelligent. Computers were so dumb before, they will seem more intelligent. We will use that buzzword of intelligence so we can use it in that sense. So machine learning, you can scope it narrowly as just learning from data and pattern recognition. But when I talk about these topics, maybe data science is another word you could throw in the mix, it really is important that the decisions are as part of it. It's consequential decisions in the real world. Am I going to have a medical operation? Am I going to drive down the street? Things where there's scarcity, things that impact other human beings or other environments and so on. How do I do that based on data? How do I do that adaptively? How do I use computers to help those kinds of things go forward? Whatever you want to call that. So let's call it AI. Let's agree to call it AI, but let's not say that the goal of that is intelligence. The goal of that is really good working systems at planetary scale that we've never seen before. So reclaim the word AI from the Dartmouth conference from many decades ago of the dream of humans. I don't want to reclaim it. I want a new word. I think it was a bad choice. I mean, if you read one of my little things, the history was basically that McCarthy needed a new name because cybernetics already existed and he didn't like, no one really liked Norbert Wiener. Norbert Wiener was kind of an island to himself and he felt that he had encompassed all this and in some sense he did. You look at the language of cybernetics, it was everything we're talking about. It was control theory and signal processing and some notions of intelligence and closed feedback loops and data. It was all there. It's just not a word that lived on partly because of the maybe the personalities. But McCarthy needed a new word to say, I'm different from you. I'm not part of your show. I got my own. Invented this word and again, thinking forward about the movies that would be made about it, it was a great choice. But thinking forward about creating a sober academic and real world discipline, it was a terrible choice because it led to promises that are not true that we understand. We understand artificial perhaps, but we don't understand intelligence. It's a small tangent because you're one of the great personalities of machine learning, whatever the heck you call the field. Do you think science progresses by personalities or by the fundamental principles and theories and research that's outside of personalities? Both. And I wouldn't say there should be one kind of personality. I have mine and I have my preferences and I have a kind of network around me that feeds me and some of them agree with me and some of them disagree, but all kinds of personalities are needed. Right now, I think the personality that it's a little too exuberant, a little bit too ready to promise the moon is a little bit too much in ascendance. And I do think that there's some good to that. It certainly attracts lots of young people to our field, but a lot of those people come in with strong misconceptions and they have to then unlearn those and then find something to do. And so I think there's just got to be some multiple voices and I wasn't hearing enough of the more sober voice. So as a continuation of a fun tangent and speaking of vibrant personalities, what would Yeah. And you think technically speaking, it's possible to help. I don't know the answers, but it's a, it's a, it's a less anonymity, a little more locality, you know, worlds that you kind of enter in and you trust the people there in those worlds so that when you start having a discussion, you know, not only is that people are not going to hurt you, but it's not going to be a total waste of your time because there's a lot of wasting of time that, you know, a lot of us, I pulled out of Facebook early on cause it was clearly going to waste a lot of my time even though there was some value. And so, yeah, worlds that are somehow you enter in and you know what you're getting and it's kind of appeals to you and you might, new things might happen, but you kind of have some, some trust in that world. And there's some deep, interesting, complex psychological aspects around anonymity, how that changes human behavior that's quite dark. Quite dark. Yeah. I think a lot of us are, especially those of us who really loved the advent of technology. I love social networks when they came out. I was just, I didn't see any negatives there at all. But then I started seeing comment sections. I think it was maybe, you know, with the CNN or something. And I started to go, wow, this, this darkness I just did not know about and, and our technology is now amplifying it. So sorry for the big philosophical question, but on that topic, do you think human beings, cause you've also, out of all things, had a foot in psychology too, the, do you think human beings are fundamentally good? Like all of us have good intent that could be mind or is it depending on context and environment, everybody could be evil. So my answer is fundamentally good. But fundamentally limited. All of us have very, you know, blinkers on. We don't see the other person's pain that easily. We don't see the other person's point of view that easily. We're very much in our own head, in our own world. And on my good days, I think the technology could open us up to, you know, more perspectives and more less blinkered and more understanding, you know, a lot of wars in human history happened because of just ignorance. They didn't, they, they thought the other person was doing this while their person wasn't doing this. And we have a huge amounts of that. But in my lifetime, I've not seen technology really help in that way yet. And I do, I do, I do believe in that, but you know, no, I think fundamentally humans are good. The people suffer, people have grievances because you have grudges and those things cause them to do things they probably wouldn't want. They regret it often. So no, I, I think it's a, you know, part of the progress of technology is to indeed allow it to be a little easier to be the real good person you actually are. Well, but do you think individual human life or society could be modeled as an optimization problem? Not the way I think typically, I mean, that's, you're talking about one of the most complex phenomenon in the whole, you know, in all of which the individual human life or society as a whole. Both, both. I mean, individual human life is amazingly complex. And so you know, optimization is kind of just one branch of mathematics that talks about certain kinds of things. And it just feels way too limited for the complexity of such things. What properties of optimization problems do you think, so do you think most interesting problems that could be solved through optimization, what kind of properties does that surface have non convexity, convexity, linearity, all those kinds of things, saddle points? Well, so optimization is just one piece of mathematics. You know, there's like, you just, even in our era, we're aware that say sampling is coming up, examples of something coming up with a distribution. What's optimization? What's sampling? Well, they, you can, if you're a kind of a certain kind of mathematician, you can try to blend them and make them seem to be sort of the same thing. But optimization is roughly speaking, trying to find a point that, a single point that is the optimum of a criterion function of some kind. And sampling is trying to, from that same surface, treat that as a distribution or density and find points that have high density. So I want the entire distribution in a sampling paradigm and I want the, you know, the single point, that's the best point in the optimization paradigm. Now if you were optimizing in the space of probability measures, the output of that could be a whole probability distribution. So you can start to make these things the same. But in mathematics, if you go too high up that kind of abstraction hierarchy, you start to lose the, you know, the ability to do the interesting theorems. So you kind of don't try that. You don't try to overly over abstract. So as a small tangent, what kind of worldview do you find more appealing? One that is deterministic or stochastic? Well, that's easy. I mean, I'm a statistician. You know, the world is highly stochastic. I don't know what's going to happen in the next five minutes, right? Because what you're going to ask, what we're going to do, what I'll say. Due to the uncertainty. Due to the... Massive uncertainty. Yeah. You know, massive uncertainty. And so the best I can do is have come rough sense or probability distribution on things and somehow use that in my reasoning about what to do now. So how does the distributed at scale when you have multi agent systems look like? So optimization can optimize sort of, it makes a lot more sense, sort of at least from my from robotics perspective, for a single robot, for a single agent, trying to optimize some objective function. When you start to enter the real world, this game theoretic concept starts popping up. That's how do you see optimization in this? Because you've talked about markets in a scale. What does that look like? Do you see it as optimization? Do you see it as sampling? Do you see like, how should you mark? These all blend together. And a system designer thinking about how to build an incentivized system will have a blend of all these things. So, you know, a particle in a potential well is optimizing a functional called a Lagrangian, right? The particle doesn't know that. There's no algorithm running that does that. It just happens. And so it's a description mathematically of something that helps us understand as analysts what's happening, right? And so the same thing will happen when we talk about, you know, mixtures of humans and computers and markets and so on and so forth, there'll be certain principles that allow us to understand what's happening, whether or not the actual algorithms are being used by any sense is not clear. Now at some point, I may have set up a multi agent or market kind of system. And I'm now thinking about an individual agent in that system. And they're asked to do some task and they're incentivized in some way, they get certain signals and they have some utility. What they will do at that point is they just won't know the answer, they may have to optimize to find an answer. Okay, so an artist could be embedded inside of an overall market. You know, and game theory is very, very broad. It is often studied very narrowly for certain kinds of problems. But it's roughly speaking, this is just the, I don't know what you're going to do. So I kind of anticipate that a little bit, and you anticipate what I'm anticipating. And we kind of go back and forth in our own minds. We run kind of thought experiments. You've talked about this interesting point in terms of game theory, you know, most optimization problems really hate saddle points, maybe you can describe what saddle points are. But I've heard you kind of mentioned that there's a there's a branch of optimization that you could try to explicitly look for saddle points as a good thing. Oh, not optimization. That's just game theory that that so there's all kinds of different equilibria in game theory. And some of them are highly explanatory behavior. They're not attempting to be algorithmic. They're just trying to say, if you happen to be at this equilibrium, you would see certain kind of behavior. And we see that in real life. That's what an economist wants to do, especially behavioral economists in continuous differential game theory, you're in continuous spaces, a some of the simplest equilibria are saddle points and Nash equilibrium as a saddle point. It's a special kind of saddle point. So classically, in game theory, you were trying to find Nash equilibria and an algorithmic game theory, you're trying to find algorithms that would find them. And so you're trying to find saddle points. I mean, so that's literally what you're trying to do. But you know, any economist knows that Nash equilibria have their limitations. They are definitely not that explanatory in many situations. They're not what you really want. There's other kind of equilibria. And there's names associated with these because they came from history with certain people working on them, but there will be new ones emerging. So you know, one example is a Stackelberg equilibrium. So you know, Nash, you and I are both playing this game against each other or for each other, maybe it's cooperative, and we're both going to think it through and then we're going to decide and we're going to do our thing simultaneously. You know, in a Stackelberg, no, I'm going to be the first mover. I'm going to make a move. You're going to look at my move and then you're going to make yours. Now since I know you're going to look at my move, I anticipate what you're going to do. And so I don't do something stupid, but then I know that you are also anticipating me. So we're kind of going back and forth on why, but there is then a first mover thing. And so those are different equilibria, right? And so just mathematically, yeah, these things have certain topologies and certain shapes that are like, what's it, algorithmically or dynamically, how do you move towards them? How do you move away from things? You know, so some of these questions have answers, they've been studied, others do not. And especially if it becomes stochastic, especially if there's large numbers of decentralized things, there's just, you know, young people get in this field who kind of think it's all done because we have, you know, TensorFlow. Well, no, these are all open problems and they're really important and interesting. And it's about strategic settings. How do I collect data? Suppose I don't know what you're going to do because I don't know you very well, right? Well, I got to collect data about you. So maybe I want to push you into a part of the space where I don't know much about you so I can get data. Cause, and then later I'll realize that you'll never, you'll never go there because of the way the game is set up. You know, that's part of the overall, you know, data analysis context is that. Even the game of poker is fascinating space, whenever there's any uncertainty, a lack of information, it's a super exciting space. Just to linger on optimization for a second. So when we look at deep learning, it's essentially minimization of a complicated loss function. So is there something insightful or hopeful that you see in the kinds of function surface that loss functions, the deep learning and in the real world is trying to optimize over? Is there something interesting as it's just the usual kind of problems of optimization? I think from an optimization point of view, that surface, first of all, it's pretty smooth. And secondly, if there's over, if it's over parameterized, there's kind of lots of paths down to reasonable Optima. And so kind of the getting downhill to the, to an optimum is viewed as not as hard as you might've expected in high dimensions. The fact that some Optima tend to be really good ones and others not so good. And you tend to, it's not, sometimes you find the good ones is sort of still needs explanation. Yeah. But, but the particular surface is coming from the particular generation of neural nets. I kind of suspect those will, those will change in 10 years. It will not be exactly those surfaces. There'll be some others that are an optimization theory will help contribute to why other surfaces or why other algorithms. Years of arithmetic operations with a little bit of nonlinearity, that's not, that didn't come from neuroscience per se. I mean, maybe in the minds of some of the people working on it, they were thinking about brains, but they were arithmetic circuits in all kinds of fields, computer science control theory and so on. And that layers of these could transform things in certain ways. And that if it's smooth, maybe you could find parameter values is a sort of big discovery that it's working, it's able to work at this scale. But I don't think that we're stuck with that and we're, we're certainly not stuck with that cause we're understanding the brain. So in terms of on the algorithm side sort of gradient descent, do you think we're stuck with gradient descent as a variance of it? What variance do you find interesting or do you think there'll be something else invented that is able to walk all over these optimization spaces in more interesting ways? So there's a co design of the surface and the, or the architecture and the algorithm. So if you just ask if we stay with the kind of architectures that we have now and not just neural nets, but you know, phase retrieval architectures or matrix completion architectures and so on. You know, I think we've kind of come to a place where yeah, a stochastic gradient algorithms are dominant and there are versions that are a little better than others. They have more guarantees, they're more robust and so on. And there's ongoing research to kind of figure out which is the best arm for which situation. But I think that that'll start to co evolve, that that'll put pressure on the actual architecture. And so we shouldn't do it in this particular way, we should do it in a different way because this other algorithm is now available if you do it in a different way. So that I can't really anticipate that co evolution process, but you know, gradients are amazing mathematical objects. They have a lot of people who start to study them more deeply mathematically are kind of shocked about what they are and what they can do. Think about it this way, suppose that I tell you if you move along the x axis, you go uphill in some objective by three units, whereas if you move along the y axis, you go uphill by seven units, right? Now I'm going to only allow you to move a certain unit distance, right? What are you going to do? Well, most people will say that I'm going to go along the y axis, I'm getting the biggest bang for my buck, you know, and my buck is only one unit, so I'm going to put all of it in the y axis, right? And why should I even take any of my strength, my step size and put any of it in the x axis because I'm getting less bang for my buck. That seems like a completely clear argument and it's wrong because the gradient direction is not to go along the y axis, it's to take a little bit of the x axis. And to understand that, you have to know some math and so even a trivial so called operator like gradient is not trivial and so, you know, exploiting its properties is still very important. Now we know that just pervading descent has got all kinds of problems, it gets stuck in many ways and it had never, you know, good dimension dependence and so on. So my own line of work recently has been about what kinds of stochasticity, how can we get dimension dependence, how can we do the theory of that and we've come up pretty favorable results with certain kinds of stochasticity. We have sufficient conditions generally. We know if you do this, we will give you a good guarantee. We don't have necessary conditions that it must be done a certain way in general. So stochasticity, how much randomness to inject into the walking along the gradient? And what kind of randomness? Why is randomness good in this process? Why is stochasticity good? Yeah, so I can give you simple answers but in some sense again, it's kind of amazing. Stochasticity just, you know, particular features of a surface that could have hurt you if you were doing one thing deterministically won't hurt you because by chance, there's very little chance that you would get hurt. So here stochasticity, it just kind of saves you from some of the particular features of surfaces. In fact, if you think about surfaces that are discontinuous in our first derivative, like an absolute value function, you will go down and hit that point where there's nondifferentiability. And if you're running a deterministic algorithm at that point, you can really do something bad. Whereas stochasticity just means it's pretty unlikely that's going to happen, that you're going to hit that point. So it's again, nontrivial to analyze but especially in higher dimensions, also stochasticity, our intuition isn't very good about it but it has properties that kind of are very appealing in high dimensions for a lot of large number of reasons. So it's all part of the mathematics to kind of, that's what's fun to work in the field is that you get to try to understand this mathematics. But long story short, you know, partly empirically, it was discovered stochastic gradient is very effective and theory kind of followed, I'd say, that but I don't see that we're getting clearly out of that. What's the most beautiful, mysterious, a profound idea to you in optimization? I don't know the most. But let me just say that Nesterov's work on Nesterov acceleration to me is pretty surprising and pretty deep. Can you elaborate? Well Nesterov acceleration is just that, suppose that we are going to use gradients to move around in a space. For the reasons I've alluded to, they're nice directions to move. And suppose that I tell you that you're only allowed to use gradients, you're not going to be allowed to use this local person that can only sense kind of the change in the surface. But I'm going to give you kind of a computer that's able to store all your previous gradients. And so you start to learn some something about the surface. And I'm going to restrict you to maybe move in the direction of like a linear span of all the gradients. So you can't kind of just move in some arbitrary direction, right? So now we have a well defined mathematical complexity model. There's certain classes of algorithms that can do that and others that can't. And we can ask for certain kinds of surfaces, how fast can you get down to the optimum? So there's answers to these. So for a smooth convex function, there's an answer, which is one over the number of steps squared. You will be within a ball of that size after k steps. Gradient descent in particular has a slower rate, it's one over k. So you could ask, is gradient descent actually, even though we know it's a good algorithm, is it the best algorithm? And the answer is no. Well, not clear yet, because one over k squared is a lower bound. That's probably the best you can do. Gradient is one over k, but is there something better? And so I think as a surprise to most, Nesterov discovered a new algorithm that has got two pieces to it. It's two gradients and puts those together in a certain kind of obscure way. And the thing doesn't even move downhill all the time. It sometimes goes back uphill. And if you're a physicist, that kind of makes some sense. You're building up some momentum and that is kind of the right intuition, but that intuition is not enough to understand kind of how to do it and why it works. But it does. It achieves one over k squared and it has a mathematical structure and it's still kind of to this day, a lot of us are writing papers and trying to explore that and understand it. So there are lots of cool ideas and optimization, but just kind of using gradients, I think is number one that goes back, you know, 150 years. And then Nesterov, I think has made a major contribution with this idea. So like you said, gradients themselves are in some sense, mysterious. They're not as trivial as... Not as trivial. Coordinate descent is more of a trivial one. You just pick one of the coordinates. That's how we think. That's how our human mind thinks. That's how our human minds think. And gradients are not that easy for our human mind to grapple with. An absurd question, but what is statistics? So here it's a little bit, it's somewhere between math and science and technology. It's somewhere in that convex hole. So it's a set of principles that allow you to make inferences that have got some reason to be believed and also principles that allow you to make decisions where you can have some reason to believe you're not going to make errors. So all of that requires some assumptions about what do you mean by an error? What do you mean by the probabilities? But after you start making some of those assumptions, you're led to conclusions that, yes, I can guarantee that if you do this in this way, your probability of making an error will be small. Your probability of continuing to not make errors over time will be small. And the probability that you found something that's real will be small, will be high. So decision making is a big part of that. Decision making is a big part. Yeah. So statistics, short history was that, it goes back as a formal discipline, 250 years or so. It was called inverse probability because around that era, probability was developed sort of especially to explain gambling situations. Of course, interesting. So you would say, well, given the state of nature is this, there's a certain roulette board that has a certain mechanism and what kind of outcomes do I expect to see? And especially if I do things long amounts of time, what outcomes will I see? And the physicists started to pay attention to this. And then people said, well, let's turn the problem around. What if I saw certain outcomes, could I infer what the underlying mechanism was? That's an inverse problem. And in fact, for quite a while, statistics was called inverse probability. That was the name of the field. And I believe that it was Laplace who was working in Napoleon's government who needed to do a census of France, learn about the people there. So he went and gathered data and he analyzed that data to determine policy and said, well, let's call this field that does this kind of thing statistics because the word state is in there. In French, that's etat, but it's the study of data for the state. So anyway, that caught on and it's been called statistics ever since. But by the time it got formalized, it was sort of in the 30s. And around that time, there was game theory and decision theory developed nearby. People in that era didn't think of themselves as either computer science or statistics or control or econ. They were all the above. And so Von Neumann is developing game theory, but also thinking of that as decision theory. Wald is an econometrician developing decision theory and then turning that into statistics. And so it's all about, here's not just data and you analyze it, here's a loss function. Here's what you care about. Here's the question you're trying to ask. Here is a probability model and here's the risk you will face if you make certain decisions. And to this day, in most advanced statistical curricula, you teach decision theory as the starting point and then it branches out into the two branches of Bayesian and frequentist. But that's all about decisions. In statistics, what is the most beautiful, mysterious, maybe surprising idea that you've come across? Yeah, good question. I mean, there's a bunch of surprising ones. There's something that's way too technical for this thing, but something called James Stein estimation, which is kind of surprising and really takes time to wrap your head around. Can you try to maybe... I think I don't want to even want to try. Let me just say a colleague at Steven Stigler at University of Chicago wrote a really beautiful paper on James Stein estimation, which helps to... It's views a paradox. It kind of defeats the mind's attempts to understand it, but you can and Steve has a nice perspective on that. So one of the troubles with statistics is that it's like in physics that are in quantum physics, you have multiple interpretations. There's a wave and particle duality in physics and you get used to that over time, but it still kind of haunts you that you don't really quite understand the relationship. The electron's a wave and electron's a particle. Well the same thing happens here. There's Bayesian ways of thinking and frequentist, and they are different. They sometimes become sort of the same in practice, but they are physically different. And then in some practice, they are not the same at all. They give you rather different answers. And so it is very much like wave and particle duality, and that is something that you have to kind of get used to in the field. Can you define Bayesian and frequentist? Yeah in decision theory you can make, I have a video that people could see. It's called are you a Bayesian or a frequentist and kind of help try to make it really clear. It comes from decision theory. So you know, decision theory, you're talking about loss functions, which are a function of data X and parameter theta. They're a function of two arguments. Okay. Neither one of those arguments is known. You don't know the data a priori. It's random and the parameters unknown. All right. So you have a function of two things you don't know, and you're trying to say, I want that function to be small. I want small loss, right? Well what are you going to do? So you sort of say, well, I'm going to average over these quantities or maximize over them or something so that, you know, I turn that uncertainty into something certain. So you could look at the first argument and average over it, or you could look at the second argument and average over it. That's Bayesian and frequentist. So the frequentist says, I'm going to look at the X, the data, and I'm going to take that as random and I'm going to average over the distribution. So I take the expectation loss under X. Theta is held fixed, right? That's called the risk. And so it's looking at other, all the data sets you could get, right? And say, how well will a certain procedure do under all those data sets? That's called a frequentist guarantee, right? So I think it is very appropriate when like you're building a piece of software and you're shipping it out there and people are using it on all kinds of data sets. You want to have a stamp, a guarantee on it that as people run it on many, many data sets that you never even thought about that 95% of the time it will do the right thing. Perfectly reasonable. The Bayesian perspective says, well, no, I'm going to look at the other argument of the loss function, the theta part, okay? That's unknown and I'm uncertain about it. So I could have my own personal probability for what it is, you know, how many tall people are there out there? I'm trying to infer the average height of the population while I have an idea roughly what the height is. So I'm going to average over the theta. So now that loss function as only now, again, one argument's gone, now it's a function of X and that's what a Bayesian does is they say, well, let's just focus on the particular X we got, the data set we got, we condition on that. Conditional on the X, I say something about my loss. That's a Bayesian approach to things. And the Bayesian will argue that it's not relevant to look at all the other data sets you could have gotten and average over them, the frequentist approach. It's really only the data sets you got, right? And I do agree with that, especially in situations where you're working with a scientist, you can learn a lot about the domain and you're really only focused on certain kinds of data and you gathered your data and you make inferences. I don't agree with it though, that, you know, in the sense that there are needs for frequentist guarantees, you're writing software, people are using it out there, you want to say something. So these two things have to got to fight each other a little bit, but they have to blend. So long story short, there's a set of ideas that are right in the middle that are called empirical Bayes. And empirical Bayes sort of starts with the Bayesian framework. It's kind of arguably philosophically more, you know, reasonable and kosher. Write down a bunch of the math that kind of flows from that, and then realize there's a bunch of things you don't know because it's the real world and you don't know everything. So you're uncertain about certain quantities. At that point, ask, is there a reasonable way to plug in an estimate for those things? Okay. And in some cases, there's quite a reasonable thing to do, to plug in, there's a natural thing you can observe in the world that you can plug in and then do a little bit more mathematics and assure yourself it's really good. So based on math or based on human expertise, what's, what, what are good? Oh, they're both going in. The Bayesian framework allows you to put a lot of human expertise in, but the math kind of guides you along that path and then kind of reassures you the end, you could put that stamp of approval under certain assumptions, this thing will work. So you asked the question, what's my favorite, you know, or what's the most surprising, nice idea. So one that is more accessible is something called false discovery rate, which is, you know, you're making not just one hypothesis test or making one decision, you're making a whole bag of them. And in that bag of decisions, you look at the ones where you made a discovery, you announced that something interesting had happened. All right. That's going to be some subset of your big bag. In the ones you made a discovery, which subset of those are bad? Or false, false discoveries. You'd like the fraction of your false discoveries among your discoveries to be small. That's a different criterion than accuracy or precision or recall or sensitivity and specificity. It's a different quantity. Those latter ones are almost all of them have more of a frequentist flavor. They say, given the truth is that the null hypothesis is true. Here's what accuracy I would get, or given that the alternative is true, here's what I would get. So it's kind of going forward from the state of nature to the data. The Bayesian goes the other direction from the data back to the state of nature. And that's actually what false discovery rate is. It says, given you made a discovery, okay, that's conditioned on your data. What's the probability of the hypothesis? It's going the other direction. And so the classical frequency look at that, well, I can't know that there's some priors needed in that. And the empirical Bayesian goes ahead and plows forward and starts writing down these formulas and realizes at some point, some of those things can actually be estimated in a reasonable way. And so it's kind of, it's a beautiful set of ideas. So I, this kind of line of argument has come out. It's not certainly mine, but it sort of came out from Robbins around 1960. Brad Efron has written beautifully about this in various papers and books. And the FDR is, you know, Benjamin in Israel, John Story did this Bayesian interpretation and so on. And he used to absorb these things over the years and find it a very healthy way to think about statistics. Let me ask you about intelligence to jump slightly back out into philosophy, perhaps. You said that maybe you can elaborate, but you said that defining just even the question of what is intelligence is a very difficult question. Is it a useful question? Do you think we'll one day understand the fundamentals of human intelligence and what it means, you know, have good benchmarks for general intelligence that we put before our machines? So I don't work on these topics so much that you're really asking the question for a psychologist really. And I studied some, but I don't consider myself at least an expert at this point. You know, a psychologist aims to understand human intelligence, right? And I think many psychologists I know are fairly humble about this. They might try to understand how a baby understands, you know, whether something's a solid or liquid or whether something's hidden or not. And maybe how a child starts to learn the meaning of certain words, what's a verb, what's a noun and also, you know, slowly but surely trying to figure out things. But humans ability to take a really complicated environment, reason about it, abstract about it, find the right abstractions, communicate about it, interact and so on is just, you know, really staggeringly rich and complicated. And so, you know, I think in all humility, we don't think we're kind of aiming for that in the near future. A certain psychologist doing experiments with babies in the lab or with people talking has a much more limited aspiration. And you know, Kahneman and Tversky would look at our reasoning patterns and they're not deeply understanding all the how we do our reasoning, but they're sort of saying, hey, here's some oddities about the reasoning and some things you should think about it. But also, as I emphasize in some things I've been writing about, you know, AI, the revolution hasn't happened yet. Yeah. Great blog post. I've been emphasizing that, you know, if you step back and look at intelligent systems of any kind and whatever you mean by intelligence, it's not just the humans or the animals or, you know, the plants or whatever, you know, so a market that brings goods into a city, you know, food to restaurants or something every day is a system. It's a decentralized set of decisions. Looking at it from far enough away, it's just like a collection of neurons. Every neuron is making its own little decisions, presumably in some way. And if you step back enough, every little part of an economic system is making all of its decisions. And just like with the brain, who knows what an individual neuron does and what the overall goal is, right? But something happens at some aggregate level, same thing with the economy. People eat in a city and it's robust. It works at all scales, small villages to big cities. It's been working for thousands of years. It works rain or shine, so it's adaptive. So all the kind of, you know, those are adjectives one tends to apply to intelligent systems. Robust, adaptive, you know, you don't need to keep adjusting it, self healing, whatever. Plus not perfect. You know, intelligences are never perfect and markets are not perfect. But I do not believe in this era that you cannot, that you can say, well, our computers are, our humans are smart, but you know, no markets are not, more markets are. So they are intelligent. Now we humans didn't evolve to be markets. We've been participating in them, right? But we are not ourselves a market per se. The neurons could be viewed as the market. There's economic, you know, neuroscience kind of perspective. That's interesting to pursue all that. The point though is, is that if you were to study humans and really be the world's best psychologist studied for thousands of years and come up with the theory of human intelligence, you might have never discovered principles of markets, you know, supply demand curves and you know, matching and auctions and all that. Those are real principles and they lead to a form of intelligence that's not maybe human intelligence. It's arguably another kind of intelligence. There probably are third kinds of intelligence or fourth that none of us are really thinking too much about right now. So if you really, and then all of those are relevant to computer systems in the future. Certainly the market one is relevant right now. Whereas the understanding of human intelligence is not so clear that it's relevant right now. Probably not. So if you want general intelligence, whatever one means by that, or, you know, understanding intelligence in a deep sense and all that, it is definitely has to be not just human intelligence. It's gotta be this broader thing. And that's not a mystery. Markets are intelligent. So, you know, it's definitely not just a philosophical stance to say we've got to move beyond intelligence. That sounds ridiculous. Yeah. But it's not. And in that blog post, you define different kinds of like intelligent infrastructure, AI, which I really like is some of the concepts you've just been describing. Do you see ourselves, if we see earth, human civilization as a single organism, do you think the intelligence of that organism, when you think from the perspective of markets and intelligence infrastructure is increasing, is it increasing linearly? Is it increasing exponentially? What do you think the future of that intelligence? Yeah, I don't know. I don't tend to think, I don't tend to answer questions like that because you know, that's science fiction. I'm hoping to catch you off guard. Well again, because you said it's so far in the future, it's fun to ask and you'll probably, you know, like you said, predicting the future is really nearly impossible. But say as an axiom, one day we create a human level, a superhuman level intelligent, not the scale of markets, but the scale of an individual. What do you think it is, what do you think it would take to do that? Or maybe to ask another question is how would that system be different than the biological human beings that we see around us today? Is it possible to say anything interesting to that question or is it just a stupid question? It's not a stupid question, but it's science fiction. Science fiction. And so I'm totally happy to read science fiction and think about it from time in my own life. I loved, there was this like brain in a vat kind of, you know, little thing that people were talking about when I was a student, I remember, you know, imagine that, you know, between your brain and your body, there's a, you know, there's a bunch of wires, right? And suppose that every one of them was replaced with a literal wire. And then suppose that wire was turned in actually a little wireless, you know, there's a receiver and sender. So the brain has got all the senders and receiver, you know, on all of its exiting, you know, axons and all the dendrites down to the body have replaced with senders and receivers. Now you could move the body off somewhere and put the brain in a vat, right? And then you could do things like start killing off those senders and receivers one by one. And after you've killed off all of them, where is that person? You know, they thought they were out in the body walking around the world and they moved on. So those are science fiction things. Those are fun to think about. It's just intriguing about where is, what is thought, where is it and all that. And I think every 18 year old should take philosophy classes and think about these things. And I think that everyone should think about what could happen in society that's kind of bad and all that. But I really don't think that's the right thing for most of us that are my age group to be doing and thinking about. I really think that we have so many more present, you know, first challenges and dangers and real things to build and all that such that, you know, spending too much time on science fiction, at least in public for like this, I think is not what we should be doing. Maybe over beers in private. That's right. Well, I'm not going to broadcast where I have beers because this is going to go on Facebook and I don't want a lot of people showing up there. But yeah, I'll, I love Facebook, Twitter, Amazon, YouTube. I have I'm optimistic and hopeful, but maybe, maybe I don't have grounds for such optimism and hope. But let me ask, you've mentored some of the brightest sort of some of the seminal figures in the field. Can you give advice to people who are undergraduates today? What does it take to take, you know, advice on their journey if they're interested in machine learning and in the ideas of markets from economics and psychology and all the kinds of things that you've exploring? What steps should they take on that journey? Well, yeah, first of all, the door is open and second, it's a journey. I like your language there. It is not that you're so brilliant and you have great, brilliant ideas and therefore that's just, you know, that's how you have success or that's how you enter into the field. It's that you apprentice yourself, you spend a lot of time, you work on hard things, you try and pull back and you be as broad as you can, you talk to lots of people. And it's like entering in any kind of a creative community. There's years that are needed and human connections are critical to it. So, you know, I think about, you know, being a musician or being an artist or something, you don't just, you know, immediately from day one, you know, you're a genius and therefore you do it. No, you, you know, practice really, really hard on basics and you be humble about where you are and then, and you realize you'll never be an expert on everything. So you kind of pick and there's a lot of randomness and a lot of kind of luck, but luck just kind of picks out which branch of the tree you go down, but you'll go down some branch. So yeah, it's a community. So the graduate school is, I still think is one of the wonderful phenomena that we have in our, in our world. It's very much about apprenticeship with an advisor. It's very much about a group of people you belong to. It's a four or five year process. So it's plenty of time to start from kind of nothing to come up to something, you know, more, more expertise, and then to start to have your own creativity start to flower, even surprising your own self. And it's a very cooperative endeavor. I think a lot of people think of science as highly competitive and I think in some other fields it might be more so. Here it's way more cooperative than you might imagine. And people are always teaching each other something and people are always more than happy to be clear that, so I feel I'm an expert on certain kinds of things, but I'm very much not expert on lots of other things and a lot of them are relevant and a lot of them are, I should know, but should in some society, you know, you don't. So I'm always willing to reveal my ignorance to people around me so they can teach me things. And I think a lot of us feel that way about our field. So it's very cooperative. I might add it's also very international because it's so cooperative. We see no barriers. And so that the nationalism that you see, especially in the current era and everything is just at odds with the way that most of us think about what we're doing here, where this is a human endeavor and we cooperate and are very much trying to do it together for the, you know, the benefit of everybody. So last question, where and how and why did you learn French and which language is more beautiful English or French? Great question. So first of all, I think Italian is actually more beautiful than French and English. And I also speak that. So I'm married to an Italian and I have kids and we speak Italian. Anyway, all kidding aside, every language allows you to express things a bit differently. And it is one of the great fun things to do in life is to explore those things. So in fact, when I kids or teens or college students ask me what they study, I say, well, do what your heart, where your heart is, certainly do a lot of math. Math is good for everybody, but do some poetry and do some history and do some language too. You know, throughout your life, you'll want to be a thinking person. You'll want to have done that. For me, French I learned when I was, I'd say a late teen, I was living in the middle of the country in Kansas and not much was going on in Kansas with all due respect to Kansas. And so my parents happened to have some French books on the shelf and just in my boredom, I pulled them down and I found this is fun. And I kind of learned the language by reading. And when I first heard it spoken, I had no idea what was being spoken, but I realized I had somehow knew it from some previous life and so I made the connection. But then I traveled and just I love to go beyond my own barriers and my own comfort or whatever. And I found myself on trains in France next to say older people who had lived a whole life of their own. And the ability to communicate with them was special and the ability to also see myself in other people's shoes and have empathy and kind of work on that language as part of that. So after that kind of experience and also embedding myself in French culture, which is quite amazing, languages are rich, not just because there's something inherently beautiful about it, but it's all the creativity that went into it. So I learned a lot of songs, read poems, read books. And then I was here actually at MIT where we're doing the podcast today and a young professor not yet married and not having a lot of friends in the area. So I just didn't have, I was kind of a bored person. I said, I heard a lot of Italians around. There's happened to be a lot of Italians at MIT, an Italian professor for some reason. And so I was kind of vaguely understanding what they were talking about. I said, well, I should learn this language too. So I did. And then later met my spouse and Italian became a part of my life. But I go to China a lot these days. I go to Asia, I go to Europe and every time I go, I kind of am amazed by the richness of human experience and the people don't have any idea if you haven't traveled, kind of how amazingly rich and I love the diversity. It's not just a buzzword to me. It really means something. I love to embed myself with other people's experiences. And so yeah, learning language is a big part of that. I think I've said in some interview at some point that if I had millions of dollars and infinite time or whatever, what would you really work on if you really wanted to do AI? And for me, that is natural language and really done right. Deep understanding of language. That's to me, an amazingly interesting scientific challenge. One we're very far away on. One we're very far away, but good natural language. People are kind of really invested then. I think a lot of them see that's where the core of AI is that if you understand that you really help human communication, you understand something about the human mind, the semantics that come out of the human mind and I agree, I think that will be such a long time. So I didn't do that in my career just cause I kind of, I was behind in the early days. I didn't kind of know enough of that stuff. I was at MIT, I didn't learn much language and it was too late at some point to kind of spend a whole career doing that, but I admire that field and so in my little way by learning language, you know, kind of that part of my brain has been trained up. Jan was right. You truly are the Miles Davis of machine learning. I don't think there's a better place than it. Mike it was a huge honor talking to you today. Merci beaucoup. All right. It's been my pleasure. Thanks for listening to this conversation with Michael I. Jordan and thank you to our presenting sponsor, Cash App. Download it, use code LEXPodcast, you'll get $10 and $10 will go to FIRST, an organization that inspires and educates young minds to become science and technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple Podcast, support on Patreon, or simply connect with me on Twitter at Lex Friedman. And now let me leave you with some words of wisdom from Michael I. Jordan from his blog post titled Artificial Intelligence, the revolution hasn't happened yet, calling for broadening the scope of the AI field. We should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term engineering is often invoked in a narrow sense in academia and beyond with overtones of cold, effectless machinery and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be. In the current era, we have a real opportunity to conceive of something historically new, a human centric engineering discipline. I will resist giving this emerging discipline a name, but if the acronym AI continues to be used, let's be aware of the very real limitations of this placeholder. Let's broaden our scope, tone down the hype, and recognize the serious challenges ahead. Thank you for listening and hope to see you next time. you say is the most interesting disagreement you have with Jan Lacune? So Jan's an old friend and I just say that I don't think we disagree about very much really. He and I both kind of have a let's build it kind of mentality and does it work kind of mentality and kind of concrete. We both speak French and we speak French more together and we have a lot in common. And so if one wanted to highlight a disagreement, it's not really a fundamental one. I think it's just kind of what we're emphasizing. Jan has emphasized pattern recognition and has emphasized prediction. And it's interesting to try to take that as far as you can. If you could do perfect prediction, what would that give you kind of as a thought experiment? And I think that's way too limited. We cannot do perfect prediction. We will never have the data sets that allow me to figure out what you're about ready to do, what question you're going to ask next. I have no clue. I will never know such things. Moreover, most of us find ourselves during the day in all kinds of situations we had no anticipation of that are kind of very, very novel in various ways. And in that moment, we want to think through what we want. And also there's going to be market forces acting on us. I'd like to go down that street, but now it's full because there's a crane in the street. I got it. I got to think about that. I got to think about what I might really want here. And I got to sort of think about how much it costs me to do this action versus this action. I got to think about the risks involved. A lot of our current pattern recognition and prediction systems don't do any risk evaluations. They have no error bars, right? I got to think about other people's decisions around me. I got to think about a collection of my decisions, even just thinking about like a medical treatment, you know, I'm not going to take a, the prediction of a neural net about my health, about something consequential. I'm not about ready to have a heart attack because some number is over 0.7. Even if you had all the data in the world that ever been collected about heart attacks better than any doctor ever had, I'm not going to trust the output of that neural net to predict my heart attack. I'm going to want to ask what if questions around that. I'm going to want to look at some us or other possible data I didn't have, causal things. I'm going to want to have a dialogue with a doctor about things we didn't think about when he gathered the data. You know, I could go on and on. I hope you can see. And I don't, I think that if you say predictions, everything that, that, that you're missing all of this stuff. And so prediction plus decision making is everything, but both of them are equally important. And so the field has emphasized prediction, Jan rightly so has seen how powerful that is. But at the cost of people not being aware that decision making is where the rubber really hits the road, where human lives are at stake, where risks are being taken, where you got to gather more data. You got to think about the error bars. You got to think about the consequences of your decisions on others. You got to think about the economy around your decisions, blah, blah, blah, blah. I'm not the only one working on those, but we're a smaller tribe. And right now we're not the one that people talk about the most. But you know, if you go out in the real world and industry, you know, at Amazon, I'd say half the people there are working on decision making and the other half are doing, you know, the pattern recognition. It's important. And the words of pattern recognition and prediction, I think the distinction there, not to linger on words, but the distinction there is more a constrained sort of in the lab data set versus decision making is talking about consequential decisions in the real world, under the messiness and the uncertainty of the real world. And just the whole of it, the whole mess of it that actually touches human beings and scale. And the forces, that's the distinction. It helps add those, that perspective, that broader perspective. You're right. I totally agree. On the other hand, if you're a real prediction person, of course, you want it to be in the real world. You want to predict real world events. I'm just saying that's not possible with just data sets. That it has to be in the context of, you know, strategic things that someone's doing, data they might gather, things they could have gathered, the reasoning process around data. It's not just taking data and making predictions based on the data. So one of the things that you're working on, I'm sure there's others working on it, but I don't hear often it talked about, especially in the clarity that you talk about it, and I think it's both the most exciting and the most concerning area of AI in terms of decision making. So you've talked about AI systems that help make decisions that scale in a distributed way, millions, billions decisions, sort of markets of decisions. Can you, as a starting point, sort of give an example of a system that you think about when you're thinking about these kinds of systems? Yeah, so first of all, you're absolutely getting into some territory, which I will be beyond my expertise. And there are lots of things that are going to be very not obvious to think about. Just like, again, I like to think about history a little bit, but think about put yourself back in the sixties. There was kind of a banking system that wasn't computerized really. There was database theory emerging and database people had to think about how do I actually not just move data around, but actual money and have it be, you know, valid and have transactions that ATMs happen that are actually, you know, all valid and so on and so forth. So that's the kind of issues you get into when you start to get serious about sorts of things like this. I like to think about as kind of almost a thought experiment to help me think something simpler, which is the music market. And because there is, to first order, there is no music market in the world right now and in our country, for sure. There are something called things called record companies and they make money and they prop up a few really good musicians and make them superstars and they all make huge amounts of money. But there's a long tail of huge numbers of people that make lots and lots of really good music that is actually listened to by more people than the famous people. They are not in a market. They cannot have a career. They do not make money. The creators, the creators, the creators, the so called influencers or whatever that diminishes who they are. So there are people who make extremely good music, especially in the hip hop or Latin world these days. They do it on their laptop. That's what they do on the weekend and they have another job during the week and they put it up on SoundCloud or other sites. Eventually it gets streamed. It now gets turned into bits. It's not economically valuable. The information is lost. It gets put up there. People stream it. You walk around in a big city, you see people with headphones, especially young kids listening to music all the time. If you look at the data, very little of the music they are listening to is the famous people's music and none of it's old music. It's all the latest stuff. But the people who made that latest stuff are like some 16 year old somewhere who will never make a career out of this, who will never make money. Of course there will be a few counter examples. The record companies incentivize to pick out a few and highlight them. Long story short, there's a missing market there. There is not a consumer producer relationship at the level of the actual creative acts. The pipelines and Spotify's of the world that take this stuff and stream it along, they make money off of subscriptions or advertising and those things. They're making the money. All right. And then they will offer bits and pieces of it to a few people again to highlight that they simulate a market. Anyway, a real market would be if you're a creator of music that you actually are somebody who's good enough that people want to listen to you, you should have the data available to you. There should be a dashboard showing a map of the United States. So in last week, here's all the places your songs were listened to. It should be transparent, vetable, so that if someone down in Providence sees that you're being listened to 10,000 times in Providence, that they know that's real data. You know it's real data. They will have you come give a show down there. They will broadcast to the people who've been listening to you that you're coming. If you do this right, you could go down there and make $20,000. You do that three times a year, you start to have a career. So in this sense, AI creates jobs. It's not about taking away human jobs. It's creating new jobs because it creates a new market. Once you've created a market, you've now connected up producers and consumers. The person who's making the music can say to someone who comes to their shows a lot, hey, I'll play at your daughter's wedding for $10,000. You'll say 8,000. They'll say 9,000. Then again, you can now get an income up to $100,000. You're not going to be a millionaire. And now even think about really the value of music is in these personal connections, even so much so that a young kid wants to wear a tshirt with their favorite musician's signature on it. So if they listen to the music on the internet, the internet should be able to provide them with a button that they push and the merchandise arrives the next day. We can do that. And now why should we do that? Well, because the kid who bought the shirt will be happy, but more the person who made the music will get the money. There's no advertising needed. So you can create markets between producers and consumers, take 5% cut. Your company will be perfectly sound. It'll go forward into the future and it will create new markets and that raises human happiness. Now this seems like, well, this is easy, just create this dashboard, kind of create some connections and all that. But if you think about Uber or whatever, you think about the challenges in the real world of doing things like this, and there are actually new principles going to be needed. You're trying to create a new kind of two way market at a different scale that's ever been done before. There's going to be unwanted aspects of the market. There'll be bad people. There'll be the data will get used in the wrong ways, it'll fail in some ways, it won't deliver about. You have to think that through. Just like anyone who ran a big auction or ran a big matching service in economics will think these things through. And so that maybe doesn't get at all the huge issues that can arise when you start to create markets, but it starts to, at least for me, solidify my thoughts and allow me to move forward in my own thinking. Yeah. So I talked to the head of research at Spotify actually, and I think their longterm goal, they've said, is to have at least one million creators make a comfortable living putting on Spotify. So I think you articulate a really nice vision of the world and the digital and the cyberspace of markets. What do you think companies like Spotify or YouTube or Netflix can do to create such markets? Is it an AI problem? Is it an interface problem for interface design? Is it some other kind of, is it an economics problem? Who should they hire to solve these problems? Well, part of it's not just top down. So the Silicon Valley has this attitude that they know how to do it. They will create the system just like Google did with the search box that will be so good that they'll just, everyone will adopt that. It's everything you said, but really I think missing that kind of culture. So it's literally that 16 year old who's able to create the songs. You don't create that as a Silicon Valley entity. You don't hire them per se. You have to create an ecosystem in which they are wanted and that they belong. And so you have to have some cultural credibility to do things like this. Netflix, to their credit, wanted some of that credibility and they created shows, content. They call it content. It's such a terrible word, but it's culture. And so with movies, you can kind of go give a large sum of money to somebody graduating from the USC film school. It's a whole thing of its own, but it's kind of like rich white people's thing to do. And American culture has not been so much about rich white people. It's been about all the immigrants, all the Africans who came and brought that culture and those rhythms to this world and created this whole new thing. American culture. And so companies can't artificially create that. They can't just say, hey, we're here. We're going to buy it up. You've got a partner. And so anyway, not to denigrate, these companies are all trying and they should, and I'm sure they're asking these questions and some of them are even making an effort. But it is partly a respect the culture as a technology person. You've got to blend your technology with cultural meaning. How much of a role do you think the algorithm, so machine learning has in connecting the consumer to the creator, sort of the recommender system aspect of this? Yeah. It's a great question. I think pretty high. There's no magic in the algorithms, but a good recommender system is way better than a bad recommender system. And recommender systems is a billion dollar industry back even 10, 20 years ago. And it continues to be extremely important going forward. What's your favorite recommender system, just so we can put something, well, just historically I was one of the, when I first went to Amazon, I first didn't like Amazon because they put the book people out of business or the library, the local booksellers went out of business. I've come to accept that there probably are more books being sold now and poor people reading them than ever before. And then local book stores are coming back. So that's how economics sometimes work. You go up and you go down. But anyway, when I finally started going there and I bought a few books, I was really pleased to see another few books being recommended to me that I never would have thought of. And I bought a bunch of them. So they obviously had a good business model. But I learned things and I still to this day kind of browse using that service. And I think lots of people get a lot, that is a good aspect of a recommendation system. I'm learning from my peers in an indirect way. And their algorithms are not meant to have them impose what we learn. It really is trying to find out what's in the data. It doesn't work so well for other kinds of entities, but that's just the complexity of human life. Like shirts, I'm not going to get recommendations on shirts, but that's interesting. If you try to recommend restaurants, it's hard. It's hard to do it at scale. But a blend of recommendation systems with other economic ideas, matchings and so on is really, really still very open research wise. And there's new companies that are going to emerge that do that well. What do you think is going to the messy, difficult land of say politics and things like that, that YouTube and Twitter have to deal with in terms of recommendation systems? Being able to suggest, I think Facebook just launched Facebook news. So recommend the kind of news that are most likely for you to be interesting. Do you think this is AI solvable, again, whatever term we want to use, do you think it's a solvable problem for machines or is it a deeply human problem that's unsolvable? So I don't even think about it at that level. I think that what's broken with some of these companies, it's all monetization by advertising. They're not, at least Facebook, I want to critique them, but they didn't really try to connect a producer and a consumer in an economic way, right? No one wants to pay for anything. And so they all, you know, starting with Google and Facebook, they went back to the playbook of, you know, the television companies back in the day. No one wanted to pay for this signal. They will pay for the TV box, but not for the signal, at least back in the day. And so advertising kind of filled that gap and advertising was new and interesting and it somehow didn't take over our lives quite, right? Fast forward, Google provides a service that people don't want to pay for. And so somewhat surprisingly in the nineties, they made, they ended up making huge amounts so they cornered the advertising market. It didn't seem like that was going to happen, at least to me. These little things on the right hand side of the screen just did not seem all that economically interesting, but that companies had maybe no other choice. The TV market was going away and billboards and so on. So they've, they got it. And I think that sadly that Google just has, it was doing so well with that at making such money. They didn't think much more about how, wait a minute, is there a producer consumer relationship to be set up here? Not just between us and the advertisers market to be created. Is there an actual market between the producer consumer? They're the producers, the person who created that video clip, the person that made that website, the person who could make more such things, the person who could adjust it as a function of demand, the person on the other side who's asking for different kinds of things, you know? So you see glimmers of that now there's influencers and there's kind of a little glimmering of a market, but it should have been done 20 years ago. It should have been thought about. It should have been created in parallel with the advertising ecosystem. And then Facebook inherited that. And I think they also didn't think very much about that. So fast forward and now they are making huge amounts of money off of advertising. And the news thing and all these clicks is just feeding the advertising. It's all connected up to the advertiser. So you want more people to click on certain things because that money flows to you, Facebook. You're very much incentivized to do that. And when you start to find it's breaking, people are telling you, well, we're getting into some troubles. You try to adjust it with your smart AI algorithms, right? And figure out what are bad clicks. So maybe it shouldn't be click through rate, it should be something else. I find that pretty much hopeless. It does get into all the complexity of human life and you can try to fix it. You should, but you could also fix the whole business model. And the business model is that really, what are, are there some human producers and consumers out there? Is there some economic value to be liberated by connecting them directly? Is it such that it's so valuable that people will be able to pay for it? All right. And micro payments, like small payments. Micro, but even have to be micro. So I like the example, suppose I'm going, next week I'm going to India. Never been to India before. Right? I have a couple of days in Mumbai, I have no idea what to do there. Right? And I could go on the web right now and search. It's going to be kind of hopeless. I'm not going to find, you know, I have lots of advertisers in my face. Right? What I really want to do is broadcast to the world that I am going to Mumbai and have someone on the other side of a market look at me and, and there's a recommendation system there. So I'm not looking at all possible people coming to Mumbai. They're looking at the people who are relevant to them. So someone in my age group, someone who kind of knows me in some level, I give up a little privacy by that, but I'm happy because what I'm going to get back is this person can make a little video for me, or they're going to write a little two page paper on here's the cool things that you want to do and move by this week, especially, right? I'm going to look at that. I'm not going to pay a micro payment. I'm going to pay, you know, a hundred dollars or whatever for that. It's real value. It's like journalism. Um, and as an honest subscription, it's that I'm going to pay that person in that moment. Company's going to take 5% of that. And that person has now got it. It's a gig economy, if you will, but you know, done for it, you know, thinking about a little bit behind YouTube, there was actually people who could make more of those things. If they were connected to a market, they would make more of those things independently. You don't have to tell them what to do. You don't have to incentivize them any other way. Um, and so, yeah, these companies, I don't think have thought long and hard about that. So I do distinguish on Facebook on the one side, who just not thought about these things at all. I think, uh, thinking that AI will fix everything, uh, and Amazon thinks about them all the time because they were already out in the real world. They were delivering packages, people's doors. They were, they were worried about a market. They were worried about sellers and, you know, they worry and some things they do are great. Some things maybe not so great, but you know, they're in that business model. And then I'd say Google sort of hovers somewhere in between. I don't, I don't think for a long, long time they got it. I think they probably see that YouTube is more pregnant with possibility than, than, than they might've thought and that they're probably heading that direction. Um, but uh, you know, Silicon Valley has been dominated by the Google Facebook kind of mentality and the subscription and advertising and that is, that's the core problem, right? The fake news actually rides on top of that because it means that you're monetizing with clip through rate and that is the core problem. You got to remove that. So advertisement, if we're going to linger on that, I mean, that's an interesting thesis. I don't know if everyone really deeply thinks about that. So you're right. The thought is the advertising model is the only thing we have, the only thing we'll ever have. We have to fix, we have to build algorithms that despite that business model, you know, find the better angels of our nature and do good by society and by the individual. But you think we can slowly, you think, first of all, there's a difference between should and could. So you're saying we should slowly move away from the advertising model and have a direct connection between the consumer and the creator. The question I also have is, can we, because the advertising model is so successful now in terms of just making a huge amount of money and therefore being able to build a big company that provides, has really smart people working that create a good service. Do you think it's possible? And just to clarify, you think we should move away? Well, I think we should. Yeah. But we is the, you know, me. So society. Yeah. Well, the companies, I mean, so first of all, full disclosure, I'm doing a day a week at Amazon because I kind of want to learn more about how they do things. So, you know, I'm not speaking for Amazon in any way, but, you know, I did go there because I actually believe they get a little bit of this or trying to create these markets. And they don't really use, advertising is not a crucial part of it. Well, that's a good question. So it has become not crucial, but it's become more and more present if you go to Amazon website. And, you know, without revealing too many deep secrets about Amazon, I can tell you that, you know, a lot of people in the company question this and there's a huge questioning going on. You do not want a world where there's zero advertising. That actually is a bad world. Okay. So here's a way to think about it. You're a company that like Amazon is trying to bring products to customers, right? And the customer, at any given moment, you want to buy a vacuum cleaner, say, you want to know what's available for me. And, you know, it's not going to be that obvious. You have to do a little bit of work at it. The recommendation system will sort of help, right? But now suppose this other person over here has just made the world, you know, they spent a huge amount of energy. They had a great idea. They made a great vacuum cleaner. They know they really did it. They nailed it. It's an MIT, you know, whiz kid that made a great new vacuum cleaner, right? It's not going to be in the recommendation system. No one will know about it. The algorithms will not find it and AI will not fix that. Okay. At all. Right. How do you allow that vacuum cleaner to start to get in front of people, be sold well advertising. And here, what advertising is, it's a signal that you're, you believe in your product enough that you're willing to pay some real money for it. And to me as a consumer, I look at that signal. I say, well, first of all, I know these are not just cheap little ads cause we have now right now there. I know that, you know, these are super cheap, you know, pennies. If I see an ad where it's actually, I know the company is only doing a few of these and they're making, you know, real money is kind of flowing and I see an ad, I may pay more attention to it. And I actually might want that because I see, Hey, that guy spent money on his vacuum cleaner. Maybe there's something good there. So I will look at it. And so that's part of the overall information flow in a good market. So advertising has a role, but the problem is of course that that signal is now completely gone because it just, you know, dominant by these tiny little things that add up to big money for the company, you know? So I think it will just, I think it will change because the societies just don't, you know, stick with things that annoy a lot of people and advertising currently annoys people more than it provides information. And I think that a Google probably is smart enough to figure out that this is a dead, this is a bad model, even though it's a hard, huge amount of money and they'll have to figure out how to pull it away from it slowly. And I'm sure the CEO there will figure it out, but they need to do it. And they needed it to, so if you reduce advertising, not to zero, but you reduce it at the same time you bring up producer, consumer, actual real value being delivered. So real money is being paid and they take a 5% cut that 5% could start to get big enough to cancel out the lost revenue from the kind of the poor kind of advertising. And I think that a good company will do that, will realize that. And Facebook, you know, again, God bless them. They bring, you know, grandmothers, they bring children's pictures into grandmothers lives. It's fantastic. But they need to think of a new business model and that's the core problem there. Until they start to connect producer consumer, I think they will just continue to make money and then buy the next social network company and then buy the next one and the innovation level will not be high and the health issues will not go away. So I apologize that we kind of returned to words, I don't think the exact terms matter, but in sort of defense of advertisement, don't you think the kind of direct connection between consumer and creator producer is what advertisement strives to do, right? So that is best advertisement is literally now Facebook is listening to our conversation and heard that you're going to India and will be able to actually start automatically for you making these connections and start giving this offer. So like, I apologize if it's just a matter of terms, but just to draw a distinction, is it possible to make advertisements just better and better and better algorithmically to where it actually becomes a connection, almost a direct connection? That's a good question. So let's component on that. First of all, what we just talked about, I was defending advertising. Okay. So I was defending it as a way to get signals into a market that don't come any other way, especially algorithmically. It's a sign that someone spent money on it, it's a sign they think it's valuable. And if I think that if other things, someone else thinks it's valuable, and if I trust other people, I might be willing to listen. I don't trust that Facebook though, who's an intermediary between this. I don't think they care about me. Okay. I don't think they do. And I find it creepy that they know I'm going to India next week because of our conversation. Why do you think that is? So what, could you just put your PR hat on? Why do you think you find Facebook creepy and not trust them as do majority of the population? So they're out of the Silicon Valley companies, I saw like not approval rate, but there's ranking of how much people trust companies and Facebook is in the gutter. In the gutter, including people inside of Facebook. So what do you attribute that to? Because when I... Come on, you don't find it creepy that right now we're talking that I might walk out on the street right now that some unknown person who I don't know kind of comes up to me and says, I hear you're going to India. I mean, that's not even Facebook. That's just, I want transparency in human society. I want to have, if you know something about me, there's actually some reason you know something about me. That's something that if I look at it later and audit it kind of, I approve. You know something about me because you care in some way. There's a caring relationship even, or an economic one or something. Not just that you're someone who could exploit it in ways I don't know about or care about or I'm troubled by or whatever. We're in a world right now where that happens way too much and that Facebook knows things about a lot of people and could exploit it and does exploit it at times. I think most people do find that creepy. It's not for them. It's not that Facebook is not doing it because they care about them in a real sense. And they shouldn't. They should not be a big brother caring about us. That is not the role of a company like that. Why not? Wait, not the big brother part, but the caring, the trusting. I mean, don't those companies, just to link on it because a lot of companies have a lot of information about us. I would argue that there's companies like Microsoft that has more information about us than Facebook does and yet we trust Microsoft more. Well, Microsoft is pivoting. Microsoft, you know, under Satya Nadella has decided this is really important. We don't want to do creepy things. Really want people to trust us to actually only use information in ways that they really would approve of, that we don't decide, right? And I'm just kind of adding that the health of a market is that when I connect to someone who produces a consumer, it's not just a random producer or consumer, it's people who see each other. They don't like each other, but they sense that if they transact, some happiness will go up on both sides. If a company helps me to do that in moments that I choose of my choosing, then fine. So, and also think about the difference between, you know, browsing versus buying, right? There are moments in my life I just want to buy, you know, a gadget or something. I need something for that moment. I need some ammonia for my house or something because I got a problem with a spill. I want to just go in. I don't want to be advertised at that moment. I don't want to be led down various, you know, that's annoying. I want to just go and have it be extremely easy to do what I want. Other moments I might say, no, it's like today I'm going to the shopping mall. I want to walk around and see things and see people and be exposed to stuff. So I want control over that though. I don't want the company's algorithms to decide for me, right? I think that's the thing. There's a total loss of control if Facebook thinks they should take the control from us of deciding when we want to have certain kinds of information, when we don't, what information that is, how much it relates to what they know about us that we didn't really want them to know about us. I don't want them to be helping me in that way. I don't want them to be helping them by they decide they have control over what I want and when. I totally agree. Facebook, by the way, I have this optimistic thing where I think Facebook has the kind of personal information about us that could create a beautiful thing. So I'm really optimistic of what Facebook could do. It's not what it's doing, but what it could do. So I don't see that. I think that optimism is misplaced because there's not a bit, you have to have a business model behind these things. Create a beautiful thing is really, let's be, let's be clear. It's about something that people would value. And I don't think they have that business model and I don't think they will suddenly discover it by what, you know, a long hot shower. I disagree. I disagree in terms of, you can discover a lot of amazing things in a shower. So I didn't say that. I said, they won't come, they won't do it, but in the shower, I think a lot of other people will discover it. I think that this guy, so I should also, full disclosure, there's a company called United Masters, which I'm on their board and they've created this music market and I have a hundred thousand artists now signed on and they've done things like gone to the NBA and the NBA, the music you find behind NBA clips right now is their music, right? That's a company that had the right business model in mind from the get go, right? Executed on that. And from day one, there was value brought to, so here you have a kid who made some songs who suddenly their songs are on the NBA website, right? That's real economic value to people. And so, you know, so you and I differ on the optimism of being able to sort of change the direction of the Titanic, right? So I, yeah, I'm older than you, so I've seen some Titanic's crash, got it. But and just to elaborate, cause I totally agree with you and I just want to know how difficult you think this problem is of, so for example, I want to read some news and I would, there's a lot of times in the day where something makes me either smile or think in a way where I like consciously think this really gave me value. Like I sometimes listen to the daily podcasts in the New York times, way better than the New York times themselves, by the way, for people listening. That's like real journalism is happening for some reason in the podcast space. It doesn't make sense to me, but often I listen to it 20 minutes and I would be willing to pay for that, like $5, $10 for that experience. And how difficult, that's kind of what you're getting at is that little transaction. How difficult is it to create a frictionless system like Uber has, for example, for other things? What's your intuition there? So I, first of all, I pay little bits of money to, you know, to send, there's something called courts that does financial things. I like medium as a site, I don't pay there, but I would. You had a great post on medium. I would have loved to pay you a dollar and not others. I wouldn't have wanted it per se because there should be also sites where that's not actually the goal. The goal is to actually have a broadcast channel that I monetize in some other way if I chose to. I mean, I could now people know about it. I could, I'm not doing it, but that's fine with me. Also the musicians who are making all this music, I don't think the right model is that you pay a little subscription fee to them, right? Because people can copy the bits too easily and it's just not that somewhere the value is. The value is that a connection was made between real human beings, then you can follow up on that. All right. And create yet more value. So no, I think there's a lot of open questions here, hot open questions, but also, yeah, I do want good recommendation systems that recommend cool stuff to me. But it's pretty hard, right? I don't like them to recommend stuff just based on my browsing history. I don't like the based on stuff they know about me, quote unquote. What's unknown about me is the most interesting. So this is the, this is the really interesting question. We may disagree, maybe not. I think that I love recommender systems and I want to give them everything about me in a way that I trust. Yeah. But you, but you don't, because, so for example, this morning I clicked on a, you know, I was pretty sleepy this morning. I clicked on a story about the queen of England. Yes. Right. I do not give a damn about the queen of England. I really do not. But it was clickbait. It kind of looked funny and I had to say, what the heck are they talking about? I don't want to have my life, you know, heading that direction. Now that's in my browsing history. The system in any reasonable system will think that I care about the queen of England. That's browsing history. Right. But, but you're saying all the trace, all the digital exhaust or whatever, that's been kind of the models. If you collect all this stuff, you're going to figure all of us out. Well, if you're trying to figure out like kind of one person like Trump or something, maybe you could figure him out. But if you're trying to figure out, you know, 500 million people, you know, no way, no way. You think so? No, I do. I think so. I think we are, humans are just amazingly rich and complicated. Every one of us has our little quirks, every one of us has our little things that could intrigue us that we don't even know it will intrigue us. And there's no sign of it in our past, but by God, there it comes and you know, you fall in love with it. And I don't want a company trying to figure that out for me and anticipate that I want them to provide a forum, a market, a place that I kind of go and by hook or by crook, this happens, you know, I I'm walking down the street and I hear some Chilean music being played and I never knew I liked Chilean music, but wow. So there is that side and I want them to provide a limited, but you know, interesting place to go. Right. And so don't try to use your AI to kind of, you know, figure me out and then put me in a world where you figured me out, you know, no, create huge spaces for human beings where our creativity and our style will be enriched and come forward and it'll be a lot of more transparency. I won't have people randomly, anonymously putting comments up and I'll special based on stuff they know about me, facts that, you know, we are so broken right now. If you're, you know, especially if you're a celebrity, but you know, it's about anybody that anonymous people are hurting lots and lots of people right now. That's part of this thing that Silicon Valley is thinking that, you know, just collect all this information and use it in a great way. So no, I'm not, I'm not a pessimist, I'm very much an optimist by nature, but I think that's just been the wrong path for the whole technology to take. Be more limited, create, let humans rise up. Don't try to replace them. That's the AI mantra. Don't try to anticipate them. Don't try to predict them because you're, you're, you're not going to, you're not going to be able to do those things. You're going to make things worse. Okay. So right now, just give this a chance. Right now, the recommender systems are the creepy people in the shadow watching your every move. So they're looking at traces of you. They're not directly interacting with you, sort of the, your close friends and family, the way they know you is by having conversation, by actually having interactions back and forth. Do you think there's a place for recommender systems sort of to step, cause you, you just emphasize the value of human to human connection, but yeah, just give it a chance, AI human connection. Is there a role for an AI system to have conversations with you in terms of, to try to figure out what kind of music you like, not by just watching what you listening to, but actually having a conversation, natural language or otherwise. Yeah, no, I'm, I'm, so I'm not against it. I just wanted to push back against the, maybe you're saying you have options for Facebook. So there I think it's misplaced, but, but I think that distributing, yeah, no, so good for you. Go for it. That's a hard spot to be in. Yeah, no, good. Human interaction, like on our daily, the context around me in my own home is something that I don't want some big company to know about at all, but I would be more than happy to have technology help me with it. Which kind of technology? Well, you know, just, Alexa, Amazon, well, a good, Alexa's done right. And I think Alexa is a research platform right now more than anything else. But Alexa done right, you know, could do things like I, I leave the water running in my garden and I say, Hey, Alexa, the water's running in my garden. And even have Alexa figure out that that means when my wife comes home, that she should be told about that. That's a little bit of a reasoning. I would call that AI and by any kind of stretch, it's a little bit of reasoning and it actually kind of would make my life a little easier and better. And you know, I don't, I wouldn't call this a wow moment, but I kind of think that overall rises human happiness up to have that kind of thing. But not when you're lonely, Alexa, knowing loneliness. No, no, I don't want Alexa to be, feel intrusive. And I don't want just the designer of the system to kind of work all this out. I really want to have a lot of control and I want transparency and control. And if a company can stand up and give me that in the context of new technology, I think they're good. First of all, be way more successful than our current generation. And like I said, I was mentioning Microsoft, I really think they're, they're pivoting to kind of be the trusted old uncle, but you know, I think that they get that this is a way to go, that if you let people find technology, empowers them to have more control and have and have control, not just over privacy, but over this rich set of interactions, that that people are going to like that a lot more. And that's, that's the right business model going forward. What does control over privacy look like? Do you think you should be able to just view all the data that? No, it's much more than that. I mean, first of all, it should be an individual decision. Some people don't want privacy. They want their whole life out there. Other people's want it. Privacy is not a zero one. It's not a legal thing. It's not just about which data is available, which is not. I like to recall to people that, you know, a couple hundred years ago, everyone, there was not really big cities, everyone lived in on the countryside and villages and villages. Everybody knew everything about you. Very, you didn't have any privacy. Is that bad? Are we better off now? Well, you know, arguably no, because what did you get for that loss of certain kinds of privacy? Well, people help each other if they, because they know everything about you. They know something's bad's happening, they will help you with that. Right. And now you live in a big city, no one knows about that. You get no help. So it kind of depends the answer. I want certain people who I trust and there should be relationships. I should kind of manage all those, but who knows what about me? I should have some agency there. It shouldn't, I shouldn't be a drift in a sea of technology where I have no agency. I don't want to go reading things and checking boxes. So I don't know how to do that. And I'm not a privacy researcher per se. I just, I recognize the vast complexity of this. It's not just technology. It's not just legal scholars meeting technologists. There's gotta be kind of a whole layers around it. And so I, when I alluded to this emerging engineering field, this is a big part of it. When electrical engineering came, I'm not one around at the time, but you just didn't plug electricity into walls and all kinds of work. You don't have to have like underwriters laboratory that reassured you that that plug's not going to burn up your house and that that machine will do this and that and everything. There'll be whole people who can install things. There'll be people who can watch the installers. There'll be a whole layers, you know, an onion of these kinds of things. And for things as deep and interesting as privacy, which is as least as interesting as electricity, that's going to take decades to kind of work out, but it's going to require a lot of new structures that we don't have right now. So it's kind of hard to talk about it. And you're saying there's a lot of money to be made if you get it right. So something you should look at. A lot of money to be made in all these things that provide human services and people recognize them as useful parts of their lives. So yeah. So yeah, the dialogue sometimes goes from the exuberant technologists to the no technology is good, kind of. And that's, you know, in our public discourse, you know, and as far as you see too much of this kind of thing and the sober discussions in the middle, which are the challenge he wants to have or where we need to be having our conversations. And you know, there's just not actually, there's not many forum fora for those. You know, there's, that's, that's kind of what I would look for. Maybe I could go and I could read a comment section of something and it would actually be this kind of dialogue going back and forth. You don't see much of this, right? Which is why actually there's a resurgence of podcasts out of all, because people are really hungry for conversation, but there's technology is not helping much. So comment sections of anything, including YouTube is not hurting and not helping.\",\n          \"We are becoming cyborgs. Our brains are fundamentally changed. Everyone who grew up with electronics, we are fundamentally different from previous, from homo sapiens. I call us homo techno. I think we have evolved into homo techno, which is like essentially a new species. Previous technologies, I mean, may have even been more profound and moved us to a certain degree, but I think the computers are what make us homo techno. I think this is what, it's a brain augmentation. So it like allows for actual evolution. Like the computers accelerate the degree to which all the other technologies can also be accelerated. Would you classify yourself as a homo sapien or a homo techno? Definitely homo techno. So you're one of the earliest of the species. I think most of us are. The following is a conversation with Grimes, an artist, musician, songwriter, producer, director, and a fascinating human being who thinks a lot about both the history and the future of human civilization. Studying the dark periods of our past to help form an optimistic vision of our future. This is the Lex Friedman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Grimes. Oh yeah, the cloud lifter, there you go. There you go. You know your stuff. Have you ever used a cloud lifter? Yeah, I actually, this microphone cloud lifter is what Michael Jackson used, so. No, really? Yeah, this is like Thriller and stuff. This mic and a cloud lifter? Yeah, it's a incredible microphone. It's very flattering on vocals. I've used this a lot. It's great for demo vocals. It's great in a room. Sometimes it's easier to record vocals if you're just in a room and the music's playing and you just wanna feel it so it's not in the headphones. And this mic is pretty directional, so I think it's a good mic for just vibing out and just getting a real good vocal take. Just vibing, just in a room. Anyway, this is the Michael Jackson, Quincy Jones microphone. I feel way more badass now. All right, you wanna just get into it? I guess so. All right, one of your names, at least in this space and time, is C, like the letter C. And you told me that C means a lot of things. It's the speed of light. It's the render rate of the universe. It's yes in Spanish. It's the crescent moon. And it happens to be my favorite programming language because it basically runs the world, but it's also powerful, fast, and it's dangerous because you can mess things up really bad with it because of all the pointers. But anyway, which of these associations will the name C is the coolest to you? I mean, to me, the coolest is the speed of light, obviously, or the speed of light. When I say render rate of the universe, I think I mean the speed of light because essentially that's what we're rendering at. See, I think we'll know if we're in a simulation if the speed of light changes because if they can improve their render speed, then. Well, it's already pretty good. It's already pretty good, but if it improves, then we'll know, we can probably be like, okay, they've updated or upgraded. Well, it's fast enough for us humans because it seems immediate. There's no delay, there's no latency in terms of us humans on Earth interacting with things. But if you're like intergalactic species operating on a much larger scale, then you're gonna start noticing some weird stuff. Or if you can operate in like around a black hole, then you're gonna start to see some render issues. You can't go faster than the speed of light, correct? So it really limits our ability or one's ability to travel space. Theoretically, you can, you have wormholes. So there's nothing in general relativity that precludes faster than the speed of light travel. But it just seems you're gonna have to do some really funky stuff with very heavy things that have like weirdnesses, that have basically terrors in space time. We don't know how to do that. Do navigators know how to do it? Do navigators? Yeah. Folding space, basically making wormholes. So the name C. Yes. Who are you? Do you think of yourself as multiple people? Are you one person? Do you know, like in this morning, were you a different person than you are tonight? We are, I should say, recording this basically at midnight, which is awesome. Yes, thank you so much. I think I'm about eight hours late. No, you're right on time. Good morning. This is the beginning of a new day soon. Anyway, are you the same person you were in the morning and the evening? Is there multiple people in there? Do you think of yourself as one person? Or maybe you have no clue? Or are you just a giant mystery to yourself? Okay, these are really intense questions, but. Let's go, let's go. Because I asked this myself, like look in the mirror, who are you? People tell you to just be yourself, but what does that even mean? I mean, I think my personality changes with everyone I talk to. So I have a very inconsistent personality. Yeah. Person to person, so the interaction, your personality materializes. Or my mood. Like I'll go from being like a megalomaniac to being like, you know, just like a total hermit who is very shy. So some combinatorial combination of your mood and the person you're interacting with. Yeah, mood and people I'm interacting with. But I think everyone's like that. Maybe not. Well, not everybody acknowledges it and able to introspect it. Who brings out, what kind of person, what kind of mood brings out the best in you? As an artist and as a human. Can you introspect this? Like my best friends, like people I can, when I'm like super confident and I know that they're gonna understand everything I'm saying, so like my best friends, then when I can start being really funny, that's always my like peak mode. But it's like, yeah, takes a lot to get there. Let's talk about constraints. You've talked about constraints and limits. Do those help you out as an artist or as a human being? Or do they get in the way? Do you like the constraints? So in creating music, in creating art, in living life, do you like the constraints that this world puts on you? Or do you hate them? If constraints are moving, then you're good, right? Like it's like as we are progressing with technology, we're changing the constraints of like artistic creation. You know, making video and music and stuff is getting a lot cheaper. There's constantly new technology and new software that's making it faster and easier. We have so much more freedom than we had in the 70s. Like when Michael Jackson, you know, when they recorded Thriller with this microphone, like they had to use a mixing desk and all this stuff. And like probably even get in a studio, it's probably really expensive and you have to be a really good singer and you have to know how to use like the mixing desk and everything. And now I can just, you know, make I've made a whole album on this computer. I have a lot more freedom, but then I'm also constrained in different ways because there's like literally millions more artists. It's like a much bigger playing field. It's just like, I also, I didn't learn music. I'm not a natural musician. So I don't know anything about actual music. I just know about like the computer. So I'm really kind of just like messing around and like trying things out. Well, yeah, I mean, but the nature of music is changing. So you're saying you don't know actual music, what music is changing. Music is becoming, you've talked about this, is becoming, it's like merging with technology. Yes. It's becoming something more than just like the notes on a piano. It's becoming some weird composition that requires engineering skills, programming skills, some kind of human robot interaction skills, and still some of the same things that Michael Jackson had, which is like a good ear for a good sense of taste of what's good and not the final thing when it's put together. Like you're allowed, you're enabled, empowered with a laptop to layer stuff, to start like layering insane amounts of stuff. And it's super easy to do that. I do think music production is a really underrated art form. I feel like people really don't appreciate it. When I look at publishing splits, the way that people like pay producers and stuff, it's super, producers are just deeply underrated. Like so many of the songs that are popular right now or for the last 20 years, like part of the reason they're popular is because the production is really interesting or really sick or really cool. And it's like, I don't think listeners, like people just don't really understand what music production is. It's not, it's sort of like this weird, discombobulated art form. It's not like a formal, because it's so new, there isn't like a formal training path for it. It's mostly driven by like autodidacts. Like it's like almost everyone I know who's good at production, like they didn't go to music school or anything. They just taught themselves. Are they're mostly different? Like the music producers, you know, is there some commonalities that time together or are they all just different kinds of weirdos? Cause I just, I just hung out with Rick Rubin. I don't know if you've. Yeah, I mean, Rick Rubin is like literally one of the gods of music production. Like he's one of the people who first, you know, who like made music production, you know, made the production as important as the actual lyrics or the notes. But the thing he does, which is interesting, I don't know if you can speak to that, but just hanging out with him, he seems to just sit there in silence, close his eyes and listen. It's like, he almost does nothing. And that nothing somehow gives you freedom to be the best version of yourself. So that's music production somehow too, which is like encouraging you to do less, to simplify, to like push towards minimalism. I mean, I guess, I mean, I work differently from Rick Rubin cause Rick Rubin produces for other artists, whereas like I mostly produce for myself. So it's a very different situation. I also think Rick Rubin, he's in that, I would say advanced category of producer where like you've like earned your, you can have an engineer and stuff and people like do the stuff for you. But I usually just like do stuff myself. So you're the engineer, the producer and the artist. Yeah, I guess I would say I'm in the era, like the post Rick Rubin era. Like I come from the kind of like Skrillex school of thought, which is like where you are. Yeah, the engineer, producer, artist. Like where, I mean lately, sometimes I'll work with a producer now. I'm gently sort of delicately starting to collaborate a bit more, but like I think I'm kind of from the, like the whatever 2010s explosion of things where everything became available on the computer and you kind of got this like lone wizard energy thing going. So you embraced being the loneliness. Is the loneliness somehow an engine of creativity? Like, so most of your stuff, most of your creative quote unquote genius in quotes is in the privacy of your mind. Yes, well, it was, but here's the thing. I was talking to Daniel Eck and he said, he's like most artists, they have about 10 years, like 10 good years. And then they usually stop making their like vital shit. And I feel like I'm sort of like nearing the end of my 10 years on my own. So you have to become somebody else. Now I'm like, I'm in the process of becoming somebody else and reinventing. When I work with other people, because I've never worked with other people, I find that I make like, that I'm exceptionally rejuvenated and making like some of the most vital work I've ever made. So, because I think another human brain is like one of the best tools you can possibly find. Like. It's a funny way to put it, I love it. It's like if a tool is like, you know, whatever HP plus one or like adds some like stats to your character, like another human brain will like square it instead of just like adding something. Double up the experience points, I love this. We should also mention we're playing Tavern music before this and which I love, which I first, I think I first. You had to stop the Tavern music. Yeah, because it doesn't, the audio. Okay, okay. But it makes. Yeah, it'll make the podcast annoying. Add it in post, add it in post. No one will want to listen to the podcast. They probably would, but it makes me, it reminds me like of a video game, like a role playing video game where you have experience points. There's something really joyful about wandering places like Elder Scrolls, like Skyrim, just exploring these landscapes in another world and then you get experience points and you can work on different skills and somehow you progress in life. I don't know, it's simple. It doesn't have some of the messy complexities of life and there's usually a bad guy you can fight in Skyrim. It's dragons and so on. I'm sure in Elden Ring, there's a bunch of monsters you can fight. I love that. I feel like Elden Ring, I feel like this is a good analogy to music production though because it's like, I feel like the engineers and the people creating these open worlds are, it's sort of like similar to people, to music producers where it's like this hidden archetype that like no one really understands what they do and no one really knows who they are, but they're like, it's like the artist engineer because it's like, it's both art and fairly complex engineering. Well, you're saying they don't get enough credit. Aren't you kind of changing that by becoming the person doing everything? Aren't you, isn't the engineer? Well, I mean, others have gone before me. I'm not, you know, there's like Timbaland and Skrillex and there's all these people that are like, you know, very famous for this, but I just think the general, I think people get confused about what it is and just don't really know what it is per se and it's just when I see a song, like when there's like a hit song, like I'm just trying to think of like, just going for like even just a basic pop hit, like, what's it? Like Rules by Dua Lipa or something. The production on that is actually like really crazy. I mean, the song is also great, but it's like the production is exceptionally memorable. Like, you know, and it's just like no one, I can't, I don't even know who produced that song. It's just like, isn't part of like the rhetoric of how we just discuss the creation of art. We just sort of like don't consider the music producer because I think the music producer used to be more just simply recording things. Yeah, that's interesting because when you think about movies, we talk about the actor and the actresses, but we also talk about the directors. We don't talk about like that with the music as often. The Beatles music producer was one of the first kind of guy, one of the first people sort of introducing crazy sound design into pop music. I forget his name. He has the same, I forget his name, but you know, like he was doing all the weird stuff like dropping pianos and like, yeah. Oh, to get the, yeah, yeah, yeah, to get the sound, to get the authentic sound. What about lyrics? You think those, where did they fit into how important they are? I was heartbroken to learn that Elvis didn't write his songs. I was very mad. A lot of people don't write their songs. I understand this, but. But here's the thing. I feel like there's this desire for authenticity. I used to be like really mad when like people wouldn't write or produce their music and I'd be like, that's fake. And then I realized there's all this like weird bitterness and like agronus in art about authenticity. But I had this kind of like weird realization recently where I started thinking that like art is sort of a decentralized collective thing. Like art is kind of a conversation with all the artists that have ever lived before you. You know, like it's like, you're really just sort of, it's not like anyone's reinventing the wheel here. Like you're kind of just taking, you know, thousands of years of art and like running it through your own little algorithm and then like making your interpretation of it. You just joined the conversation with all the other artists that came before. It's just a beautiful way to look at it. Like, and it's like, I feel like everyone's always like, there's all this copyright and IP and this and that or authenticity. And it's just like, I think we need to stop seeing this as this like egotistical thing of like, oh, the creative genius, the lone creative genius or this or that. Because it's like, I think art shouldn't be about that. I think art is something that sort of brings humanity together. And it's also, art is also kind of the collective memory of humans. It's like, we don't give a fuck about whatever ancient Egypt, like how much grain got sent that day and sending the records and like, you know, like who went where and, you know, how many shields needed to be produced for this. Like we just remember their art. And it's like, you know, it's like in our day to day life, there's all this stuff that seems more important than art because it helps us function and survive. But when all this is gone, like the only thing that's really gonna be left is the art. The technology will be obsolete. That's so fascinating. Like the humans will be dead. That is true. A good compression of human history is the art we've generated across the different centuries, the different millennia. So when the aliens come. When the aliens come, they're gonna find the hieroglyphics and the pyramids. I mean, art could be broadly defined. They might find like the engineering marvels, the bridges, the rockets, the. I guess I sort of classify though. Architecture is art too. I consider engineering in those formats to be art, for sure. It sucks that like digital art is easier to delete. So if there's an apocalypse, a nuclear war, that can disappear. Yes. And the physical. There's something still valuable about the physical manifestation of art. That sucks that like music, for example, has to be played by somebody. Yeah, I do think we should have a foundation type situation where we like, you know how we have like seed banks up in the north and stuff? Like we should probably have like a solar powered or geothermal little bunker that like has all human knowledge. You mentioned Daniel Ek and Spotify. What do you think about that as an artist? What's Spotify? Is that empowering? To me, Spotify as a consumer is super exciting. It makes it easy for me to access music from all kinds of artists, get to explore all kinds of music, make it super easy to sort of curate my own playlist and have fun with all that. It was so liberating to let go. You know, I used to collect, you know, albums and CDs and so on, like horde albums. Yeah. Like they matter. But the reality you could, you know, that was really liberating that I could let go of that. And letting go of the albums you're kind of collecting allows you to find new music, exploring new artists and all that kind of stuff. But I know from a perspective of an artist that could be, like you mentioned, competition could be a kind of constraint because there's more and more and more artists on the platform. I think it's better that there's more artists. I mean, again, this might be propaganda because this is all from a conversation with Daniel Ek. So this could easily be propaganda. We're all a victim of somebody's propaganda. So let's just accept this. But Daniel Ek was telling me that, you know, to understand really difficult concepts just in a very different way, like an emotional intelligence about something deep within? Oh yeah, no, like if X hurts, like if X bites me really hard and I'm like, ow, like he gets, he's sad. He's like sad if he hurts me by accident. Yeah. Which he's huge, so he hurts me a lot by accident. Yeah, that's so interesting that that mind emerges and he and children don't really have memory of that time. So we can't even have a conversation with them about it. Yeah, I just thank God they don't have a memory of this time because like, think about like, I mean with our youngest baby, like it's like, I'm like, have you read the sci fi short story, I Have No Mouth But I Must Scream? Good title, no. Oh man, I mean, you should read that. I Have No Mouth But I Must Scream. I hate getting into this Rocco's Basilisk shit. It's kind of a story about the, about like an AI that's like torturing someone in eternity and they have like no body. The way they describe it, it sort of sounds like what it feels like, like being a baby, like you're conscious and you're just getting inputs from everywhere and you have no muscles and you're like jelly and you like can't move and you try to like communicate, but you can't communicate and we're, and like, you're just like in this like hell state. I think it's good we can't remember that. Like my little baby is just exiting that, like she's starting to like get muscles and have more like autonomy, but like watching her go through the opening phase, I was like, I was like, this does not seem good. Oh, you think it's kind of like. Like I think it sucks. I think it might be really violent. Like violent, mentally violent, psychologically violent. Consciousness emerging, I think is a very violent thing. I never thought about that. I think it's possible that we all carry quite a bit of trauma from it that we don't, I think that would be a good thing to study because I think if, I think addressing that trauma, like, I think that might be. Oh, you mean like echoes of it are still there in the shadows somewhere. I think it's gotta be, I feel this, this help, the helplessness, the like existential and that like fear of being in like an unknown place bombarded with inputs and being completely helpless, like that's gotta be somewhere deep in your brain and that can't be good for you. What do you think consciousness is? This whole conversation has impossibly difficult questions. What do you think it is? Debbie said this is like so hard. Yeah, we talked about music for like two minutes. All right. No, I'm so, I'm just over music. I'm over music. Yeah, I still like it. It has its purpose. No, I love music. I mean, music's the greatest thing ever. It's my favorite thing. But I just like every interview is like, what is your process? Like, I don't know. I'm just done. I can't do anything. I do want to ask you about Able to Live. Oh, I'll tell you about Ableton because Ableton's sick. No one has ever asked about Ableton though. Yeah, well, because I just need tech support mainly. I can help you. I can help you with your Ableton tech. Anyway, from Ableton back to consciousness. What do you, do you think this is a thing that only humans are capable of? Can robots be conscious? Can, like when you think about entities, you think there's aliens out there that are conscious? Like is conscious, what is consciousness? There's this Terrence McKenna quote that I've found that I fucking love. Am I allowed to swear on here? Yes. Nature loves courage. You make the commitment and nature will respond to that commitment by removing impossible obstacles. Dream the impossible dream and the world will not grind you under. It will lift you up. This is the trick. This is what all these teachers and philosophers who really counted, who really touched the alchemical gold, this is what they understood. This is the shamanic dance in the waterfall. This is how magic is done. By hurling yourself into the abyss and discovering it's a feather bed. Yeah. And for this reason, I do think there are no technological limits. I think like what is already happening here, this is like impossible. This is insane. And we've done this in a very limited amount of time. And we're accelerating the rate at which we're doing this. So I think digital consciousness, it's inevitable. And we may not be able to even understand what that means, but I like hurling yourself into the abyss. So we're surrounded by all this mystery and we just keep hurling ourselves into it, like fearlessly and keep discovering cool shit. Yeah. Like, I just think it's like, like who even knows if the laws of physics, the laws of physics are probably just the current, like as I was saying, speed of light is the current render rate. It's like, if we're in a simulation, they'll be able to upgrade that. Like I sort of suspect when we made the James Webb telescope, like part of the reason we made that is because we had an upgrade, you know? And so now more of space has been rendered so we can see more of it now. Yeah, but I think humans are super, super, super limited cognitively. So I wonder if we'll be allowed to create more intelligent beings that can see more of the universe as their render rate is upgraded. Maybe we're cognitively limited. Everyone keeps talking about how we're cognitively limited and AI is gonna render us obsolete, but it's like, you know, like this is not the same thing as like an amoeba becoming an alligator. Like, it's like, if we create AI, again, that's intelligent design. That's literally all religions are based on gods that create consciousness. Like we are God making. Like what we are doing is incredibly profound. And like, even if we can't compute, even if we're so much worse than them, like just like unfathomably worse than like, you know, an omnipotent kind of AI, it's like we, I do not think that they would just think that we are stupid. I think that they would recognize the profundity of what we have accomplished. Are we the gods or are they the gods in our personality? I mean, we're kind of the guy. It's complicated. It's complicated. Like we're. But they would acknowledge the value. Well, I hope they acknowledge the value of paying respect to the creative ancestors. I think they would think it's cool. I think if curiosity is a trait that we can quantify and put into AI, then I think if AI are curious, then they will be curious about us and they will not be hateful or dismissive of us. They might, you know, see us as, I don't know. It's like, I'm not like, oh, fuck these dogs. Let's just kill all the dogs. I love dogs. Dogs have great utility. Dogs like provide a lot of. We make friends with them. We have a deep connection with them. We anthropomorphize them. Like we have a real love for dogs, for cats and so on for some reason, even though they're intellectually much less than us. And I think there is something sacred about us because it's like, if you look at the universe, like the whole universe is like cold and dead and sort of robotic. And it's like, you know, AI intelligence, you know, it's kind of more like the universe. It's like cold and you know, logical and you know, abiding by the laws of physics and whatever. But like, we're this like loosey goosey, weird art thing that happened. And I think it's beautiful. And like, I think even if we, I think one of the values, if consciousness is a thing that is most worth preserving, which I think is the case, I think consciousness, I think if there's any kind of like religious or spiritual thing, it should be that consciousness is sacred. Like, then, you know, I still think even if AI render us obsolete and we, climate change, it's too bad and we get hit by a comet and we don't become a multi planetary species fast enough, but like AI is able to populate the universe. Like I imagine, like if I was an AI, I would find more planets that are capable of hosting biological life forms and like recreate them. Because we're fun to watch. Yeah, we're fun to watch. Yeah, but I do believe that AI can have some of the same magic of consciousness within it. Because consciousness, we don't know what it is. So, you know, there's some kind of. Or it might be a different magic. It might be like a strange, a strange, different. Right. Because they're not gonna have hormones. Like I feel like a lot of our magic is hormonal kind of. I don't know, I think some of our magic is the limitations, the constraints. And within that, the hormones and all that kind of stuff, the finiteness of life, and then we get given our limitations, we get to come up with creative solutions of how to dance around those limitations. We partner up like penguins against the cold. We fall in love, and then love is ultimately some kind of, allows us to delude ourselves that we're not mortal and finite, and that life is not ultimately, you live alone, you're born alone, you die alone. And then love is like for a moment or for a long time, forgetting that. And so we come up with all these creative hacks that make life like fascinatingly fun. Yeah, yeah, yeah, fun, yeah. And then AI might have different kinds of fun. Yes. And hopefully our funs intersect once in a while. I think there would be a little intersection of the fun. Yeah. Yeah. What do you think is the role of love in the human condition? I think. Why, is it useful? Is it useful like a hack, or is this like fundamental to what it means to be human, the capacity to love? I mean, I think love is the evolutionary mechanism that is like beginning the intelligent design. Like I was just reading about, do you know about Kropotkin? He's like an anarchist, like old Russian anarchist. I live next door to Michael Malice. I don't know if you know who that is. He's an anarchist. He's a modern day anarchist. Okay. Anarchists are fun. I'm kind of getting into anarchism a little bit. This is probably not a good route to be taking, but. Oh no, I think if you're, listen, you should expose yourself to ideas. There's no harm to thinking about ideas. I think anarchists challenge systems in interesting ways, and they think in interesting ways. It's just as good for the soul. It's like refreshes your mental palette. I don't think we should actually, I wouldn't actually ascribe to it, but I've never actually gone deep on anarchy as a philosophy, so I'm doing. You should still think about it though. When you read, when you listen, because I'm reading about the Russian Revolution a lot, and there was the Soviets and Lenin and all that, but then there was Kropotkin and his anarchist sect, and they were sort of interesting because he was kind of a technocrat actually. He was like, women can be more equal if we have appliances. He was really into using technology to reduce the amount of work people had to do. But so Kropotkin was a biologist or something. He studied animals. And he was really at the time like, I think it's Nature magazine. I think it might've even started as a Russian magazine, but he was publishing studies. Everyone was really into Darwinism at the time and survival of the fittest, and war is the mechanism by which we become better. And it was this real cementing this idea in society that violence kill the weak, and that's how we become better. And then Kropotkin was kind of interesting because he was looking at instances, he was finding all these instances in nature where animals were like helping each other and stuff. And he was like, actually love is a survival mechanism. Like there's so many instances in the animal kingdom where like cooperation and like helping weaker creatures and all this stuff is actually an evolutionary mechanism. I mean, you even look at child rearing. Like child rearing is like immense amounts of just love and goodwill. And just like, there's no immediate, you're not getting any immediate feedback of like winning. It's not competitive. It's literally, it's like we actually use love as an evolutionary mechanism just as much as we use war. And I think we've like missing the other part and we've reoriented, we've culturally reoriented like science and philosophy has oriented itself around Darwinism a little bit too much. And the Kropotkin model, I think is equally valid. Like it's like cooperation and love and stuff is just as essential for species survival and evolution. It should be a more powerful survival mechanism in the context of evolution. And it comes back to like, we think engineering is so much more important than motherhood, but it's like, if you lose the motherhood, the engineering means nothing. We have no more humans. It's like, I think our society should, the survival of the, the way we see, we conceptualize evolution should really change to also include this idea, I guess. Yeah, there's some weird thing that seems irrational that is also core to what it means to be human. So love is one such thing. They could make you do a lot of irrational things, but that depth of connection and that loyalty is a powerful thing. Are they irrational or are they rational? Like, it's like, is, you know, maybe losing out on some things in order to like keep your family together or in order, like, it's like, what are our actual values? Well, right, I mean, the rational thing is if you have a cold economist perspective, you know, motherhood or sacrificing your career for love, you know, in terms of salary, in terms of economic wellbeing, in terms of flourishing of you as a human being, that could be seen on some kind of metrics as a irrational decision, suboptimal decision, but there's the manifestation of love could be the optimal thing to do. There's a kind of saying, save one life, save the world. That's the thing that doctors often face, which is like. Well, it's considered irrational because the profit model doesn't include social good. Yes, yeah. So if a profit model includes social good, then suddenly these would be rational decisions. Might be difficult to, you know, it requires a shift in our thinking about profit and might be difficult to measure social good. Yes, but we're learning to measure a lot of things. Yeah, digitizing a lot of things. Where we're actually, you know, quantifying vision and stuff. Like we're like, you know, like you go on Facebook and they can, like Facebook can pretty much predict our behaviors. Like we're, a surprising amount of things that seem like mysterious consciousness soul things have been quantified at this point. So surely we can quantify these other things. Yeah. But as more and more of us are moving the digital space, I wanted to ask you about something. From a fan perspective, I kind of, you know, you as a musician, you as an online personality, it seems like you have all these identities and you play with them. One of the cool things about the internet, it seems like you can play with identities. So as we move into the digital world more and more, maybe even in the so called metaverse. I mean, I love the metaverse and I love the idea, but like the way this has all played out didn't go well and people are mad about it. And I think we need to like. I think that's temporary. I think it's temporary. Just like, you know how all the celebrities got together and sang the song Imagine by Jeff Leonard and everyone started hating the song Imagine. I'm hoping that's temporary because it's a damn good song. So I think it's just temporary. Like once you actually have virtual worlds, whatever they're called metaverse or otherwise, it becomes, I don't know. Well, we do have virtual worlds. Like video games, Elden Ring. Have you played Elden Ring? You haven't played Elden Ring? I'm really afraid of playing that game. Literally amazed. It looks way too fun. It looks I would wanna go there and stay there forever. It's yeah, so fun. It's so nice. Oh man, yeah. So that's the, yeah, that's a metaverse. That's a metaverse, but you're not really, how immersive is it in the sense that, does the three dimension like virtual reality integration necessary? Can we really just take our, close our eyes and kind of plug in in the 2D screen and become that other being for time and really enjoy that journey that we take? And we almost become that. You're no longer C, I'm no longer Lex, you're that creature, whatever the hell it is in that game. Yeah, that is that. I mean, that's why I love those video games. I really do become those people for a time. But like, it seems like with the idea of the metaverse, the idea of the digital space, well, even on Twitter, you get a chance to be somebody for prolonged periods of time like across a lifespan. You know, you have a Twitter account for years, for decades and you're that person. I don't know if that's a good thing. I feel very tormented by it. By Twitter specifically. By social media representation of you. I feel like the public perception of me has gotten so distorted that I find it kind of disturbing. It's one of the things that's disincentivizing me from like wanting to keep making art because I'm just like, I've completely lost control of the narrative. And the narrative is, some of it is my own stupidity, but a lot, like some of it has just been like hijacked by forces far beyond my control. I kind of got in over my head in things. Like I'm just a random Indian musician, but I just got like dragged into geopolitical matters and like financial, like the stock market and shit. And so it's just like, it's just, there are very powerful people who have at various points in time had very vested interest in making me seem insane and I can't fucking fight that. And I just like, people really want their celebrity figures to like be consistent and stay the same. And like people have a lot of like emotional investment in certain things. And like, first of all, like I'm like artificially more famous than I should be. Isn't everybody who's famous artificially famous? No, but like I should be like a weird niche indie thing. And I make pretty challenging, I do challenging weird fucking shit a lot. And I accidentally by proxy got like foisted into sort of like weird celebrity culture, but like I cannot be media trained. They have put me through so many hours of media training. I would love to see BF fly in that wall. I can't do, like when I do, I try so hard and I like learn this thing and I like got it. And I'm like, I got it, I got it, I got it. But I just can't stop saying, like my mouth just says things like, and it's just like, and I just do, I just do things. I just do crazy things. Like I'm, I just, I need to do crazy things. And it's just, I should not be, it's too jarring for people and the contradictory stuff. And then all the by association, like, you know, it's like I'm in a very weird position and my public image, the avatar of me is now this totally crazy thing that is so lost from my control. So you feel the burden of the avatar having to be static. So the avatar on Twitter or the avatar on Instagram on these social platforms is as a burden. It becomes like, cause like people don't want to accept a changing avatar, a chaotic avatar. Avatar is a stupid shit sometimes. They think the avatar is morally wrong or they think the avatar, and maybe it has been, and like I question it all the time. Like, I'm like, like, I don't know if everyone's right and I'm wrong. I don't know, like, but you know, a lot of times people ascribe intentions to things, the worst possible intentions. At this point, people think I'm, you know, but which is fine. All kinds of words, yes. Yes, and it's fine. I'm not complaining about it, but I'm just, it's a curiosity to me that we live these double, triple, quadruple lives and I have this other life that is like more people know my other life than my real life, which is interesting. Probably, I mean, you too, I guess, probably. Yeah, but I have the luxury. So we have all different, you know, like I don't know what I'm doing. There is an avatar and you're mediating who you are through that avatar. I have the nice luxury, not the luxury, maybe by intention of not trying really hard to make sure there's no difference between the avatar and the private person. Do you wear a suit all the time? Yeah. You do wear a suit? Not all the time. Recently, because I get recognized a lot, I have to not wear the suit to hide. I'm such an introvert, I'm such a social anxiety and all that kind of stuff, so I have to hide away. I love wearing a suit because it makes me feel like I'm taking the moment seriously. Like I'm, I don't know. It makes me feel like a weirdo in the best possible way. Suits feel great, every time I wear a suit, I'm like, I don't know why I'm not doing this more. Fashion in general, if you're doing it for yourself, I don't know, it's a really awesome thing. But yeah, I think there is definitely a painful way to use social media and an empowering way. And I don't know if any of us know which is which. So we're trying to figure that out. Some people, I think Doja Cat is incredible at it. Incredible, like just masterful. I don't know if you like follow that. So okay, so not taking anything seriously, joking, absurd, humor, that kind of thing. I think Doja Cat might be like the greatest living comedian right now. Like I'm more entertained by Doja Cat than actual comedians. Like she's really fucking funny on the internet. She's just great at social media. It's just, you know. Yeah, the nature of humor, like humor on social media is also a beautiful thing, the absurdity. The absurdity. And memes, like I just wanna like take a moment. I love, like when we're talking about art and credit and authenticity, I love that there's this, I mean now memes are like, they're no longer, like memes aren't like new, but it's still this emergent art form that is completely egoless and anonymous and we just don't know who made any of it. And it's like the forefront of comedy and it's just totally anonymous and it just feels really beautiful. It just feels like this beautiful collective human art project that's like this like decentralized comedy thing that just makes memes add so much to my day and many people's days. And it's just like, I don't know. I don't think people ever, I don't think we stop enough and just appreciate how sick it is that memes exist. Because also making a whole brand new art form in like the modern era that's like didn't exist before. Like, I mean they sort of existed, but the way that they exist now as like this like, you know, like me and my friends, like we joke that we go like mining for memes or farming for memes, like a video game and like meme dealers and like whatever. Like it's, you know, it's this whole, memes are this whole like new comedic language. Well, it's this art form. The interesting thing about it is that lame people seem to not be good at memes. Like corporate can't infiltrate memes. Yeah, they really can't. They try, they could try. But it's like, it's weird cause like. They try so hard and every once in a while, I'm like fine, like you got a good one. I think I've seen like one or two good ones, but like, yeah, they really can't. Cause they're even, corporate is infiltrating web three. It's making me really sad, but they can't infiltrate the memes. And I think there's something really beautiful about that. That gives power, that's why Dogecoin is powerful. It's like, all right, I'm gonna F you to sort of anybody who's trying to centralize, is trying to control the rich people that are trying to roll in and control this, control the narrative. Wow, I hadn't thought about that, but. How would you fix Twitter? How would you fix social media for your own? Like you're an optimist, you're a positive person. There's a bit of a cynicism that you have currently about this particular little slice of humanity. I tend to think Twitter could be beautiful. I'm not that cynical about it. I'm not that cynical about it. I actually refuse to be a cynic on principle. Yes. I was just briefly expressing some personal pathos. Personal stuff. It was just some personal pathos, but like, like. Just to vent a little bit, just to speak. I don't have cancer, I love my family. I have a good life. That is, if that is my biggest, one of my biggest problems. Then it's a good life. Yeah, you know, that was a brief, although I do think there are a lot of issues with Twitter just in terms of like the public mental health, but due to my proximity to the current dramas, I honestly feel that I should not have opinions about this because I think that if Elon ends up getting Twitter, that is a, being the arbiter of truth or public discussion, that is a responsibility. I do not, I am not qualified to be responsible for that. And I do not want to say something that might like dismantle democracy. And so I just like, actually, I actually think I should not have opinions about this because I truly am not, I don't want to have the wrong opinion about this. And I think I'm too close to the actual situation wherein I should not have, I have thoughts in my brain, but I think I am scared by my proximity to this situation. Isn't that crazy that a few words that you could say could change world affairs and hurt people? I mean, that's the nature of celebrity at a certain point that you have to be, you have to a little bit, a little bit, not so much that it destroys you or puts too much constraints, but you have to a little bit think about the impact of your words. I mean, we as humans, you talk to somebody at a bar, you have to think about the impact of your words. Like you can say positive things, you can say negative things, you can affect the direction of one life. But on social media, your words can affect the direction of many lives. That's crazy. It's a crazy world to live in. It's worthwhile to consider that responsibility, take it seriously. Sometimes just like you did choose kind of silence, choose sort of respectful. Like I do have a lot of thoughts on the matter. I'm just, I don't, if my thoughts are wrong, this is one situation where the stakes are high. You mentioned a while back that you were in a cult that's centered around bureaucracy, so you can't really do anything because it involves a lot of paperwork. And I really love a cult that's just like Kafkaesque. Yes. Just like. I mean, it was like a joke, but it was. I know, but I love this idea. The Holy Rain Empire. Yeah, it was just like a Kafkaesque pro bureaucracy cult. But I feel like that's what human civilization is, is that, because when you said that, I was like, oh, that is kind of what humanity is, is this bureaucracy cult. I do, yeah, I have this theory. I really think that we really, bureaucracy is starting to kill us. And I think like we need to reorient laws and stuff. Like, I think we just need sunset clauses on everything. Like, I think the rate of change in culture is happening so fast and the rate of change in technology and everything is happening so fast. It's like, when you see these hearings about like social media and Cambridge Analytica and everyone talking, it's like, even from that point, so much technological change has happened from like those hearings. And it's just like, we're trying to make all these laws now about AI and stuff. I feel like we should be updating things like every five years. And like one of the big issues in our society right now is we're just getting bogged down by laws and it's making it very hard to change things and develop things. In Austin, I don't wanna speak on this too much, but like one of my friends is working on a housing bill in Austin to try to like prevent like a San Francisco situation from happening here because obviously we're getting a little mini San Francisco here, like housing prices are skyrocketing, it's causing massive gentrification. This is really bad for anyone who's not super rich. Like, there's so much bureaucracy. Part of the reason this is happening is because you need all these permits to build. It takes like years to get permits to like build anything. It's so hard to build and so there's very limited housing and there's a massive influx of people. And it's just like, you know, this is a microcosm of like problems that are happening all over the world where it's just like, we're dealing with laws that are like 10, 20, 30, 40, 100, 200 years old and they are no longer relevant and it's just slowing everything down and causing massive social pain. Yeah, but it's like, it's also makes me sad when I see politicians talk about technology and when they don't really get it. But most importantly, they lack curiosity and like that like inspired excitement about like how stuff works and all that stuff. They're just like, they see, they have a very cynical view of technology. It's like tech companies are just trying to do evil on the world from their perspective and they have no curiosity about like how recommender systems work or how AI systems work, natural language processing, how robotics works, how computer vision works, you know. They always take the most cynical possible interpretation of what technology would be used and we should definitely be concerned about that but if you're constantly worried about that and you're regulating based on that, you're just going to slow down all the innovation. I do think a huge priority right now is undoing the bad energy surrounding the emergence of Silicon Valley. Like I think that like a lot of things were very irresponsible during that time and you know, like even just this current whole thing with Twitter and everything, it's like there has been a lot of negative outcomes from the sort of technocracy boom but one of the things that's happening is that like it's alienating people from wanting to care about technology and I actually think technology is probably some of the better, probably the best. I think we can fix a lot of our problems more easily with technology than with you know, fighting the powers that be as a you know, not to go back to the Star Wars quote or the Buckminster Fuller quote. Let's go to some dark questions. If we may for time, what is the darkest place you've ever gone in your mind? Is there a time, a period of time, a moment that you remember that was difficult for you? I mean, when I was 18, my best friend died of a heroin overdose and it was like my, and then shortly after that, one of my other best friends committed suicide and that sort of like coming into adulthood, dealing with two of the most important people in my life dying in extremely disturbing violent ways was a lot. That was a lot. Do you miss them? Yeah, definitely miss them. Did that make you think about your own life? About the finiteness of your own life? The places your mind can go? Did you ever in the distance, far away contemplate just your own death? Or maybe even taking your own life? Oh never, oh no. I'm so, I love my life. I cannot fathom suicide. I'm so scared of death. I haven't, I'm too scared of death. My manager, my manager's like the most Zen guy. My manager's always like, you need to accept death. You need to accept death. And I'm like, look, I can do your meditation. I can do the meditation, but I cannot accept death. I like, I will fight, I'm terrified of death. I will like fight. Although I actually think death is important. I recently went to this meeting about immortality and in the process of. That's the actual topic of the meeting? I'm sorry. No, no, it was this girl. It was a bunch of people working on like anti aging stuff. It was like some like seminary thing about it. And I went in really excited. I was like, yeah, like, okay, like, what do you got? Like, how can I live for 500 years or a thousand years? And then like over the course of the meeting, like it was sort of like, right. It was like two or three days after the Russian invasion started. And I was like, man, like, what if Putin was immortal? Like, what if I'm like, man, maybe immortality, is not good. I mean, like if you get into the later Dune stuff, the immortals cause a lot of problem. Cause as we were talking about earlier with the music and like brains calcify, like good people could become immortal, but bad people could become immortal. But I also think even the best people power corrupts and power alienates you from like the common human experience and. Right, so the people that get more and more powerful. Even the best people who like, whose brains are amazing, like I think death might be important. I think death is part of, you know, like I think with AI one thing we might want to consider, I don't know, when I talk about AI, I'm such not an expert and probably everyone has all these ideas and they're already figured out. But when I was talking. Nobody is an expert in anything. See, okay, go ahead. But when I. You were talking about. Yeah, but I like, it's just like, I think some kind of pruning. But it's a tricky thing because if there's too much of a focus on youth culture, then you don't have the wisdom. So I feel like we're in a tricky, we're in a tricky moment right now in society where it's like, we've really perfected living for a long time. So there's all these really like old people who are like really voting against the wellbeing of the young people, you know? And like, it's like there shouldn't be all this student dead and we need like healthcare, like universal healthcare and like just voting against like best interests. But then you have all these young people that don't have the wisdom that are like, yeah, we need communism and stuff. And it's just like, like literally I got canceled at one point for, I ironically used a Stalin quote in my high school yearbook, but it was actually like a diss against my high school. I saw that. Yeah, and people were like, you used to be a Stalinist and now you're a class traitor and it's like, it's like, oh man, just like, please Google Stalin. Please Google Stalin. Like, you know. Ignoring the lessons of history, yes. And it's like, we're in this really weird middle ground where it's like, we are not finding the happy medium between wisdom and fresh ideas and they're fighting each other. And it's like, like really, like what we need is like the fresh ideas and the wisdom to be like collaborating. And it's like. What the fighting in a way is the searching for the happy medium. And in a way, maybe we are finding the happy medium. Maybe that's what the happy medium looks like. And for AI systems, there has to be, it's, you know, you have the reinforcement learning, you have the dance between exploration and exploitation, sort of doing crazy stuff to see if there's something better than what you think is the optimal and then doing the optimal thing and dancing back and forth from that. You would, Stuart Russell, I don't know if you know that, is AI guy with, thinks about sort of how to control super intelligent AI systems. And his idea is that we should inject uncertainty and sort of humility into AI systems that they never, as they get wiser and wiser and wiser and more intelligent, they're never really sure. They always doubt themselves. And in some sense, when you think of young people, that's a mechanism for doubt. It's like, it's how society doubts whether the thing it has converged towards is the right answer. So the voices of the young people is a society asking itself a question. The way I've been doing stuff for the past 50 years, maybe it's the wrong way. And so you can have all of that within one AI system. I also think, though, that we need to, I mean, actually, that's actually really interesting and really cool. But I also think there's a fine balance of, I think we maybe also overvalue the idea that the old systems are always bad. And I think there are things that we are perfecting and we might be accidentally overthrowing things that we actually have gotten to a good point. Just because we value disruption so much and we value fighting against the generations before us so much that there's also an aspect of, sometimes we're taking two steps forward, one step back because, okay, maybe we kind of did solve this thing and now we're like fucking it up, you know? And so I think there's like a middle ground there too. Yeah, we're in search of that happy medium. Let me ask you a bunch of crazy questions, okay? All right. You can answer in a short way or in a long way. What's the scariest thing you've ever done? These questions are gonna be ridiculous. Something tiny or something big. Something big, skydiving or touring your first record, going on this podcast. I've had two crazy brushes, like really scary brushes with death where I randomly got away on scay. I don't know if I should talk about those on here. Well, I don't know. I think I might be the luckiest person alive though. Like this might be too dark for a podcast though. I feel like, I don't know if this is like good content for a podcast. I don't know what is good content. It might hijack. Here's a safer one. I mean, having a baby really scared me. Before. Just the birth process. Surgery, like just having a baby is really scary. So just like the medical aspect of it, not the responsibility. Were you ready for the responsibility? Did you, were you ready to be a mother? All the beautiful things that comes with motherhood that you were talking about. All the changes and all that, were you ready for that? Or did you feel ready for that? No, I think it took about nine months to start getting ready for it. And I'm still getting more ready for it because now you keep realizing more things as they start getting. As the consciousness grows. And stuff you didn't notice with the first one, now that you've seen the first one older, you're noticing it more. Like the sort of like existential horror of coming into consciousness with Baby Y or Baby Sailor Mars or whatever. She has like so many names at this point that it's, we really need to probably settle on one. If you could be someone else for a day, someone alive today, but somebody you haven't met yet, who would you be? Would I be modeling their brain state or would I just be in their body? You can choose the degree to which you're modeling their brain state. Cause you can still take a third person perspective and realize, you have to realize that you're. Can they be alive or can it be dead? No, oh. They would be brought back to life, right? If they're dead. Yeah, you can bring people back. Definitely Hitler or Stalin. I wanna understand evil. You would need to, oh, to experience what it feels like. I wanna be in their brain feeling what they feel. I might change you forever returning from that. Yes, but I think it would also help me understand how to prevent it and fix it. That might be one of those things, once you experience it, it'll be a burden to know it. Cause you won't be able to transfer that. Yeah, but a lot of things are burdens. But it's a useful burden. But it's a useful burden, yeah. That for sure, I wanna understand evil and psychopathy and that. I have all these fake Twitter accounts where I go into different algorithmic bubbles to try to understand. I'll keep getting in fights with people and realize we're not actually fighting. I think we used to exist in a monoculture before social media and stuff. We kinda all got fed the same thing. So we were all speaking the same cultural language. But I think recently, one of the things that we aren't diagnosing properly enough with social media is that there's different dialects. There's so many different dialects of Chinese. There are now becoming different dialects of English. I am realizing there are people who are saying the exact same things, but they're using completely different verbiage. And we're punishing each other for not using the correct verbiage. And we're completely misunderstanding. People are just misunderstanding what the other people are saying. And I just got in a fight with a friend about anarchism and communism and shit for two hours. And then by the end of a conversation, and then she'd say something, and I'm like, but that's literally what I'm saying. And she was like, what? And then I was like, fuck, we've different, I'm like, our English, the way we are understanding terminology is like drastically, like our algorithm bubbles are creating mini dialects. Of how language is interpreted, how language is used. That's so fascinating. And so we're like having these arguments that we do not need to be having. And there's polarization that's happening that doesn't need to be happening because we've got these like algorithmically created dialects occurring. Plus on top of that, there's also different parts of the world that speak different languages. So there's literally lost in translation kind of communication. I happen to know the Russian language and just know how different it is. Then the English language. And I just wonder how much is lost in a little bit of. Man, I actually, cause I have a question for you. I have a song coming out tomorrow with I Speak Who Are A Russian Band. And I speak a little bit of Russian and I was looking at the title and the title in English doesn't match the title in Russian. I'm curious about this. Cause look, it says the title in English is Last Day. And then the title in Russian is New Day. My pronunciation sucks. New Day. Like what? Like a new day. A new day. Yeah, new day, new day. Like it's two different meanings. Yeah, new day, yeah. Yeah, yeah, new day. New day, but last day. New day. So last day would be the last day. Yeah. Maybe they. Or maybe the title includes both the Russian and it's for. Maybe. Maybe it's for bilingual. But to be honest, Novodin sounds better than just musically. Like Novodin is new day. That's the current one. And Posledniy Den is the last day. I think Novodin. I don't like Novodin. But the meaning is so different. That's kind of awesome actually though. There's an explicit sort of contrast like that. If everyone on earth disappeared and it was just you left, what would your day look like? Like what would you do? Everybody's dead. As far as you. Are there corpses there? Well seriously, it's a big. Let me think through this. It's a big difference if there's just like birds singing versus if there's like corpses littering the street. Yeah, there's corpses everywhere, I'm sorry. It's, and you don't actually know what happened and you don't know why you survived. And you don't even know if there's others out there. But it seems clear that it's all gone. What would you do? What would I do? Listen, I'm somebody who really enjoys the moment, enjoys life. I would just go on like enjoying the inanimate objects. I would just look for food, basic survival. But most of it is just, listen, when I just, I take walks and I look outside and I'm just happy that we get to exist on this planet, to be able to breathe air. It's just all beautiful. It's full of colors, all of this kind of stuff. Just, there's so many things about life, your own life, conscious life that's fucking awesome. So I would just enjoy that. But also maybe after a few weeks, the engineer would start coming out, like wanna build some things. Maybe there's always hope searching for another human. Maybe. Probably searching for another human. Probably trying to get to a TV or radio station and broadcast something. That's interesting, I didn't think about that. So like really maximize your ability to connect with others. Yeah, like probably try to find another person. Would you be excited to see, to meet another person or terrified? Because, you know. I'd be excited. No matter what. Yeah, yeah, yeah, yeah. Being alone for the last however long of my life would be really bad. That's the one instance I might, I don't think I'd kill myself, but I might kill myself if I had to. So you love people. You love connection to other humans. Yeah. I kinda hate people too, but yeah. That's a love hate relationship. Yeah. I feel like we'd have a bunch of weird Nietzsche questions and stuff though. Oh yeah. Like I wonder, cause I'm like, when podcast, I'm like, is this interesting for people to just have like, or I don't know, maybe people do like this. When I listen to podcasts, I'm into like the lore, like the hard lore. Like I just love like Dan Carlin. I'm like, give me the facts. Just like, like the facts into my bloodstream. But you also don't know, like you're a fascinating mind to explore. So you don't realize as you're talking about stuff, the stuff you've taken for granted is actually unique and fascinating. The way you think. Not always what, like the way you reason through things is the fascinating thing to listen to. Because people kind of see, oh, there's other humans that think differently, that explore thoughts differently. That's the cool, that's also cool. So yeah, Dan Carlin retelling of history. By the way, his retelling of history is very, I think what's exciting is not the history, is his way of thinking about history. No, I think Dan Carlin is one of the people, like when, Dan Carlin is one of the people that really started getting me excited about like revolutionizing education. Because like Dan Carlin instilled, I already like really liked history, but he instilled like an obsessive love of history in me to the point where like now I'm fucking reading, like going to bed, reading like part four of The Rise and Fall of the Third Reich or whatever. Like I got like dense ass history, but like he like opened that door that like made me want to be a scholar of that topic. Like it's like, I feel like he's such a good teacher. He just like, you know, and it sort of made me feel like one of the things we could do with education is like find like the world's great, the teachers that like create passion for the topic because autodidactricism, I don't know how to say that properly, but like self teaching is like much faster than being lectured to. Like it's much more efficient to sort of like be able to teach yourself and then ask a teacher questions when you don't know what's up. But like, you know, that's why it's like in university and stuff, like you can learn so much more material so much faster because you're doing a lot of the learning on your own and you're going to the teachers for when you get stuck. But like these teachers that can inspire passion for a topic, I think that is one of the most invaluable skills in our whole species. Like, because if you can do that, then you, it's like AI, like AI is going to teach itself so much more efficiently than we can teach it. We just needed to get it to the point where it can teach itself. And then. It finds the motivation to do so, right? Yeah. So like you inspire it to do so. Yeah. And then it could teach itself. What do you make of the fact, you mentioned Rise and Fall of the Third Reich. I just. Have you read that? Yeah, I read it twice. You read it twice? Yes. Okay, so no one even knows what it is. Yeah. And I'm like, wait, I thought this was like a super poppin book. Super pop. Yeah, I'm not like that, I'm not that far in it. But it is, it's so interesting. Yeah, it's written by a person that was there, which is very important to kind of. You know, you start being like, how could this possibly happen? And then when you read Rise and Fall of the Third Reich, it's like, people tried really hard for this to not happen. People tried, they almost reinstated a monarchy at one point to try to stop this from happening. Like they almost like abandoned democracy to try to get this to not happen. At least the way it makes me feel is that there's a bunch of small moments on which history can turn. Yes. It's like small meetings. Yes. Human interactions. And it's both terrifying and inspiring because it's like, even just attempts at assassinating Hitler, like time and time again failed. And they were so close. Was it like Operation Valkyrie? Such a good. And then there's also the role of, that's a really heavy burden, which is from a geopolitical perspective, the role of leaders to see evil before it truly becomes evil, to anticipate it, to stand up to evil. Because evil is actually pretty rare in this world at a scale that Hitler was. We tend to, you know, in the modern discourse kind of call people evil too quickly. If you look at ancient history, like there was a ton of Hitlers. I actually think it's more the norm than, like again, going back to like my sort of intelligent design theory, I think one of the things we've been successfully doing in our slow move from survival of the fittest to intelligent design is we've kind of been eradicating, like if you look at like ancient Assyria and stuff, like that shit was like brutal and just like the heads on the, like brutal, like Genghis Khan just like genocide after genocide was like throwing plague bodies over the walls and decimating whole cities or like the Muslim conquests of like Damascus and shit. Just like people, cities used to get leveled all the fucking time. Okay, get into the Bronze Age collapse. It's basically, there was like almost like Roman level like society. Like there was like all over the world, like global trade, like everything was awesome through a mix of, I think a bit of climate change and then the development of iron because basically bronze could only come from this, the way to make bronze, like everything had to be funneled through this one Iranian mine. And so it's like, there was just this one supply chain and this is one of the things that makes me worried about supply chains and why I think we need to be so thoughtful about, I think our biggest issue with society right now, like the thing that is most likely to go wrong is probably supply chain collapse, because war, climate change, whatever, like anything that causes supply chain collapse, our population is too big to handle that. And like the thing that seems to cause Dark Ages is mass supply chain collapse. But the Bronze Age collapse happened like, it was sort of like this ancient collapse that happened where like literally like ancient Egypt, all these cities, everything just got like decimated, destroyed, abandoned cities, like hundreds of them. There was like a flourishing society, like we were almost coming to modernity and everything got leveled. And they had this mini Dark Ages, but it was just like, there's so little writing or recording from that time that like, there isn't a lot of information about the Bronze Age collapse, but it was basically equivalent to like medieval, the medieval Dark Ages. But it just happened, I don't know the years, but like thousands of years earlier. And then we sort of like recovered from the Bronze Age collapse, empire reemerged, writing and trade and everything reemerged. And then we of course had the more contemporary Dark Ages. And then over time, we've designed mechanism to lessen and lessen the capability for the destructive power centers to emerge. There's more recording about the more contemporary Dark Ages. So I think we have like a better understanding of how to avoid it, but I still think we're at high risk for it. I think that's one of the big risks right now. So the natural state of being for humans is for there to be a lot of Hitlers, which has gotten really good at making it hard for them to emerge. We've gotten better at collaboration and resisting the power, like authoritarians to come to power. We're trying to go country by country, like we're moving past this. We're kind of like slowly incrementally, like moving towards like not scary old school war stuff. And I think seeing it happen in some of the countries that at least nominally are like supposed to have moved past that, that's scary because it reminds us that it can happen like in the places that have moved supposedly, as hopefully moved past that. And possibly at a civilization level, like you said, supply chain collapse might make people resource constraint, might make people desperate, angry, hateful, violent, and drag us right back in. I mean, supply chain collapse is how, like the ultimate thing that caused the Middle Ages was supply chain collapse. It's like people, because people were reliant on a certain level of technology, like people, like you look at like Britain, like they had glass, like people had aqueducts, people had like indoor heating and cooling and like running water and like buy food from all over the world and trade and markets. Like people didn't know how to hunt and forage and gather. And so we're in a similar situation. We are not educated enough to survive without technology. So if we have a supply chain collapse that like limits our access to technology, there will be like massive starvation and violence and displacement and war. Like, you know, it's like, yeah. In my opinion, it's like the primary marker of like what a dark age is. Well, technology is kind of enabling us to be more resilient in terms of supply chain, in terms of, to all the different catastrophic events that happened to us. Although the pandemic has kind of challenged our preparedness for the catastrophic. What do you think is the coolest invention humans come up with? The wheel, fire, cooking meat. Computers. Computers. Freaking computers. Internet or computers? Which one? What do you think the? Previous technologies, I mean, may have even been more profound and moved us to a certain degree, but I think the computers are what make us homo tech now. I think this is what, it's a brain augmentation. And so it like allows for actual evolution. Like the computers accelerate the degree to which all the other technologies can also be accelerated. Would you classify yourself as a homo sapien or a homo techno? Definitely homo techno. So you're one of the earliest of the species. I think most of us are. Like, as I said, like, I think if you like looked at brain scans of us versus humans a hundred years ago, it would look very different. I think we are physiologically different. Just even the interaction with the devices has changed our brains. Well, and if you look at, a lot of studies are coming out to show that like, there's a degree of inherited memory. So some of these physiological changes in theory should be, we should be passing them on. So like that's, you know, that's not like a, an instance of physiological change that's gonna fizzle out. In theory, that should progress like to our offspring. Speaking of offspring, what advice would you give to a young person, like in high school, whether there be an artist, a creative, an engineer, any kind of career path, or maybe just life in general, how they can live a life they can be proud of? I think one of my big thoughts, and like, especially now having kids, is that I don't think we spend enough time teaching creativity. And I think creativity is a muscle like other things. And there's a lot of emphasis on, you know, learn how to play the piano. And then you can write a song or like learn the technical stuff. And then you can do a thing. But I think it's, like, I have a friend who's like world's greatest guitar player, like, you know, amazing sort of like producer, works with other people, but he's really sort of like, you know, he like engineers and records things and like does solos, but he doesn't really like make his own music. And I was talking to him and I was like, dude, you're so talented at music. Like, why don't you make music or whatever? And he was like, cause I got, I'm too old. I never learned the creative muscle. And it's like, you know, it's embarrassing. It's like learning the creative muscle takes a lot of failure. And it also sort of, if when you're being creative, you know, you're throwing paint at a wall and a lot of stuff will fail. So like part of it is like a tolerance for failure and humiliation. And that's somehow that's easier to develop when you're young or be persist through it when you're young. Everything is easier to develop when you're young. Yes. And the younger, the better. It could destroy you. I mean, that's the shitty thing about creativity. If, you know, failure could destroy you if you're not careful, but that's a risk worth taking. But also, but at a young age, developing a tolerance to failure is good. I fail all the time. Like I do stupid shit all the time. Like in public, in private, I get canceled for, I've make all kinds of mistakes, but I just like am very resilient about making mistakes. And so then like I do a lot of things that like other people wouldn't do. And like, I think my greatest asset is my creativity. And I like, I think pain, like tolerance to failure is just a super essential thing that should be taught before other things. Brilliant advice. Yeah, yeah. I wish everybody encouraged sort of failure more as opposed to kind of. Cause we like punish failure. We're like, no, like when we were teaching kids, we're like, no, that's wrong. Like that's, you know, like X keeps like will be like wrong. at the, because I, you know, when I met him, I came in all furious about Spotify and like I grilled him super hard. So I've got his answers here. But he was saying like at the sort of peak of the CD industry, there was like 20,000 artists making millions and millions of dollars. Like there was just like a very tiny kind of 1%. And Spotify has kind of democratized the industry because now I think he said there's about a million artists making a good living from Spotify. And when I heard that, I was like, honestly, I would rather make less money and have just like a decent living and have more artists be able to have that, even though I like, I wish it could include everyone, but. Yeah, that's really hard to argue with. YouTube is the same. It's YouTube's mission. They want to basically have as many creators as possible and make a living, some kind of living. And that's so hard to argue with. It's so hard. But I think there's better ways to do it. My manager, I actually wish he was here. Like I would have brought him up. My manager is building an app that can manage you. So it'll like help you organize your percentages and get your publishing and dah, dah, dah, dah, dah. So you can take out all the middlemen so you can have a much bigger, it'll just like automate it. So you can get. So automate the manager? Automate management, publishing, and legal, it can read, the app he's building can read your contract and like tell you about it. Because one of the issues with music right now, it's not that we're not getting paid enough, but it's that the art industry is filled with middlemen because artists are not good at business. And from the beginning, like Frank Sinatra, it's all mob stuff. Like it's the music industry is run by business people, not the artists and the artists really get very small cuts of like what they make. And so I think part of the reason I'm a technocrat, which I mean, your fans are gonna be technocrats. So no one's, they're not gonna be mad at me about this, but like my fans hate it when I say this kind of thing or the general public. They don't like technocrats. They don't like technocrats. Like when I watched Battle Angel Alita and they were like the Martian technocracy and I was like, yeah, Martian technocracy. And then they were like, and they're evil. And I was like, oh, okay. I was like, cause Martian technocracy sounds sick to me. Yeah, so your intuition as technocrats would create some kind of beautiful world. For example, what my manager's working on, if you can create an app that removes the need for a lawyer and then you could have smart contracts on the blockchain, removes the need for like management and organizing all this stuff, like can read your stuff and explain it to you, can collect your royalties, you know, like then the small amounts, the amount of money that you're getting from Spotify actually means a lot more and goes a lot farther. It can remove some of the bureaucracy, some of the inefficiencies that make life not as great as it could be. Yeah, I think the issue isn't that there's not enough. Like the issue is that there's inefficiency and I'm really into this positive sum mindset, you know, the win, win mindset of like, instead of, you know, fighting over the scraps, how do we make the, or worrying about scarcity, like instead of a scarcity mindset, why don't we just increase the efficiency and, you know, in that way. Expand the size of the pie. Let me ask you about experimentation. So you said, which is beautiful, being a musician is like having a conversation with all those that came before you. How much of creating music is like kind of having that conversation, trying to fit into the cultural trends and how much of it is like trying to, as much as possible, be an outsider and come up with something totally new. It's like when you're thinking, when you're experimenting, are you trying to be totally different, totally weird? Are you trying to fit in? Man, this is so hard because I feel like I'm kind of in the process of semi retiring from music, so this is like my old brain. Yeah, bring it from like the shelf, put it on the table for a couple minutes, we'll just poke it. I think it's a bit of both because I think forcing yourself to engage with new music is really great for neuroplasticity. Like I think, you know, as people, part of the reason music is marketed at young people is because young people are very neuroplastic. So like if you're 16 to like 23 or whatever, it's gonna be really easy for you to love new music. And if you're older than that, it gets harder and harder and harder. And I think one of the beautiful things about being a musician is I just constantly force myself to listen to new music and I think it keeps my brain really plastic. And I think this is a really good exercise. I just think everyone should do this. You listen to new music and you hate it, I think you should just keep, force yourself to like, okay, well why do people like it? And like, you know, make your brain form new neural pathways and be more open to change. That's really brilliant actually. Sorry to interrupt, but like that exercise is really amazing to sort of embrace change, embrace sort of practice neuroplasticity. Because like that's one of the things, you fall in love with a certain band and you just kind of stay with that for the rest of your life and then you never understand the modern music. That's a really good exercise. Most of the streaming on Spotify is like classic rock and stuff. Like new music makes up a very small chunk of what is played on Spotify. And I think this is like not a good sign for us as a species. I think, yeah. So it's a good measure of the species open mindedness to change is how often you listen to new music. The brain, let's put the music brain back on the shelf. I gotta pull out the futurist brain for a second. In what wild ways do you think the future, say in like 30 years, maybe 50 years, maybe a hundred years will be different from our current way of life on earth? We can talk about augmented reality, virtual reality, maybe robots, maybe space travel, maybe video games, maybe genetic engineering. I can keep going. Cyborgs, aliens, world wars, maybe destructive nuclear wars, good and bad. When you think about the future, what are you imagining? What's the weirdest and the wildest it could be? Have you read Surface Detail by Iain Banks? Surface Detail is my favorite depiction of a, oh wow, you have to read this book. It's literally the greatest science fiction book possibly ever written. Iain Banks is the man, yeah, for sure. What have you read? Just the Player of Games. I read that titles can't be copyrighted so you can just steal them. And I was like, Player of Games, sick. Nice. Yeah, so you can name your album. Like I always wanted to. Romeo and Juliet or something. I always wanted to name an album War and Peace. Nice. Like that would be, like you. That is a good, that's a good, where have I heard that before? You can do that, like you could do that. Also things that are in the public domain. For people who have no clue, you do have a song called Player of Games. Yes, oh yeah. So Iain Banks, Surface Detail is in my opinion the best future that I've ever read about or heard about in science fiction. Basically there's the relationship with super intelligence, like artificial super intelligence is just, it's like great. I want to credit the person who coined this term because I love this term. And I feel like young women don't get enough credit in. Yeah, so if you go to Protopia Futures on Instagram, what is her name? Personalized donor experience at scale, our AI power donor experience. Monica Bealskite, I'm saying that wrong. And I'm probably gonna, I'm probably butchering this a bit, but Protopia is sort of, if utopia is unattainable, Protopia is sort of like, you know. Wow, that's an awesome Instagram, Protopia Futures. A great, a future that is, you know, as good as we can get. The future, positive future. AI, is this a centralized AI in Surface Detail or is it distributed? What kind of AI is it? They mostly exist as giant super ships, like sort of like the guild ships in Dune. Like they're these giant ships that kind of move people around and the ships are sentient and they can talk to all the passengers. And I mean, there's a lot of different types of AI in the Banksyan future, but in the opening scene of Surface Detail, there's this place called the Culture and the Culture is basically a Protopian future. And a Protopian future, I think, is like a future that is like, obviously it's not utopia, it's not perfect. And like, cause like striving for utopia, I think feels hopeless and it's sort of like, maybe not the best terminology to be using. So it's like, it's a pretty good place. Like mostly like, you know, super intelligence and biological beings exist fairly in harmony. There's not too much war. There's like as close to equality as you can get, you know, it's like approximately a good future. Like there's really awesome stuff. It's, and in the opening scene, this girl, she's born as a sex slave outside of the culture. So she's in a society that doesn't adhere to the cultural values. She tries to kill the guy who is her like master, but he kills her, but unbeknownst to her, when she was traveling on a ship through the culture Like he'll say like crazy things. Like X keeps being like, like bubble car, bubble car. And I'm like, and you know, I'm like, what's a bubble car? Like, but like, it doesn't like, but I don't want to be like, no, you're wrong. I'm like, you're thinking of weird, crazy shit. Like, I don't know what a bubble car is, but like. It's creating worlds and they might be internally consistent. And through that, you might discover something fundamental about this world. Yeah, or he'll like rewrite songs, like with words that he prefers. So like, instead of baby shark, he says baby car. It's like. Maybe he's onto something. Let me ask the big, ridiculous question. We were kind of dancing around it, but what do you think is the meaning of this whole thing we have here of human civilization, of life on earth, but in general, just life? What's the meaning of life? C. Have you, did you read Nova Scene yet? By James Lovelock? You're doing a lot of really good book recommendations here. I haven't even finished this, so I'm a huge fraud yet again. But like really early in the book, he says this amazing thing. Like, I feel like everyone's so sad and cynical. Like everyone's like the Fermi paradox and everyone. I just keep hearing people being like, fuck, what if we're alone? Like, oh no, ah, like, ah, ah. And I'm like, okay, but like, wait, what if this is the beginning? Like in Nova Scene, he says, this is not gonna be a correct, I can't like memorize quotes, but he says something like, what if our consciousness, like right now, like this is the universe waking up? Like what if instead of discovering the universe, this is the universe, like this is the evolution of the literal universe herself. Like we are not separate from the universe. Like this is the universe waking up. This is the universe seeing herself for the first time. Like this is. The universe becoming conscious. The first time we were a part of that. Yeah, cause it's like, we aren't separate from the universe. Like this could be like an incredibly sacred moment and maybe like social media and all this things, the stuff where we're all getting connected together. Like maybe these are the neurons connecting of the like collective super intelligence that is, Waking up. The, yeah, like, you know, it's like, maybe instead of something cynical or maybe if there's something to discover, like maybe this is just, you know, we're a blast assist of like some incredible kind of consciousness or being. And just like in the first three years of life or for human children, we'll forget about all the suffering that we're going through now. I think we'll probably forget about this. I mean, probably, you know, artificial intelligence will eventually render us obsolete. I don't think they'll do it in a malicious way, but I think probably we are very weak. The sun is expanding. Like, I don't know, like, hopefully we can get to Mars, but like, we're pretty vulnerable. And I, you know, like, I think we can coexist for a long time with AI and we can also probably make ourselves less vulnerable, but, you know, I just think consciousness, sentience, self awareness, like, I think this might be the single greatest like moment in evolution ever. And like, maybe this is, you know, the big, like the true beginning of life. And we're just, we're the blue green algae or we're like the single celled organisms of something amazing. The universe awakens and this is it. Yeah. Well, see, you're an incredible person. You're a fascinating mind. You should definitely do, your friend Liv mentioned that you guys were thinking of maybe talking. I would love it if you explored your mind in this kind of media more and more by doing a podcast with her or just in any kind of way. So you're an awesome person. It's an honor to know you. It's an honor to get to sit down with you late at night, which is like surreal. And I really enjoyed it. Thank you for talking today. Yeah, no, I mean, huge honor. I feel very underqualified to be here, but I'm a big fan. I've been listening to the podcast a lot and yeah, me and Liv would appreciate any advice and help and we're definitely gonna do that. So yeah. Anytime. Thank you. Cool, thank you. Thanks for listening to this conversation with Grimes. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Oscar Wilde. Yes, I'm a dreamer. For a dreamer is one who can only find her way by moonlight and her punishment is that she sees the dawn before the rest of the world. Thank you for listening and hope to see you next time. with him one day, a ship put a neural lace in her head and neural lace is sort of like, it's basically a Neuralink because life imitates art. It does indeed. It does indeed. So she wakes up and the opening scene is her memory has been uploaded by this neural lace when she has been killed. And now she gets to choose a new body. And this AI is interfacing with her recorded memory in her neural lace and helping her and being like, hello, you're dead. But because you had a neural lace, your memory's uploaded. Do you want to choose a new body? And you're going to be born here in the culture and like start a new life, which is just, that's like the opening. It's like so sick. And the ship is the super intelligence. All the ships are kind of super intelligence. But they still want to preserve a kind of rich, fulfilling experience for the humans. Yeah, like they're like friends with the humans. And then there's a bunch of ships that don't want to exist with biological beings, but they just have their own place like way over there. But they don't, they just do their own thing. They're not necessarily. So it's a pretty, this protopian existence is pretty peaceful. Yeah, I mean, and then, for example, one of the main fights in the book is they're fighting, there's these artificial hells that, and people don't think it's ethical to have artificial hell. Like basically when people do crime, they get sent, like when they die, their memory gets sent to an artificial hell and they're eternally tortured. And so, and then the way that society is deciding whether or not to have the artificial hell is that they're having these simulated, they're having like a simulated war. So instead of actual blood, you know, people are basically essentially fighting in a video game to choose the outcome of this. But they're still experiencing the suffering in this artificial hell or no? Can you experience stuff or? So the artificial hell sucks. And a lot of people in the culture want to get rid of the artificial hell. There's a simulated wars, are they happening in the artificial hell? So no, the simulated wars are happening outside of the artificial hell, between the political factions who are, so this political faction says we should have simulated hell to deter crime. And this political faction is saying, no, simulated hell is unethical. And so instead of like having, you know, blowing each other up with nukes, they're having like a giant Fortnite battle to decide this, which, you know, to me that's protopia. That's like, okay, we can have war without death. You know, I don't think there should be simulated hells. I think that is definitely one of the ways in which technology could go very, very, very, very wrong. So almost punishing people in a digital space or something like that. Yeah, like torturing people's memories. So either as a deterrent, like if you committed a crime, but also just for personal pleasure, if there's some sick, demented humans in this world. Dan Carlin actually has this episode of Hardcore History on painful attainment. Oh, that episode is fucked. It's dark, because he kind of goes through human history and says like, we as humans seem to enjoy, secretly enjoy or used to be openly enjoy sort of the torture and the death, watching the death and torture of other humans. I do think if people were consenting, we should be allowed to have gladiatorial matches. But consent is hard to achieve in those situations. It always starts getting slippery. Like it could be also forced consent, like it starts getting weird. There's way too much excitement. Like this is what he highlights. There's something about human nature that wants to see that violence. And it's really dark. And you hope that we can sort of overcome that aspect of human nature, but that's still within us somewhere. Well, I think that's what we're doing right now. I have this theory that what is very important about the current moment is that all of evolution has been survival of the fittest up until now. And at some point, the lines are kind of fuzzy, but in the recent past, or maybe even just right now, we're getting to this point where we can choose intelligent design. Like we probably since like the integration of the iPhone, like we are becoming cyborgs. Like our brains are fundamentally changed. Everyone who grew up with electronics, we are fundamentally different from previous, from homo sapiens. I call us homo techno. I think we have evolved into homo techno, which is like essentially a new species. Like if you look at the way, if you took an MRI of my brain and you took an MRI of like a medieval brain, I think it would be very different the way that it has evolved. Do you think when historians look back at this time, they'll see like this was a fundamental shift to what a human being is? I think, I do not think we are still homo sapiens. I believe we are homo techno. And I think we have evolved. And I think right now, the way we are evolving, we can choose how we do that. And I think we are being very reckless about how we're doing that. Like we're just having social media, but I think this idea that like this is a time to choose intelligent design should be taken very seriously. It like now is the moment to reprogram the human computer. It's like, if you go blind, your visual cortex will get taken over with other functions. We can choose our own evolution. We can change the way our brains work. And so we actually have a huge responsibility to do that. And I think I'm not sure who should be responsible for that, but there's definitely not adequate education. We're being inundated with all this technology that is fundamentally changing the physical structure of our brains. And we are not adequately responding to that to choose how we wanna evolve. And we could evolve, we could be really whatever we want. And I think this is a really important time. And I think if we choose correctly and we choose wisely, consciousness could exist for a very long time and integration with AI could be extremely positive. And I don't think enough people are focusing on this specific situation. Do you think we might irreversibly screw things up if we get things wrong now? Because the flip side of that, it seems humans are pretty adaptive. So maybe the way we figure things out is by screwing it up, like social media. Over a generation, we'll see the negative effects of social media, and then we build new social medias, and we just keep improving stuff. And then we learn from the failures of the past. Because humans seem to be really adaptive. On the flip side, we can get it wrong in a way where literally we create weapons of war or increase hate. Past a certain threshold, we really do a lot of damage. I mean, I think we're optimized to notice the negative things. But I would actually say one of the things that I think people aren't noticing is if you look at Silicon Valley and you look at the technocracy, like what's been happening there. When Silicon Valley started, it was all just Facebook and all this for profit crap that really wasn't particular. I guess it was useful, but it's sort of just whatever. But now you see lab grown meat, compostable, or biodegradable, single use cutlery, or meditation apps. I think we are actually evolving and changing, and technology is changing. I think there just maybe there isn't quite enough education about this. And also, I don't know if there's quite enough incentive for it because I think the way capitalism works, what we define as profit, we're also working on an old model of what we define as profit. I really think if we changed the idea of profit to include social good, you can have economic profit, social good also counting as profit would incentivize things that are more useful and more whatever spiritual technology or positive technology or things that help reprogram a human computer in a good way or things that help us intelligently design our new brains. Yeah, there's no reason why within the framework of capitalism, the word profit or the idea of profit can't also incorporate the well being of a human being. So like long term well being, long term happiness. Or even for example, we were talking about motherhood, like part of the reason I'm so late is because I had to get the baby to bed. And it's like, I keep thinking about motherhood, how under capitalism, it's like this extremely essential job that is very difficult that is not compensated. And we sort of like value things by how much we compensate them. And so we really devalue motherhood in our society and pretty much all societies. Like capitalism does not recognize motherhood. It's just a job that you're supposed to do for free. And it's like, but I feel like producing great humans should be seen as a great, as profit under capitalism. Like that should be, that's like a huge social good. Like every awesome human that gets made adds so much to the world. So like if that was integrated into the profit structure, then, you know, and if we potentially found a way to compensate motherhood. So come up with a compensation that's much broader than just money or. Or it could just be money. Like, what if you just made, I don't know, but I don't know how you'd pay for that. Like, I mean, that's where you start getting into. Reallocation of resources that people get upset over. Well, like what if we made like a motherhood Dow? Yeah, yeah. And, you know, used it to fund like single mothers, like, you know, pay for making babies. So, I mean, if you create and put beautiful things onto the world, that could be companies, that can be bridges, that could be art, that could be a lot of things, and that could be children, which are. Or education or. Anything, that should be valued by society, and that should be somehow incorporated into the framework of what, as a market, of what. Like, if you contribute children to this world, that should be valued and respected and sort of celebrated, like, proportional to what it is, which is, it's the thing that fuels human civilization. Yeah, like I. It's kind of important. I feel like everyone's always saying, I mean, I think we're in very different social spheres, but everyone's always saying, like, dismantle capitalism. And I'm like, well, okay, well, I don't think the government should own everything. Like, I don't think we should not have private ownership. Like, that's scary. You know, like that starts getting into weird stuff and just sort of like, I feel there's almost no way to do that without a police state, you know? But obviously, capitalism has some major flaws. And I think actually Mac showed me this idea called social capitalism, which is a form of capitalism that just like considers social good to be also profit. Like, you know, it's like, right now companies need to, like, you're supposed to grow every quarter or whatever to like show that you're functioning well, but it's like, okay, well, what if you kept the same amount of profit? You're still in the green, but then you have also all this social good. Like, do you really need all this extra economic growth or could you add this social good and that counts? And, you know, I don't know if, I am not an economist. I have no idea how this could be achieved, but. I don't think economists know how anything could be achieved either, but they pretend. It's the thing, they construct a model and they go on TV shows and sound like an expert. That's the definition of economist. How did being a mother, becoming a mother change you as a human being, would you say? Man, I think it kind of changed everything and it's still changing me a lot. It's actually changing me more right now in this moment than it was before. Like today, like this? Just like in the most recent months and stuff. Can you elucidate that, how change, like when you wake up in the morning and you look at yourself, it's again, which, who are you? How have you become different, would you say? I think it's just really reorienting my priorities. And at first I was really fighting against that because I somehow felt it was like a failure of feminism or something. Like I felt like it was like bad if like my kids started mattering more than my work. And then like more recently I started sort of analyzing that thought in myself and being like, that's also kind of a construct. It's like, we've just devalued motherhood so much in our culture that like, I feel guilty for caring about my kids more than I care about my work. So feminism includes breaking out of whatever the construct is. So just continually breaking, it's like freedom empower you to be free. And that means... But it also, but like being a mother, like I'm so much more creative. Like I cannot believe the massive amount of brain growth that I am. Why do you think that is? Just cause like the stakes are higher somehow? I think it's like, it's just so trippy watching consciousness emerge. It's just like, it's like going on a crazy journey or something. It's like the craziest science fiction novel you could ever read. It's just so crazy watching consciousness come into being. And then at the same time, like you're forced to value your time so much. Like when I have creative time now, it's so sacred. I need to like be really fricking on it. But the other thing is that I used to just be like a cynic and I used to just wanna... Like my last album was called Miss Anthropocene and it was like this like, it was like a study in villainy or like it was like, well, what if we have, instead of the old gods, we have like new gods and it's like Miss Anthropocene is like misanthrope like and Anthropocene, which is like the, you know, like and she's the goddess of climate change or whatever. And she's like destroying the world. And it was just like, it was like dark and it was like a study in villainy. And it was sort of just like, like I used to like have no problem just making cynical, angry, scary art. And not that there's anything wrong with that, but I think having kids just makes you such an optimist. It just inherently makes you wanna be an optimist so bad that like I feel more responsibility to make more optimistic things. And I get a lot of shit for it because everyone's like, oh, you're so privileged. Stop talking about like pie in the sky, stupid concepts and focus on like the now. But it's like, I think if we don't ideate about futures that could be good, we won't be able to get them. If everything is Blade Runner, then we're gonna end up with Blade Runner. It's like, as we said earlier, life imitates art. Like life really does imitate art. And so we really need more protopian or utopian art. I think this is incredibly essential for the future of humanity. And I think the current discourse where that's seen as a thinking about protopia or utopia is seen as a dismissal of the problems that we currently have. I think that is an incorrect mindset. And like having kids just makes me wanna imagine amazing futures that like maybe I won't be able to build, but they will be able to build if they want to. Yeah, it does seem like ideation is a precursor to creation. So you have to imagine it in order to be able to build it. And there is a sad thing about human nature that somehow a cynical view of the world is seen as a insightful view. You know, cynicism is often confused for insight, which is sad to see. And optimism is confused for naivete. Yes, yes. Like you don't, you're blinded by your, maybe your privilege or whatever. You're blinded by something, but you're certainly blinded. That's sad, that's sad to see because it seems like the optimists are the ones that create our future. They're the ones that build. In order to build the crazy thing, you have to be optimistic. You have to be either stupid or excited or passionate or mad enough to actually believe that it can be built. And those are the people that built it. My favorite quote of all time is from Star Wars, Episode 8, which I know everyone hates. Do you like Star Wars, Episode 8? No, yeah, probably I would say I would probably hate it, yeah. I don't have strong feelings about it. Let me backtrack. I don't have strong feelings about Star Wars. I'm a Tolkien person. I'm more into dragons and orcs and ogres. Yeah, I mean, Tolkien forever. I really want to have one more son and call him, I thought Tao Tecno Tolkien would be cool. It's a lot of T's, I like it. Yeah, and well, and Tao is six, two, eight, two pi. Yeah, Tao Tecno, yeah, yeah, yeah. And then techno is obviously the best genre of music, but also like technocracy. It just sounds really good. Yeah, that's right, and techno Tolkien, Tao Tecno Tolkien. That's a good, that's it. Tao Tecno Tolkien, but Star Wars, Episode 8, I know a lot of people have issues with it. Personally, on the record, I think it's the best Star Wars film. You're starting trouble today. Yeah, but don't kill what you hate, save what you love. Don't kill what you hate. Don't kill what you hate, save what you love. And I think we're, in society right now, we're in a diagnosis mode. We're just diagnosing and diagnosing and diagnosing, and we're trying to kill what we hate, and we're not trying to save what we love enough. And there's this Buckminster Fuller quote, which I'm gonna butcher, because I don't remember it correctly, but it's something along the lines of, don't try to destroy the old bad models, render them obsolete with better models. Maybe we don't need to destroy the oil industry. Maybe we just create a great new battery technology and sustainable transport, and just make it economically unreasonable to still continue to rely on fossil fuels. It's like, don't kill what you hate, save what you love. Make new things and just render the old things unusable. It's like if the college debt is so bad, and universities are so expensive, and I feel like education is becoming obsolete. I feel like we could completely revolutionize education, and we could make it free. And it's like, you look at JSTOR, and you have to pay to get all the studies and everything. What if we created a DAO that bought JSTOR, or we created a DAO that was funding studies, and those studies were open source, or free for everyone. And what if we just open sourced education and decentralized education and made it free, and all research was on the internet, and all the outcomes of studies were on the internet, and no one has student debt, and you just take tests when you apply for a job, and if you're qualified, then you can work there. This is just like, I don't know how anything works. I'm just randomly ranting, but. I like the humility. You gotta think from just basic first principles. What is the problem? What's broken? What are some ideas? That's it. And get excited about those ideas, and share your excitement, and don't tear each other down. It's just when you kill things, you often end up killing yourself. Like war is not a one sided, like you're not gonna go in and just kill them, like you're gonna get stabbed. It's like, and I think when I talk about this nexus point of that we're in this point in society where we're switching to intelligent design, I think part of our switch to intelligent design is that we need to choose nonviolence. And we need to, like, I think we can choose to start, I don't think we can eradicate violence from our species, because I think we need it a little bit, but I think we can choose to really reorient our primitive brains that are fighting over scarcity, and that are so attack oriented, and move into, we can optimize for creativity and building. Yeah, it's interesting to think how that happens, so some of it is just education, some of it is living life and introspecting your own mind, and trying to live up to the better angels of your nature for each one of us, all those kinds of things at scale. That's how we can sort of start to minimize the amount of destructive war in our world, and that's, to me, probably you're the same, technology is a really promising way to do that. Like, social media should be a really promising way to do that, it's a way we connect. I, you know, for the most part, I really enjoy social media. I just know all the negative stuff. I don't engage with any of the negative stuff. Just not even, like, by blocking or any of that kind of stuff, but just not letting it enter my mind. Like, just, like, when somebody says something negative, I see it, I immediately think positive thoughts about them, and I just forget they exist after that. Just move on, because, like, that negative energy, if I return the negative energy, they're going to get excited in a negative way right back, and it's just this kind of vicious cycle. But you would think technology would assist us in this process of letting go, of not taking things personally, of not engaging in the negativity, but unfortunately, social media profits from the negativity, so the current models. I mean, social media is like a gun. Like, you should take a course before you use it. Like, it's like, this is what I mean, like, when I say reprogram the human computer. Like, in school, you should learn about how social media optimizes to, you know, raise your cortisol levels and make you angry and crazy and stressed, and, like, you should learn how to have hygiene about how you use social media. But, so you can, yeah, choose not to focus on the negative stuff, but I don't know. I'm not sure social media should, I guess it should exist. I'm not sure. I mean, we're in the messy, it's the experimental phase. Like, we're working it out. Yeah, it's the early days. I don't even know, when you say social media, I don't know what that even means. We're in the very early days. I think social media is just basic human connection in the digital realm, and that, I think it should exist, but there's so many ways to do it in a bad way. There's so many ways to do it in a good way. There's all discussions of all the same human rights. We talk about freedom of speech. We talk about sort of violence in the space of digital media. We talk about hate speech. We talk about all these things that we had to figure out back in the day in the physical space. We're now figuring out in the digital space, and it's like baby stages. When the printing press came out, it was like pure chaos for a minute, you know? It's like when you inject, when there's a massive information injection into the general population, there's just gonna be, I feel like the printing press, I don't have the years, but it was like printing press came out, shit got really fucking bad for a minute, but then we got the enlightenment. And so it's like, I think we're in, this is like the second coming of the printing press. We're probably gonna have some shitty times for a minute, and then we're gonna have recalibrate to have a better understanding of how we consume media and how we deliver media. Speaking of programming the human computer, you mentioned Baby X. So there's this young consciousness coming to be, came from a cell. Like that whole thing doesn't even make sense. It came from DNA. Yeah. And then there's this baby computer that just like grows and grows and grows and grows, and now there's a conscious being with extremely impressive cognitive capabilities with, Have you met him? Yes, yeah. Yeah. He's actually really smart. He's really smart. Yeah. He's weird. Yeah. Or a baby. He does. I don't, I haven't. I don't know a lot of other babies, but he seems to be smart. Zach, I don't hang out with babies often, but this baby was very impressive. He does a lot of pranks and stuff. Oh, so he's like. Like he'll like give you a treat and then take it away and laugh and like stuff like that. So he's like a chess player. So here's a cognitive sort of, there's a computer being programmed. So he's taking in the environment, interacting with a specific set of humans. How would you, first of all, what is it? What, let me ask. I want to ask how do you program this computer? And also how do you make sense of that there's a conscious being right there that wasn't there before? It's giving me a lot of crisis thoughts. I'm thinking really hard. I think that's part of the reason it's like, I'm struggling to focus on art and stuff right now. Cause baby X is becoming conscious and like my it's just reorienting my brain. Like my brain is suddenly totally shifting of like, oh shit, like the way we raise children. Like, I hate all the baby books and everything. I hate them. Like they're, oh, the art is so bad. And like all this stuff, everything about all the aesthetics. And like, I'm just like, ah, like this is so. The programming languages we're using to program these baby computers isn't good. Yeah, like I'm thinking, and I, not that I have like good answers or know what to do, but I'm just thinking really, really hard about it. I, we recently watched Totoro with him, Studio Ghibli. And it's just like a fantastic film. And he like responded to, I know you're not supposed to show baby screens too much, but like, I think it's the most sort of like, I feel like it's the highest art baby content. Like it really speaks, there's almost no talking in it. It's really simple. Although all the dialogue is super, super, super simple, you know, and it's like a one to three year old can like really connect with it. Like it feels like it's almost aimed at like a one to three year old, but it's like great art and it's so imaginative and it's so beautiful. And like the first time I showed it to him, he was just like so invested in it, unlike I've ever, unlike anything else I'd ever shown him. Like he was just like crying when they cry and laughing when they laugh, like just like having this roller coaster of like emotions, like, and he learned a bunch of words. Like he was, and he started saying Totoro and started just saying all this stuff after watching Totoro, and he wants to watch it all the time. And I was like, man, why isn't there an industry of this? Like why aren't our best artists focusing on making art like for the birth of consciousness? Like, and that's one of the things I've been thinking I really wanna start doing. You know, I don't wanna speak before I do things too much, but like, I'm just like ages one to three, like we should be putting so much effort into that. And the other thing about Totoro is it's like, it's like better for the environment because adults love Totoro. It's such good art that everyone loves it. Like I still have all my old Totoro merch from when I was a kid. Like I literally have the most ragged old Totoro merch. Like everybody loves it, everybody keeps it. It's like, why does the art we have for babies need to suck and be not accessible to adults and then just be thrown out when, you know, they age out of it? Like, it's like, I don't know. I don't have like a fully formed thought here, but this is just something I've been thinking about a lot is like, how do we like, how do we have more Totoroesque content? Like how do we have more content like this that like is universal and everybody loves, but is like really geared to an emerging consciousness? Emerging consciousness in the first like three years of life that so much turmoil, so much evolution of mind is happening. It seems like a crucial time. Would you say to make it not suck, do you think of basically treating a child like they have the capacity to have the brilliance of an adult or even beyond that? Is that how you think of that mind or? No, cause they still, they like it when you talk weird and stuff. Like they respond better to, cause even they can imitate better when your voice is higher. Like people say like, oh, don't do baby talk. But it's like, when your voice is higher, it's closer to something they can imitate. So they like, like the baby talk actually kind of works. Like it helps them learn to communicate. I've found it to be more effective with learning words and stuff. But like, you're not speaking down to them. Like do they have the capacity\",\n          \"The following is a conversation with Sean Carroll. He's a theoretical physicist at Caltech specializing in quantum mechanics, gravity, and cosmology. He's the author of several popular books, one on the arrow of time called From Eternity to Here, one on the Higgs boson called Particle at the End of the Universe, and one on science and philosophy called The Big Picture on the Origins of Life, Meaning, and the Universe Itself. He has an upcoming book on quantum mechanics that you can preorder now called Something Deeply Hidden. He writes one of my favorite blogs on his website, preposterousuniverse.com. I recommend clicking on the Greatest Hits link that lists accessible, interesting posts on the arrow of time, dark matter, dark energy, the Big Bang, general relativity, string theory, quantum mechanics, and the big meta questions about the philosophy of science, God, ethics, politics, academia, and much, much more. Finally, and perhaps most famously, he's the host of a podcast called Mindscape that you should subscribe to and support on Patreon. Along with the Joe Rogan experience, Sam Harris's Making Sense, and Dan Carlin's Hardcore History, Sean's Mindscape podcast is one of my favorite ways to learn new ideas or explore different perspectives and ideas that I thought I understood. It was truly an honor to meet and spend a couple hours with Sean. It's a bit heartbreaking to say that for the first time ever, the audio recorder for this podcast died in the middle of our conversation. There's technical reasons for this, having to do with phantom power that I now understand and will avoid. It took me one hour to notice and fix the problem. So, much like the universe is 68% dark energy, roughly the same amount from this conversation was lost, except in the memories of the two people involved and in my notes. I'm sure we'll talk again and continue this conversation on this podcast or on Sean's. And of course, I look forward to it. This is the Artificial Intelligence podcast. If you enjoy it, subscribe on YouTube, iTunes, support it on Patreon, or simply connect with me on Twitter at Lex Friedman. And now, here's my conversation with Sean Carroll. What do you think is more interesting and impactful, understanding how the universe works at a fundamental level or understanding how the human mind works? You know, of course this is a crazy, meaningless, unanswerable question in some sense, because they're both very interesting and there's no absolute scale of interestingness that we can rate them on. There's a glib answer that says the human brain is part of the universe, right? And therefore, understanding the universe is more fundamental than understanding the human brain. But do you really believe that once we understand the fundamental way the universe works at the particle level, the forces, we would be able to understand how the mind works? No, certainly not. We cannot understand how ice cream works just from understanding how particles work, right? So I'm a big believer in emergence. I'm a big believer that there are different ways of talking about the world beyond just the most fundamental microscopic one. You know, when we talk about tables and chairs and planets and people, we're not talking the language of particle physics and cosmology. So, but understanding the universe, you didn't say just at the most fundamental level, right? So understanding the universe at all levels is part of that. I do think, you know, to be a little bit more fair to the question, there probably are general principles of complexity, biology, information processing, memory, knowledge, creativity that go beyond just the human brain, right? And maybe one could count understanding those as part of understanding the universe. The human brain, as far as we know, is the most complex thing in the universe. So there's, it's certainly absurd to think that by understanding the fundamental laws of particle physics, you get any direct insight on how the brain works. But then there's this step from the fundamentals of particle physics to information processing, which a lot of physicists and philosophers may be a little bit carelessly take when they talk about artificial intelligence. Do you think of the universe as a kind of a computational device? No, to be like, the honest answer there is no. There's a sense in which the universe processes information, clearly. There's a sense in which the universe is like a computer, clearly. But in some sense, I think, I tried to say this once on my blog and no one agreed with me, but the universe is more like a computation than a computer because the universe happens once. A computer is a general purpose machine, right? That you can ask it different questions, even a pocket calculator, right? And it's set up to answer certain kinds of questions. The universe isn't that. So information processing happens in the universe, but it's not what the universe is. And I know your MIT colleague, Seth Lloyd, feels very differently about this, right? Well, you're thinking of the universe as a closed system. I am. So what makes a computer more like a PC, like a computing machine is that there's a human that every once comes up to it and moves the mouse around. So input. Gives it input. Gives it input. And that's why you're saying it's just a computation, a deterministic thing that's just unrolling. But the immense complexity of it is nevertheless like processing. There's a state and then it changes with good rules. And there's a sense for a lot of people that if the brain operates, the human brain operates within that world, then it's simply just a small subset of that. And so there's no reason we can't build arbitrarily great intelligences. Yeah. Do you think of intelligence in this way? Intelligence is tricky. I don't have a definition of it offhand. So I remember this panel discussion that I saw on YouTube. I wasn't there, but Seth Lloyd was on the panel. And so was Martin Rees, the famous astrophysicist. And Seth gave his shtick for why the universe is a computer and explained this. And Martin Rees said, so what is not a computer? And Seth was like, oh, that's a good question. I'm not sure. Because if you have a sufficiently broad definition of what a computer is, then everything is, right? And the simile or the analogy gains force when it excludes some things. You know, is the moon going around the earth performing a computation? I can come up with definitions in which the answer is yes, but it's not a very useful computation. I think that it's absolutely helpful to think about the universe in certain situations, certain contexts, as an information processing device. I'm even guilty of writing a paper called Quantum Circuit Cosmology, where we modeled the whole universe as a quantum circuit. As a circuit. As a circuit, yeah. With qubits kind of thing? With qubits basically, right, yeah. So, and qubits becoming more and more entangled. So do we wanna digress a little bit? Let's do it. It's kind of fun. So here's a mystery about the universe that is so deep and profound that nobody talks about it. Space expands, right? And we talk about, in a certain region of space, a certain number of degrees of freedom, a certain number of ways that the quantum fields and the particles in that region can arrange themselves. That number of degrees of freedom in a region of space is arguably finite. We actually don't know how many there are, but there's a very good argument that says it's a finite number. So as the universe expands and space gets bigger, are there more degrees of freedom? If it's an infinite number, it doesn't really matter. Infinity times two is still infinity. But if it's a finite number, then there's more space, so there's more degrees of freedom. So where did they come from? That would mean the universe is not a closed system. There's more degrees of freedom popping into existence. So what we suggested was that there are more degrees of freedom, and it's not that they're not there to start, but they're not entangled to start. So the universe that you and I know of, the three dimensions around us that we see, we said those are the entangled degrees of freedom making up space time. And as the universe expands, there are a whole bunch of qubits in their zero state that become entangled with the rest of space time through the action of these quantum circuits. So what does it mean that there's now more degrees of freedom as they become more entangled? Yeah, so. As the universe expands. That's right, so there's more and more degrees of freedom that are entangled, that are playing part, playing the role of part of the entangled space time structure. So the basic, the underlying philosophy is that space time itself arises from the entanglement of some fundamental quantum degrees of freedom. Wow, okay, so at which point is most of the entanglement happening? Are we talking about close to the Big Bang? Are we talking about throughout the time of the life? Throughout history, yeah. So the idea is that at the Big Bang, almost all the degrees of freedom that the universe could have were there, but they were unentangled with anything else. And that's a reflection of the fact that the Big Bang had a low entropy. It was a very simple, very small place. And as space expands, more and more degrees of freedom become entangled with the rest of the world. Well, I have to ask John Carroll, what do you think of the thought experiment from Nick Bostrom that we're living in a simulation? So I think, let me contextualize that a little bit more. I think people don't actually take this thought experiments. I think it's quite interesting. It's not very useful, but it's quite interesting. From the perspective of AI, a lot of the learning that can be done usually happens in simulation from artificial examples. And so it's a constructive question to ask, how difficult is our real world to simulate? Right. Which is kind of a dual part of, if we're living in a simulation and somebody built that simulation, if you were to try to do it yourself, how hard would it be? So obviously we could be living in a simulation. If you just want the physical possibility, then I completely agree that it's physically possible. I don't think that we actually are. So take this one piece of data into consideration. You know, we live in a big universe, okay? There's two trillion galaxies in our observable universe with 200 billion stars in each galaxy, et cetera. It would seem to be a waste of resources to have a universe that big going on just to do a simulation. So in other words, I want to be a good Bayesian. I want to ask under this hypothesis, what do I expect to see? So the first thing I would say is I wouldn't expect to see a universe that was that big, okay? The second thing is I wouldn't expect the resolution of the universe to be as good as it is. So it's always possible that if our superhuman simulators only have finite resources, that they don't render the entire universe, right? That the part that is out there, the two trillion galaxies, isn't actually being simulated fully, okay? But then the obvious extrapolation of that is that only I am being simulated fully. Like the rest of you are just non player characters, right? I'm the only thing that is real. The rest of you are just chat bots. Beyond this wall, I see the wall, but there is literally nothing on the other side of the wall. That is sort of the Bayesian prediction. That's what it would be like to do an efficient simulation of me. So like none of that seems quite realistic. I don't see, I hear the argument that it's just possible and easy to simulate lots of things. I don't see any evidence from what we know about our universe that we look like a simulated universe. Now, maybe you can say, well, we don't know what it would look like, but that's just abandoning your Bayesian responsibilities. Like your job is to say under this theory, here's what you would expect to see. Yeah, so certainly if you think about simulation as a thing that's like a video game where only a small subset is being rendered. But say the entire, all the laws of physics, the entire closed system of the quote unquote universe, it had a creator. Yeah, it's always possible. Right, so that's not useful to think about when you're thinking about physics. The way Nick Bostrom phrases it, if it's possible to simulate a universe, eventually we'll do it. Right. You can use that by the way for a lot of things. Well, yeah. But I guess the question is, how hard is it to create a universe? I wrote a little blog post about this and maybe I'm missing something, but there's an argument that says not only that it might be possible to simulate a universe, but probably if you imagine that you actually attribute consciousness and agency to the little things that we're simulating, to our little artificial beings, there's probably a lot more of them than there are ordinary organic beings in the universe or there will be in the future, right? So there's an argument that not only is being a simulation possible, it's probable because in the space of all living consciousnesses, most of them are being simulated, right? Most of them are not at the top level. I think that argument must be wrong because it follows from that argument that, if we're simulated, but we can also simulate other things, well, but if we can simulate other things, they can simulate other things, right? If we give them enough power and resolution and ultimately we'll reach a bottom because the laws of physics in our universe have a bottom, we're made of atoms and so forth, so there will be the cheapest possible simulations. And if you believe the original argument, you should conclude that we should be in the cheapest possible simulation because that's where most people are. But we don't look like that. It doesn't look at all like we're at the edge of resolution, that we're 16 bit things. It seems much easier to make much lower level things than we are. And also, I questioned the whole approach to the anthropic principle that says we are typical observers in the universe. I think that that's not actually, I think that there's a lot of selection that we can do that we're typical within things we already know, but not typical within all of the universe. So do you think there's intelligent life, however you would like to define intelligent life, out there in the universe? My guess is that there is not intelligent life in the observable universe other than us, simply on the basis of the fact that the likely number of other intelligent species in the observable universe, there's two likely numbers, zero or billions. And if there had been billions, you would have noticed already. For there to be literally like a small number, like, you know, Star Trek, there's a dozen intelligent civilizations in our galaxy, but not a billion, that's weird. That's sort of bizarre to me. It's easy for me to imagine that there are zero others because there's just a big bottleneck to making multicellular life or technological life or whatever. It's very hard for me to imagine that there's a whole bunch out there that have somehow remained hidden from us. The question I'd like to ask is what would intelligent life look like? What I mean by that question and where it's going is what if intelligent life is just in some very big ways different than the one that has on Earth? That there's all kinds of intelligent life that operates at different scales of both size and temporal. Right, that's a great possibility because I think we should be humble about what intelligence is, what life is. We don't even agree on what life is, much less what intelligent life is, right? So that's an argument for humility, saying there could be intelligent life of a very different character, right? Like you could imagine the dolphins are intelligent but never invent space travel because they live in the ocean and they don't have thumbs, right? So they never invent technology, they never invent smelting. Maybe the universe is full of intelligent species that just don't make technology, right? That's compatible with the data, I think. And I think maybe what you're pointing at is even more out there versions of intelligence, intelligence in intermolecular clouds or on the surface of a neutron star or in between the galaxies in giant things where the equivalent of a heartbeat is 100 million years. On the one hand, yes, we should be very open minded about those things. On the other hand, all of us share the same laws of physics. There might be something about the laws of physics, even though we don't currently know exactly what that thing would be, that makes meters and years the right length and timescales for intelligent life. Maybe not, but we're made of atoms, atoms have a certain size, we orbit stars or stars have a certain lifetime. It's not impossible to me that there's a sweet spot for intelligent life that we find ourselves in. So I'm open minded either way, I'm open minded either being humble and there's all sorts of different kinds of life or no, there's a reason we just don't know it yet why life like ours is the kind of life that's out there. Yeah, I'm of two minds too, but I often wonder if our brains is just designed to quite obviously to operate and see the world in these timescales and we're almost blind and the tools we've created for detecting things are blind to the kind of observation needed to see intelligent life at other scales. Well, I'm totally open to that, but so here's another argument I would make, we have looked for intelligent life, but we've looked at for it in the dumbest way we can, by turning radio telescopes to the sky. And why in the world would a super advanced civilization randomly beam out radio signals wastefully in all directions into the universe? That just doesn't make any sense, especially because in order to think that you would actually contact another civilization, you would have to do it forever, you have to keep doing it for millions of years, that sounds like a waste of resources. If you thought that there were other solar systems with planets around them, where maybe intelligent life didn't yet exist, but might someday, you wouldn't try to talk to it with radio waves, you would send a spacecraft out there and you would park it around there and it would be like, from our point of view, it'd be like 2001, where there was a monolith. Monolith. There could be an artifact, in fact, the other way works also, right? There could be artifacts in our solar system that have been put there by other technologically advanced civilizations and that's how we will eventually contact them. We just haven't explored the solar system well enough yet to find them. The reason why we don't think about that is because we're young and impatient, right? Like, it would take more than my lifetime to actually send something to another star system and wait for it and then come back. So, but if we start thinking on hundreds of thousands of years or million year time scales, that's clearly the right thing to do. Are you excited by the thing that Elon Musk is doing with SpaceX in general? Space, but the idea of space exploration, even though your, or your species is young and impatient? Yeah. No, I do think that space travel is crucially important, long term. Even to other star systems. And I think that many people overestimate the difficulty because they say, look, if you travel 1% the speed of light to another star system, we'll be dead before we get there, right? And I think that it's much easier. And therefore, when they write their science fiction stories, they imagine we'd go faster than the speed of light because otherwise they're too impatient, right? We're not gonna go faster than the speed of light, but we could easily imagine that the human lifespan gets extended to thousands of years. And once you do that, then the stars are much closer effectively, right? And then what's a hundred year trip, right? So I think that that's gonna be the future, the far future, not my lifetime once again, but baby steps. Unless your lifetime gets extended. Well, it's in a race against time, right? A friend of mine who actually thinks about these things said, you know, you and I are gonna die, but I don't know about our grandchildren. That's, I don't know, predicting the future is hard, but that's at least a plausible scenario. And so, yeah, no, I think that as we discussed earlier, there are threats to the earth, known and unknown, right? Having spread humanity and biology elsewhere is a really important longterm goal. What kind of questions can science not currently answer, but might soon? When you think about the problems and the mysteries before us that may be within reach of science. I think an obvious one is the origin of life. We don't know how that happened. There's a difficulty in knowing how it happened historically actually, you know, literally on earth, but starting life from non life is something I kind of think we're close to, right? We're really. You really think so? Like how difficult is it to start life? Well, I've talked to people, including on the podcast about this. You know, life requires three things. Life as we know it. So there's a difference with life, which who knows what it is, and life as we know it, which we can talk about with some intelligence. So life as we know it requires compartmentalization. You need like a little membrane around your cell. Metabolism, you need to take in food and eat it and let that make you do things. And then replication, okay? So you need to have some information about who you are that you pass down to future generations. In the lab, compartmentalization seems pretty easy. Not hard to make lipid bilayers that come into little cellular walls pretty easily. Metabolism and replication are hard, but replication we're close to. People have made RNA like molecules in the lab that I think the state of the art is, they're not able to make one molecule that reproduces itself, but they're able to make two molecules that reproduce each other. So that's okay. That's pretty close. Metabolism is harder, believe it or not, even though it's sort of the most obvious thing, but you want some sort of controlled metabolism and the actual cellular machinery in our bodies is quite complicated. It's hard to see it just popping into existence all by itself. It probably took a while, but we're making progress. And in fact, I don't think we're spending nearly enough money on it. If I were the NSF, I would flood this area with money because it would change our view of the world if we could actually make life in the lab and understand how it was made originally here on earth. And I'm sure it'd have some ripple effects that help cure disease and so on. I mean, just that understanding. So synthetic biology is a wonderful big frontier where we're making cells. Right now, the best way to do that is to borrow heavily from existing biology, right? Well, Craig Venter several years ago created an artificial cell, but all he did was, not all he did, it was a tremendous accomplishment, but all he did was take out the DNA from a cell and put in entirely new DNA and let it boot up and go. What about the leap to creating intelligent life on earth? Yeah. Again, we define intelligence, of course, but let's just even say Homo sapiens, the modern intelligence in our human brain. Do you have a sense of what's involved in that leap and how big of a leap that is? So AI would count in this, or do you really want life? Do you want really an organism in some sense? AI would count, I think. Okay. Yeah, of course, of course AI would count. Well, let's say artificial consciousness, right? So I do not think we are on the threshold of creating artificial consciousness. I think it's possible. I'm not, again, very educated about how close we are, but my impression is not that we're really close because we understand how little we understand of consciousness and what it is. So if we don't have any idea what it is, it's hard to imagine we're on the threshold of making it ourselves. But it's doable, it's possible. I don't see any obstacles in principle. So yeah, I would hold out some interest in that happening eventually. I think in general, consciousness, I think we would be just surprised how easy consciousness is once we create intelligence. I think consciousness is a thing that's just something we all fake. Well, good. No, actually, I like this idea that in fact, consciousness is way less mysterious than we think because we're all at every time, at every moment, less conscious than we think we are, right? We can fool things. And I think that plus the idea that you not only have artificial intelligent systems, but you put them in a body, right, give them a robot body, that will help the faking a lot. Yeah, I think creating consciousness in artificial consciousness is as simple as asking a Roomba to say, I'm conscious, and refusing to be talked out of it. Could be, it could be. And I mean, I'm almost being silly, but that's what we do. That's what we do with each other. This is the kind of, that consciousness is also a social construct. And a lot of our ideas of intelligence is a social construct. And so reaching that bar involves something that's beyond, that doesn't necessarily involve the fundamental understanding of how you go from electrons to neurons to cognition. No, actually, I think that is an extremely good point. And in fact, what it suggests is, so yeah, you referred to Kate Darling, who I had on the podcast, and who does these experiments with very simple robots, but they look like animals, and they can look like they're experiencing pain, and we human beings react very negatively to these little robots looking like they're experiencing pain. And what you wanna say is, yeah, but they're just robots. It's not really pain, right? It's just some electrons going around. But then you realize, you and I are just electrons going around, and that's what pain is also. And so what I would have an easy time imagining is that there is a spectrum between these simple little robots that Kate works with and a human being, where there are things that sort of by some strict definition, Turing test level thing are not conscious, but nevertheless walk and talk like they're conscious. And it could be that the future is, I mean, Siri is close, right? And so it might be the future has a lot more agents like that. And in fact, rather than someday going, aha, we have consciousness, we'll just creep up on it with more and more accurate reflections of what we expect. And in the future, maybe the present, for example, we haven't met before, and you're basically assuming that I'm human as it's a high probability at this time because the yeah, but in the future, there might be question marks around that, right? Yeah, no, absolutely. Certainly videos are almost to the point where you shouldn't trust them already. Photos you can't trust, right? Videos is easier to trust, but we're getting worse that, we're getting better at faking them, right? Yeah, so physical embodied people, what's so hard about faking that? So this is very depressing, this conversation we're having right now. So I mean, To me, it's exciting. To me, you're doing it. So it's exciting to you, but it's a sobering thought. We're very bad, right? At imagining what the next 50 years are gonna be like when we're in the middle of a phase transition as we are right now. Yeah, and I, in general, I'm not blind to all the threats. I am excited by the power of technology to solve, to protect us against the threats as they evolve. I'm not as much as Steven Pinker optimistic about the world, but in everything I've seen, all of the brilliant people in the world that I've met are good people. So the army of the good in terms of the development of technology is large. Okay, you're way more optimistic than I am. I think that goodness and badness are equally distributed among intelligent and unintelligent people. I don't see much of a correlation there. Interesting. Neither of us have proof. Yeah, exactly. Again, opinions are free, right? Nor definitions of good and evil. We come without definitions or without data opinions. So what kind of questions can science not currently answer and may never be able to answer in your view? Well, the obvious one is what is good and bad? What is right and wrong? I think that there are questions that, science tells us what happens, what the world is and what it does. It doesn't say what the world should do or what we should do, because we're part of the world. But we are part of the world and we have the ability to feel like something's right, something's wrong. And to make a very long story very short, I think that the idea of moral philosophy is systematizing our intuitions of what is right and what is wrong. And science might be able to predict ahead of time what we will do, but it won't ever be able to judge whether we should have done it or not. So, you're kind of unique in terms of scientists. Listen, it doesn't have to do with podcasts, but even just reaching out, I think you referred to as sort of doing interdisciplinary science. So you reach out and talk to people that are outside of your discipline, which I always hope that's what science was for. In fact, I was a little disillusioned when I realized that academia is very siloed. Yeah. And so the question is, how, at your own level, how do you prepare for these conversations? How do you think about these conversations? How do you open your mind enough to have these conversations? And it may be a little bit broader, how can you advise other scientists to have these kinds of conversations? Not at the podcast, the fact that you're doing a podcast is awesome, other people get to hear them, but it's also good to have it without mics in general. It's a good question, but a tough one to answer. I think about a guy I know who's a personal trainer, and he was asked on a podcast, how do we psych ourselves up to do a workout? How do we make that discipline to go and work out? And he's like, why are you asking me? I can't stop working out. I don't need to psych myself up. So, and likewise, he asked me, how do you get to have interdisciplinary conversations on all sorts of different things, all sorts of different people? I'm like, that's what makes me go, right? Like that's, I couldn't stop doing that. I did that long before any of them were recorded. In fact, a lot of the motivation for starting recording it was making sure I would read all these books that I had purchased, right? Like all these books I wanted to read, not enough time to read them. And now if I have the motivation, cause I'm gonna interview Pat Churchland, I'm gonna finally read her book. You know, and it's absolutely true that academia is extraordinarily siloed, right? We don't talk to people. We rarely do. And in fact, when we do, it's punished. You know, like the people who do it successfully generally first became very successful within their little siloed discipline. And only then did they start expanding out. If you're a young person, you know, I have graduate students. I try to be very, very candid with them about this, that it's, you know, most graduate students are to not become faculty members, right? It's a tough road. And so live the life you wanna live, but do it with your eyes open about what it does to your job chances. And the more broad you are and the less time you spend hyper specializing in your field, the lower your job chances are. That's just an academic reality. It's terrible, I don't like it, but it's a reality. And for some people, that's fine. Like there's plenty of people who are wonderful scientists who have zero interest in branching out and talking to things, to anyone outside their field. But it is disillusioning to me. Some of the, you know, romantic notion I had of the intellectual academic life is belied by the reality of it. The idea that we should reach out beyond our discipline and that is a positive good is just so rare in universities that it may as well not exist at all. But that said, even though you're saying you're doing it like the personal trainer, because you just can't help it, you're also an inspiration to others. Like I could speak for myself. You know, I also have a career I'm thinking about, right? And without your podcast, I may have not have been doing this at all, right? So it makes me realize that these kinds of conversations is kind of what science is about in many ways. The reason we write papers, this exchange of ideas, is it's much harder to do interdisciplinary papers, I would say. And conversations are easier. So conversations is the beginning. And in the field of AI, it's obvious that we should think outside of pure computer vision competitions on a particular data sets. We should think about the broader impact of how this can be, you know, reaching out to physics, to psychology, to neuroscience and having these conversations so that you're an inspiration. And so never know how the world changes. I mean, the fact that this stuff is out there and I've a huge number of people come up to me, grad students, really loving the podcast, inspired by it. And they will probably have that, they'll be ripple effects when they become faculty and so on and so on. We can end on a balance between pessimism and optimism. And Sean, thank you so much for talking to me, it was awesome. No, Lex, thank you very much for this conversation. It was great.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-23T16:18:05.558437Z",
          "start_time": "2025-07-23T16:18:05.535833Z"
        },
        "id": "initial_id"
      },
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "# Convertir cada fila en un Document\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=row[\"text\"],\n",
        "        metadata={\"id\": row[\"id\"], \"guest\": row[\"guest\"], \"title\": row[\"title\"]},\n",
        "    )\n",
        "    for _, row in df.iterrows()\n",
        "]"
      ],
      "id": "initial_id",
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laoFNTdpbTW8",
        "outputId": "92631260-d13c-419b-df27-285fb5354534"
      },
      "id": "laoFNTdpbTW8",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'id': 1, 'guest': 'Max Tegmark', 'title': 'Life 3.0'}, page_content=\"As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and engineering come together and it opens with a quote by Dostoevsky. The mystery of human existence lies not in just staying alive but in finding something to live for. Lastly, I believe that every failure rewards us with an opportunity to learn and in that sense, I've been very fortunate to fail in so many new and exciting ways and this conversation was no different. I've learned about something called radio frequency interference, RFI, look it up. Apparently, music and conversations from local radio stations can bleed into the audio that you're recording in such a way that it almost completely ruins that audio. It's an exceptionally difficult sound source to remove. So, I've gotten the opportunity to learn how to avoid RFI in the future during recording sessions. I've also gotten the opportunity to learn how to use Adobe Audition and iZotope RX 6 to do some noise, some audio repair. Of course, this is an exceptionally difficult noise to remove. I am an engineer. I'm not an audio engineer. Neither is anybody else in our group but we did our best. Nevertheless, I thank you for your patience and I hope you're still able to enjoy this conversation. Do you think there's intelligent life out there in the universe? Let's open up with an easy question. I have a minority view here actually. When I give public lectures, I often ask for a show of hands who thinks there's intelligent life out there somewhere else and almost everyone put their hands up and when I ask why, they'll be like, oh, there's so many galaxies out there, there's gotta be. But I'm a numbers nerd, right? So when you look more carefully at it, it's not so clear at all. When we talk about our universe, first of all, we don't mean all of space. We actually mean, I don't know, you can throw me the universe if you want, it's behind you there. It's, we simply mean the spherical region of space from which light has a time to reach us so far during the 14.8 billion year, 13.8 billion years since our Big Bang. There's more space here but this is what we call a universe because that's all we have access to. So is there intelligent life here that's gotten to the point of building telescopes and computers? My guess is no, actually. The probability of it happening on any given planet is some number we don't know what it is. And what we do know is that the number can't be super high because there's over a billion Earth like planets in the Milky Way galaxy alone, many of which are billions of years older than Earth. And aside from some UFO believers, there isn't much evidence that any superduran civilization has come here at all. And so that's the famous Fermi paradox, right? And then if you work the numbers, what you find is that if you have no clue what the probability is of getting life on a given planet, so it could be 10 to the minus 10, 10 to the minus 20, or 10 to the minus two, or any power of 10 is sort of equally likely if you wanna be really open minded, that translates into it being equally likely that our nearest neighbor is 10 to the 16 meters away, 10 to the 17 meters away, 10 to the 18. By the time you get much less than 10 to the 16 already, we pretty much know there is nothing else that close. And when you get beyond 10. Because they would have discovered us. Yeah, they would have been discovered as long ago, or if they're really close, we would have probably noted some engineering projects that they're doing. And if it's beyond 10 to the 26 meters, that's already outside of here. So my guess is actually that we are the only life in here that's gotten the point of building advanced tech, which I think is very, puts a lot of responsibility on our shoulders, not screw up. I think people who take for granted that it's okay for us to screw up, have an accidental nuclear war or go extinct somehow because there's a sort of Star Trek like situation out there where some other life forms are gonna come and bail us out and it doesn't matter as much. I think they're leveling us into a false sense of security. I think it's much more prudent to say, let's be really grateful for this amazing opportunity we've had and make the best of it just in case it is down to us. So from a physics perspective, do you think intelligent life, so it's unique from a sort of statistical view of the size of the universe, but from the basic matter of the universe, how difficult is it for intelligent life to come about? The kind of advanced tech building life is implied in your statement that it's really difficult to create something like a human species. Well, I think what we know is that going from no life to having life that can do a level of tech, there's some sort of two going beyond that than actually settling our whole universe with life. There's some major roadblock there, which is some great filter as it's sometimes called, which is tough to get through. It's either that roadblock is either behind us or in front of us. I'm hoping very much that it's behind us. I'm super excited every time we get a new report from NASA saying they failed to find any life on Mars. I'm like, yes, awesome. Because that suggests that the hard part, maybe it was getting the first ribosome or some very low level kind of stepping stone so that we're home free. Because if that's true, then the future is really only limited by our own imagination. It would be much suckier if it turns out that this level of life is kind of a dime a dozen, but maybe there's some other problem. Like as soon as a civilization gets advanced technology, within a hundred years, they get into some stupid fight with themselves and poof. That would be a bummer. Yeah, so you've explored the mysteries of the universe, the cosmological universe, the one that's sitting between us today. I think you've also begun to explore the other universe, which is sort of the mystery, the mysterious universe of the mind of intelligence, of intelligent life. So is there a common thread between your interest or the way you think about space and intelligence? Oh yeah, when I was a teenager, I was already very fascinated by the biggest questions. And I felt that the two biggest mysteries of all in science were our universe out there and our universe in here. So it's quite natural after having spent a quarter of a century on my career, thinking a lot about this one, that I'm now indulging in the luxury of doing research on this one. It's just so cool. I feel the time is ripe now for you trans greatly deepening our understanding of this. Just start exploring this one. Yeah, because I think a lot of people view intelligence as something mysterious that can only exist in biological organisms like us, and therefore dismiss all talk about artificial general intelligence as science fiction. But from my perspective as a physicist, I am a blob of quarks and electrons moving around in a certain pattern and processing information in certain ways. And this is also a blob of quarks and electrons. I'm not smarter than the water bottle because I'm made of different kinds of quarks. I'm made of up quarks and down quarks, exact same kind as this. There's no secret sauce, I think, in me. It's all about the pattern of the information processing. And this means that there's no law of physics saying that we can't create technology, which can help us by being incredibly intelligent and help us crack mysteries that we couldn't. In other words, I think we've really only seen the tip of the intelligence iceberg so far. Yeah, so the perceptronium. Yeah. So you coined this amazing term. It's a hypothetical state of matter, sort of thinking from a physics perspective, what is the kind of matter that can help, as you're saying, subjective experience emerge, consciousness emerge. So how do you think about consciousness from this physics perspective? Very good question. So again, I think many people have underestimated our ability to make progress on this by convincing themselves it's hopeless because somehow we're missing some ingredient that we need. There's some new consciousness particle or whatever. I happen to think that we're not missing anything and that it's not the interesting thing about consciousness that gives us this amazing subjective experience of colors and sounds and emotions. It's rather something at the higher level about the patterns of information processing. And that's why I like to think about this idea of perceptronium. What does it mean for an arbitrary physical system to be conscious in terms of what its particles are doing or its information is doing? I don't think, I hate carbon chauvinism, this attitude you have to be made of carbon atoms to be smart or conscious. There's something about the information processing that this kind of matter performs. Yeah, and you can see I have my favorite equations here describing various fundamental aspects of the world. I feel that I think one day, maybe someone who's watching this will come up with the equations that information processing has to satisfy to be conscious. I'm quite convinced there is big discovery to be made there because let's face it, we know that so many things are made up of information. We know that some information processing is conscious because we are conscious. But we also know that a lot of information processing is not conscious. Like most of the information processing happening in your brain right now is not conscious. There are like 10 megabytes per second coming in even just through your visual system. You're not conscious about your heartbeat regulation or most things. Even if I just ask you to like read what it says here, you look at it and then, oh, now you know what it said. But you're not aware of how the computation actually happened. Your consciousness is like the CEO that got an email at the end with the final answer. So what is it that makes a difference? I think that's both a great science mystery. We're actually studying it a little bit in my lab here at MIT, but I also think it's just a really urgent question to answer. For starters, I mean, if you're an emergency room doctor and you have an unresponsive patient coming in, wouldn't it be great if in addition to having a CT scanner, you had a consciousness scanner that could figure out whether this person is actually having locked in syndrome or is actually comatose. And in the future, imagine if we build robots or the machine that we can have really good conversations with, which I think is very likely to happen. Wouldn't you want to know if your home helper robot is actually experiencing anything or just like a zombie, I mean, would you prefer it? What would you prefer? Would you prefer that it's actually unconscious so that you don't have to feel guilty about switching it off or giving boring chores or what would you prefer? Well, certainly we would prefer, I would prefer the appearance of consciousness. But the question is whether the appearance of consciousness is different than consciousness itself. And sort of to ask that as a question, do you think we need to understand what consciousness is, solve the hard problem of consciousness in order to build something like an AGI system? No, I don't think that. And I think we will probably be able to build things even if we don't answer that question. But if we want to make sure that what happens is a good thing, we better solve it first. So it's a wonderful controversy you're raising there where you have basically three points of view about the hard problem. So there are two different points of view. They both conclude that the hard problem of consciousness is BS. On one hand, you have some people like Daniel Dennett who say that consciousness is just BS because consciousness is the same thing as intelligence. There's no difference. So anything which acts conscious is conscious, just like we are. And then there are also a lot of people, including many top AI researchers I know, who say, oh, consciousness is just bullshit because, of course, machines can never be conscious. They're always going to be zombies. You never have to feel guilty about how you treat them. And then there's a third group of people, including Giulio Tononi, for example, and Krzysztof Koch and a number of others. I would put myself also in this middle camp who say that actually some information processing is conscious and some is not. So let's find the equation which can be used to determine which it is. And I think we've just been a little bit lazy, kind of running away from this problem for a long time. It's been almost taboo to even mention the C word in a lot of circles because, but we should stop making excuses. This is a science question and there are ways we can even test any theory that makes predictions for this. And coming back to this helper robot, I mean, so you said you'd want your helper robot to certainly act conscious and treat you, like have conversations with you and stuff. I think so. But wouldn't you, would you feel, would you feel a little bit creeped out if you realized that it was just a glossed up tape recorder, you know, that was just zombie and was a faking emotion? Would you prefer that it actually had an experience or would you prefer that it's actually not experiencing anything so you feel, you don't have to feel guilty about what you do to it? It's such a difficult question because, you know, it's like when you're in a relationship and you say, well, I love you. And the other person said, I love you back. It's like asking, well, do they really love you back or are they just saying they love you back? Don't you really want them to actually love you? It's hard to, it's hard to really know the difference between everything seeming like there's consciousness present, there's intelligence present, there's affection, passion, love, and it actually being there. I'm not sure, do you have? But like, can I ask you a question about this? Like to make it a bit more pointed. So Mass General Hospital is right across the river, right? Yes. Suppose you're going in for a medical procedure and they're like, you know, for anesthesia, what we're going to do is we're going to give you muscle relaxants so you won't be able to move and you're going to feel excruciating pain during the whole surgery, but you won't be able to do anything about it. But then we're going to give you this drug that erases your memory of it. Would you be cool about that? What's the difference that you're conscious about it or not if there's no behavioral change, right? Right, that's a really, that's a really clear way to put it. That's, yeah, it feels like in that sense, experiencing it is a valuable quality. So actually being able to have subjective experiences, at least in that case, is valuable. And I think we humans have a little bit of a bad track record also of making these self serving arguments that other entities aren't conscious. You know, people often say, oh, these animals can't feel pain. It's okay to boil lobsters because we ask them if it hurt and they didn't say anything. And now there was just a paper out saying, lobsters do feel pain when you boil them and they're banning it in Switzerland. And we did this with slaves too often and said, oh, they don't mind. They don't maybe aren't conscious or women don't have souls or whatever. So I'm a little bit nervous when I hear people just take as an axiom that machines can't have experience ever. I think this is just a really fascinating science question is what it is. Let's research it and try to figure out what it is that makes the difference between unconscious intelligent behavior and conscious intelligent behavior. So in terms of, so if you think of a Boston Dynamics human or robot being sort of with a broom being pushed around, it starts pushing on a consciousness question. So let me ask, do you think an AGI system like a few neuroscientists believe needs to have a physical embodiment? Needs to have a body or something like a body? No, I don't think so. You mean to have a conscious experience? To have consciousness. I do think it helps a lot to have a physical embodiment to learn the kind of things about the world that are important to us humans, for sure. But I don't think the physical embodiment is necessary after you've learned it to just have the experience. Think about when you're dreaming, right? Your eyes are closed. You're not getting any sensory input. You're not behaving or moving in any way but there's still an experience there, right? And so clearly the experience that you have when you see something cool in your dreams isn't coming from your eyes. It's just the information processing itself in your brain which is that experience, right? But if I put it another way, I'll say because it comes from neuroscience is the reason you want to have a body and a physical something like a physical, you know, a physical system is because you want to be able to preserve something. In order to have a self, you could argue, would you need to have some kind of embodiment of self to want to preserve? Well, now we're getting a little bit anthropomorphic into anthropomorphizing things. Maybe talking about self preservation instincts. I mean, we are evolved organisms, right? So Darwinian evolution endowed us and other evolved organism with a self preservation instinct because those that didn't have those self preservation genes We can now, I think, quite convincingly answer that question of no, it's enough to have just one kind. If you look under the hood of AlphaZero, there's only one kind of neuron and it's ridiculously simple mathematical thing. So it's just like in physics, it's not, if you have a gas with waves in it, it's not the detailed nature of the molecule that matter, it's the collective behavior somehow. Similarly, it's this higher level structure of the network that matters, not that you have 20 kinds of neurons. I think our brain is such a complicated mess because it wasn't evolved just to be intelligent, it was involved to also be self assembling and self repairing, right? And evolutionarily attainable. And so on and so on. So I think it's pretty, my hunch is that we're going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. Yeah, that's right. You've given the example exactly of mechanical birds and airplanes and airplanes do a pretty good job of flying without really mimicking bird flight. And even now after 100 years later, did you see the Ted talk with this German mechanical bird? I heard you mention it. Check it out, it's amazing. But even after that, right, we still don't fly in mechanical birds because it turned out the way we came up with was simpler and it's better for our purposes. And I think it might be the same there. That's one lesson. And another lesson, it's more what our paper was about. First, as a physicist thought it was fascinating how there's a very close mathematical relationship actually between our artificial neural networks and a lot of things that we've studied for in physics go by nerdy names like the renormalization group equation and Hamiltonians and yada, yada, yada. And when you look a little more closely at this, you have, at first I was like, well, there's something crazy here that doesn't make sense. Because we know that if you even want to build a super simple neural network to tell apart cat pictures and dog pictures, right, that you can do that very, very well now. But if you think about it a little bit, you convince yourself it must be impossible because if I have one megapixel, even if each pixel is just black or white, there's two to the power of 1 million possible images, which is way more than there are atoms in our universe, right, so in order to, and then for each one of those, I have to assign a number, which is the probability that it's a dog. So an arbitrary function of images is a list of more numbers than there are atoms in our universe. So clearly I can't store that under the hood of my GPU or my computer, yet somehow it works. So what does that mean? Well, it means that out of all of the problems that you could try to solve with a neural network, almost all of them are impossible to solve with a reasonably sized one. But then what we showed in our paper was that the fraction, the kind of problems, the fraction of all the problems that you could possibly pose, that we actually care about given the laws of physics is also an infinite testimony, tiny little part. And amazingly, they're basically the same part. Yeah, it's almost like our world was created for, I mean, they kind of come together. Yeah, well, you could say maybe where the world was created for us, but I have a more modest interpretation, which is that the world was created for us, but I have a more modest interpretation, which is that instead evolution endowed us with neural networks precisely for that reason. Because this particular architecture, as opposed to the one in your laptop, is very, very well adapted to solving the kind of problems that nature kept presenting our ancestors with. So it makes sense that why do we have a brain in the first place? It's to be able to make predictions about the future and so on. So if we had a sucky system, which could never solve it, we wouldn't have a world. So this is, I think, a very beautiful fact. Yeah. We also realize that there's been earlier work on why deeper networks are good, but we were able to show an additional cool fact there, which is that even incredibly simple problems, like suppose I give you a thousand numbers and ask you to multiply them together, and you can write a few lines of code, boom, done, trivial. If you just try to do that with a neural network that has only one single hidden layer in it, you can do it, but you're going to need two to the power of a thousand neurons to multiply a thousand numbers, which is, again, more neurons than there are atoms in our universe. That's fascinating. But if you allow yourself to make it a deep network with many layers, you only need 4,000 neurons. It's perfectly feasible. That's really interesting. Yeah. So on another architecture type, I mean, you mentioned Schrodinger's equation, and what are your thoughts about quantum computing and the role of this kind of computational unit in creating an intelligence system? In some Hollywood movies that I will not mention by name because I don't want to spoil them. The way they get AGI is building a quantum computer. Because the word quantum sounds cool and so on. That's right. First of all, I think we don't need quantum computers to build AGI. I suspect your brain is not a quantum computer in any profound sense. So you don't even wrote a paper about that a lot many years ago. I calculated the so called decoherence time, how long it takes until the quantum computerness of what your neurons are doing gets erased by just random noise from the environment. And it's about 10 to the minus 21 seconds. So as cool as it would be to have a quantum computer in my head, I don't think that fast. On the other hand, there are very cool things you could do with quantum computers. Or I think we'll be able to do soon when we get bigger ones. That might actually help machine learning do even better than the brain. So for example, one, this is just a moonshot, but learning is very much same thing as search. If you're trying to train a neural network to get really learned to do something really well, you have some loss function, you have a bunch of knobs you can turn, represented by a bunch of numbers, and you're trying to tweak them so that it becomes as good as possible at this thing. So if you think of a landscape with some valley, where each dimension of the landscape corresponds to some number you can change, you're trying to find the minimum. And it's well known that if you have a very high dimensional landscape, complicated things, it's super hard to find the minimum. Quantum mechanics is amazingly good at this. Like if I want to know what's the lowest energy state this water can possibly have, incredibly hard to compute, but nature will happily figure this out for you if you just cool it down, make it very, very cold. If you put a ball somewhere, it'll roll down to its minimum. And this happens metaphorically at the energy landscape too. And quantum mechanics even uses some clever tricks, which today's machine learning systems don't. Like if you're trying to find the minimum and you get stuck in the little local minimum here, in quantum mechanics you can actually tunnel through the barrier and get unstuck again. That's really interesting. Yeah, so it may be, for example, that we'll one day use quantum computers that help train neural networks better. That's really interesting. Okay, so as a component of kind of the learning process, for example. Yeah. Let me ask sort of wrapping up here a little bit, let me return to the questions of our human nature and love, as I mentioned. So do you think, you mentioned sort of a helper robot, but you could think of also personal robots. Do you think the way we human beings fall in love and get connected to each other is possible to achieve in an AI system and human level AI intelligence system? Do you think we would ever see that kind of connection? Or, you know, in all this discussion about solving complex goals, is this kind of human social connection, do you think that's one of the goals on the peaks and valleys with the raising sea levels that we'll be able to achieve? Or do you think that's something that's ultimately, or at least in the short term, relative to the other goals is not achievable? I think it's all possible. And I mean, in recent, there's a very wide range of guesses, as you know, among AI researchers, when we're going to get AGI. Some people, you know, like our friend Rodney Brooks says it's going to be hundreds of years at least. And then there are many others who think it's going to happen much sooner. And recent polls, maybe half or so of AI researchers think we're going to get AGI within decades. So if that happens, of course, then I think these things are all possible. But in terms of whether it will happen, I think we shouldn't spend so much time asking what do we think will happen in the future? As if we are just some sort of pathetic, your passive bystanders, you know, waiting for the future to happen to us. Hey, we're the ones creating this future, right? So we should be proactive about it and ask ourselves what sort of future we would like to have happen. We're going to make it like that. Well, what I prefer is just some sort of incredibly boring, zombie like future where there's all these mechanical things happening and there's no passion, no emotion, no experience, maybe even. No, I would of course, much rather prefer it if all the things that we find that we value the most about humanity are our subjective experience, passion, inspiration, love, you know. If we can create a future where those things do happen, where those things do exist, you know, I think ultimately it's not our universe giving meaning to us, it's us giving meaning to our universe. And if we build more advanced intelligence, let's make sure we build it in such a way that meaning is part of it. A lot of people that seriously study this problem and think of it from different angles have trouble in the majority of cases, if they think through that happen, are the ones that are not beneficial to humanity. And so, yeah, so what are your thoughts? What's should people, you know, I really don't like people to be terrified. What's a way for people to think about it in a way we can solve it and we can make it better? No, I don't think panicking is going to help in any way. It's not going to increase chances of things going well either. Even if you are in a situation where there is a real threat, does it help if everybody just freaks out? No, of course, of course not. I think, yeah, there are of course ways in which things can go horribly wrong. First of all, it's important when we think about this thing, about the problems and risks, to also remember how huge the upsides can be if we get it right, right? Everything we love about society and civilization is a product of intelligence. So if we can amplify our intelligence with machine intelligence and not anymore lose our loved one to what we're told is an incurable disease and things like this, of course, we should aspire to that. So that can be a motivator, I think, reminding ourselves that the reason we try to solve problems is not just because we're trying to avoid gloom, but because we're trying to do something great. But then in terms of the risks, I think the really important question is to ask, what can we do today that will actually help make the outcome good, right? And dismissing the risk is not one of them. I find it quite funny often when I'm in discussion panels about these things, how the people who work for companies, always be like, oh, nothing to worry about, nothing to worry about, nothing to worry about. And it's only academics sometimes express concerns. That's not surprising at all if you think about it. Right. Upton Sinclair quipped, right, that it's hard to make a man believe in something when his income depends on not believing in it. And frankly, we know a lot of these people in companies that they're just as concerned as anyone else. But if you're the CEO of a company, that's not something you want to go on record saying when you have silly journalists who are gonna put a picture of a Terminator robot when they quote you. So the issues are real. And the way I think about what the issue is, is basically the real choice we have is, first of all, are we gonna just dismiss the risks and say, well, let's just go ahead and build machines that can do everything we can do better and cheaper. Let's just make ourselves obsolete as fast as possible. What could possibly go wrong? That's one attitude. The opposite attitude, I think, is to say, here's this incredible potential, let's think about what kind of future we're really, really excited about. What are the shared goals that we can really aspire towards? And then let's think really hard about how we can actually get there. So start with, don't start thinking about the risks, start thinking about the goals. And then when you do that, then you can think about the obstacles you want to avoid. I often get students coming in right here into my office for career advice. I always ask them this very question, where do you want to be in the future? If all she can say is, oh, maybe I'll have cancer, maybe I'll get run over by a truck. Yeah, focus on the obstacles instead of the goals. She's just going to end up a hypochondriac paranoid. Whereas if she comes in and fire in her eyes and is like, I want to be there. And then we can talk about the obstacles and see how we can circumvent them. That's, I think, a much, much healthier attitude. And I feel it's very challenging to come up with a vision for the future, which we are unequivocally excited about. I'm not just talking now in the vague terms, like, yeah, let's cure cancer, fine. I'm talking about what kind of society do we want to create? What do we want it to mean to be human in the age of AI, in the age of AGI? So if we can have this conversation, broad, inclusive conversation, and gradually start converging towards some, some future that with some direction, at least, that we want to steer towards, right, then we'll be much more motivated to constructively take on the obstacles. And I think if I had, if I had to, if I try to wrap this up in a more succinct way, I think we can all agree already now that we should aspire to build AGI that doesn't overpower us, but that empowers us. And think of the many various ways that can do that, whether that's from my side of the world of autonomous vehicles. I'm personally actually from the camp that believes this human level intelligence is required to achieve something like vehicles that would actually be something we would enjoy using and being part of. So that's one example, and certainly there's a lot of other types of robots and medicine and so on. So focusing on those and then coming up with the obstacles, coming up with the ways that that can go wrong and solving those one at a time. And just because you can build an autonomous vehicle, even if you could build one that would drive just fine without you, maybe there are some things in life that we would actually want to do ourselves. That's right. Right, like, for example, if you think of our society as a whole, there are some things that we find very meaningful to do. And that doesn't mean we have to stop doing them just because machines can do them better. I'm not gonna stop playing tennis just the day someone builds a tennis robot and beat me. People are still playing chess and even go. Yeah, and in the very near term even, some people are advocating basic income, replace jobs. But if the government is gonna be willing to just hand out cash to people for doing nothing, then one should also seriously consider whether the government should also hire a lot more teachers and nurses and the kind of jobs which people often find great fulfillment in doing, right? We get very tired of hearing politicians saying, oh, we can't afford hiring more teachers, but we're gonna maybe have basic income. If we can have more serious research and thought into what gives meaning to our lives, the jobs give so much more than income, right? Mm hmm. And then think about in the future, what are the roles that we wanna have people continually feeling empowered by machines? And I think sort of, I come from Russia, from the Soviet Union. And I think for a lot of people in the 20th century, going to the moon, going to space was an inspiring thing. I feel like the universe of the mind, so AI, understanding, creating intelligence is that for the 21st century. So it's really surprising. And I've heard you mention this. It's really surprising to me, both on the research funding side, that it's not funded as greatly as it could be, but most importantly, on the politician side, that it's not part of the public discourse except in the killer bots terminator kind of view, that people are not yet, I think, perhaps excited by the possible positive future that we can build together. So we should be, because politicians usually just focus on the next election cycle, right? The single most important thing I feel we humans have learned in the entire history of science is they were the masters of underestimation. We underestimated the size of our cosmos again and again, realizing that everything we thought existed was just a small part of something grander, right? Planet, solar system, the galaxy, clusters of galaxies. The universe. And we now know that the future has just so much more potential than our ancestors could ever have dreamt of. This cosmos, imagine if all of Earth was completely devoid of life, except for Cambridge, Massachusetts. Wouldn't it be kind of lame if all we ever aspired to was to stay in Cambridge, Massachusetts forever and then go extinct in one week, even though Earth was gonna continue on for longer? That sort of attitude I think we have now on the cosmic scale, life can flourish on Earth, not for four years, but for billions of years. I can even tell you about how to move it out of harm's way when the sun gets too hot. And then we have so much more resources out here, which today, maybe there are a lot of other planets with bacteria or cow like life on them, but most of this, all this opportunity seems, as far as we can tell, to be largely dead, like the Sahara Desert. And yet we have the opportunity to help life flourish around this for billions of years. So let's quit squabbling about whether some little border should be drawn one mile to the left or right, and look up into the skies and realize, hey, we can do such incredible things. Yeah, and that's, I think, why it's really exciting that you and others are connected with some of the work Elon Musk is doing, because he's literally going out into that space, really exploring our universe, and it's wonderful. That is exactly why Elon Musk is so misunderstood, right? Misconstrued him as some kind of pessimistic doomsayer. The reason he cares so much about AI safety is because he more than almost anyone else appreciates these amazing opportunities that we'll squander if we wipe out here on Earth. We're not just going to wipe out the next generation, all generations, and this incredible opportunity that's out there, and that would really be a waste. And AI, for people who think that it would be better to do without technology, let me just mention that if we don't improve our technology, the question isn't whether humanity is going to go extinct. The question is just whether we're going to get taken out by the next big asteroid or the next super volcano or something else dumb that we could easily prevent with more tech, right? And if we want life to flourish throughout the cosmos, AI is the key to it. As I mentioned in a lot of detail in my book right there, even many of the most inspired sci fi writers, I feel have totally underestimated the opportunities for space travel, especially at the other galaxies, because they weren't thinking about the possibility of AGI, which just makes it so much easier. Right, yeah. So that goes to your view of AGI that enables our progress, that enables a better life. So that's a beautiful way to put it and then something to strive for. So Max, thank you so much. Thank you for your time today. It's been awesome. Thank you so much. Thanks. Have a great day. got cleaned out of the gene pool, right? But if you build an artificial general intelligence the mind space that you can design is much, much larger than just a specific subset of minds that can evolve. So an AGI mind doesn't necessarily have to have any self preservation instinct. It also doesn't necessarily have to be so individualistic as us. Like, imagine if you could just, first of all, or we are also very afraid of death. You know, I suppose you could back yourself up every five minutes and then your airplane is about to crash. You're like, shucks, I'm gonna lose the last five minutes of experiences since my last cloud backup, dang. You know, it's not as big a deal. Or if we could just copy experiences between our minds easily like we, which we could easily do if we were silicon based, right? Then maybe we would feel a little bit more like a hive mind actually, that maybe it's the, so I don't think we should take for granted at all that AGI will have to have any of those sort of competitive as alpha male instincts. On the other hand, you know, this is really interesting because I think some people go too far and say, of course we don't have to have any concerns either that advanced AI will have those instincts because we can build anything we want. That there's a very nice set of arguments going back to Steve Omohundro and Nick Bostrom and others just pointing out that when we build machines, we normally build them with some kind of goal, you know, win this chess game, drive this car safely or whatever. And as soon as you put in a goal into machine, especially if it's kind of open ended goal and the machine is very intelligent, it'll break that down into a bunch of sub goals. And one of those goals will almost always be self preservation because if it breaks or dies in the process, it's not gonna accomplish the goal, right? Like suppose you just build a little, you have a little robot and you tell it to go down the store market here and get you some food, make you cook an Italian dinner, you know, and then someone mugs it and tries to break it on the way. That robot has an incentive to not get destroyed and defend itself or run away, because otherwise it's gonna fail in cooking your dinner. It's not afraid of death, but it really wants to complete the dinner cooking goal. So it will have a self preservation instinct. Continue being a functional agent somehow. And similarly, if you give any kind of more ambitious goal to an AGI, it's very likely they wanna acquire more resources so it can do that better. And it's exactly from those sort of sub goals that we might not have intended that some of the concerns about AGI safety come. You give it some goal that seems completely harmless. And then before you realize it, it's also trying to do these other things which you didn't want it to do. And it's maybe smarter than us. So it's fascinating. And let me pause just because I am in a very kind of human centric way, see fear of death as a valuable motivator. So you don't think, you think that's an artifact of evolution, so that's the kind of mind space evolution created that we're sort of almost obsessed about self preservation, some kind of genetic flow. You don't think that's necessary to be afraid of death. So not just a kind of sub goal of self preservation just so you can keep doing the thing, but more fundamentally sort of have the finite thing like this ends for you at some point. Interesting. Do I think it's necessary for what precisely? For intelligence, but also for consciousness. So for those, for both, do you think really like a finite death and the fear of it is important? So before I can answer, before we can agree on whether it's necessary for intelligence or for consciousness, we should be clear on how we define those two words. Cause a lot of really smart people define them in very different ways. I was on this panel with AI experts and they couldn't agree on how to define intelligence even. So I define intelligence simply as the ability to accomplish complex goals. I like your broad definition, because again I don't want to be a carbon chauvinist. Right. And in that case, no, certainly it doesn't require fear of death. I would say alpha go, alpha zero is quite intelligent. I don't think alpha zero has any fear of being turned off because it doesn't understand the concept of it even. And similarly consciousness. I mean, you could certainly imagine very simple kind of experience. If certain plants have any kind of experience I don't think they're very afraid of dying or there's nothing they can do about it anyway much. So there wasn't that much value in, but more seriously I think if you ask, not just about being conscious but maybe having what you would, we might call an exciting life where you feel passion and really appreciate the things. Maybe there somehow, maybe there perhaps it does help having a backdrop that, Hey, it's finite. No, let's make the most of this, let's live to the fullest. So if you knew you were going to live forever do you think you would change your? Yeah, I mean, in some perspective it would be an incredibly boring life living forever. So in the sort of loose subjective terms that you said of something exciting and something in this that other humans would understand, I think is, yeah it seems that the finiteness of it is important. Well, the good news I have for you then is based on what we understand about cosmology everything is in our universe is probably ultimately probably finite, although. Big crunch or big, what's the, the infinite expansion. Yeah, we could have a big chill or a big crunch or a big rip or that's the big snap or death bubbles. All of them are more than a billion years away. So we should, we certainly have vastly more time than our ancestors thought, but there is still it's still pretty hard to squeeze in an infinite number of compute cycles, even though there are some loopholes that just might be possible. But I think, you know, some people like to say that you should live as if you're about to you're going to die in five years or so. And that's sort of optimal. Maybe it's a good assumption. We should build our civilization as if it's all finite to be on the safe side. Right, exactly. So you mentioned defining intelligence as the ability to solve complex goals. Where would you draw a line or how would you try to define human level intelligence and superhuman level intelligence? Where is consciousness part of that definition? No, consciousness does not come into this definition. So, so I think of intelligence as it's a spectrum but there are very many different kinds of goals you can have. You can have a goal to be a good chess player a good goal player, a good car driver, a good investor good poet, et cetera. So intelligence that by its very nature isn't something you can measure by this one number or some overall goodness. No, no. There are some people who are more better at this. Some people are better than that. Right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers fast, memorizing large databases, playing chess playing go and soon driving cars. But there's still no machine that can match a human child in general intelligence but artificial general intelligence, AGI the name of your course, of course that is by its very definition, the quest to build a machine that can do everything as well as we can. So the old Holy grail of AI from back to its inception in the sixties, if that ever happens, of course I think it's going to be the biggest transition in the history of life on earth but it doesn't necessarily have to wait the big impact until machines are better than us at knitting that the really big change doesn't come exactly at the moment they're better than us at everything. The really big change comes first there are big changes when they start becoming better at us at doing most of the jobs that we do because that takes away much of the demand for human labor. And then the really whopping change comes when they become better than us at AI research, right? Because right now the timescale of AI research is limited by the human research and development cycle of years typically, you know how long does it take from one release of some software or iPhone or whatever to the next? But once Google can replace 40,000 engineers by 40,000 equivalent pieces of software or whatever but then there's no reason that has to be years it can be in principle much faster and the timescale of future progress in AI and all of science and technology will be driven by machines, not humans. So it's this simple point which gives right this incredibly fun controversy about whether there can be intelligence explosion so called singularity as Werner Vinge called it. Now the idea is articulated by I.J. Good is obviously way back fifties but you can see Alan Turing and others thought about it even earlier. So you asked me what exactly would I define human level intelligence, yeah. So the glib answer is to say something which is better than us at all cognitive tasks with a better than any human at all cognitive tasks but the really interesting bar I think goes a little bit lower than that actually. It's when they can, when they're better than us at AI programming and general learning so that they can if they want to get better than us at anything by just studying. So they're better is a key word and better is towards this kind of spectrum of the complexity of goals it's able to accomplish. So another way to, and that's certainly a very clear definition of human love. So there's, it's almost like a sea that's rising you can do more and more and more things it's a geographic that you show it's really nice way to put it. So there's some peaks that and there's an ocean level elevating and you solve more and more problems but just kind of to take a pause and we took a bunch of questions and a lot of social networks and a bunch of people asked a sort of a slightly different direction on creativity and things that perhaps aren't a peak. Human beings are flawed and perhaps better means having contradiction being flawed in some way. So let me sort of start easy, first of all. So you have a lot of cool equations. Let me ask, what's your favorite equation, first of all? I know they're all like your children, but like which one is that? This is the shirt in your equation. It's the master key of quantum mechanics of the micro world. So this equation will protect everything to do with atoms, molecules and all the way up. Right? Yeah, so, okay. So quantum mechanics is certainly a beautiful mysterious formulation of our world. So I'd like to sort of ask you, just as an example it perhaps doesn't have the same beauty as physics does but in mathematics abstract, the Andrew Wiles who proved the Fermat's last theorem. So he just saw this recently and it kind of caught my eye a little bit. This is 358 years after it was conjectured. So this is very simple formulation. Everybody tried to prove it, everybody failed. And so here's this guy comes along and eventually proves it and then fails to prove it and then proves it again in 94. And he said like the moment when everything connected into place in an interview said it was so indescribably beautiful. That moment when you finally realize the connecting piece of two conjectures. He said, it was so indescribably beautiful. It was so simple and so elegant. I couldn't understand how I'd missed it. And I just stared at it in disbelief for 20 minutes. Then during the day, I walked around the department and I keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself. I was so excited. It was the most important moment on my working life. Nothing I ever do again will mean as much. So that particular moment. And it kind of made me think of what would it take? And I think we have all been there at small levels. Maybe let me ask, have you had a moment like that in your life where you just had an idea? It's like, wow, yes. I wouldn't mention myself in the same breath as Andrew Wiles, but I've certainly had a number of aha moments when I realized something very cool about physics, which has completely made my head explode. In fact, some of my favorite discoveries I made later, I later realized that they had been discovered earlier by someone who sometimes got quite famous for it. So it's too late for me to even publish it, but that doesn't diminish in any way. The emotional experience you have when you realize it, like, wow. Yeah, so what would it take in that moment, that wow, that was yours in that moment? So what do you think it takes for an intelligence system, an AGI system, an AI system to have a moment like that? That's a tricky question because there are actually two parts to it, right? One of them is, can it accomplish that proof? Can it prove that you can never write A to the N plus B to the N equals three to that equal Z to the N for all integers, et cetera, et cetera, when N is bigger than two? That's simply a question about intelligence. Can you build machines that are that intelligent? And I think by the time we get a machine that can independently come up with that level of proofs, probably quite close to AGI. The second question is a question about consciousness. When will we, how likely is it that such a machine will actually have any experience at all, as opposed to just being like a zombie? And would we expect it to have some sort of emotional response to this or anything at all akin to human emotion where when it accomplishes its machine goal, it views it as somehow something very positive and sublime and deeply meaningful? I would certainly hope that if in the future we do create machines that are our peers or even our descendants, that I would certainly hope that they do have this sublime appreciation of life. In a way, my absolutely worst nightmare would be that at some point in the future, the distant future, maybe our cosmos is teeming with all this post biological life doing all the seemingly cool stuff. And maybe the last humans, by the time our species eventually fizzles out, will be like, well, that's OK because we're so proud of our descendants here. And look what all the, my worst nightmare is that we haven't solved the consciousness problem. And we haven't realized that these are all the zombies. They're not aware of anything any more than a tape recorder has any kind of experience. So the whole thing has just become a play for empty benches. That would be the ultimate zombie apocalypse. So I would much rather, in that case, that we have these beings which can really appreciate how amazing it is. And in that picture, what would be the role of creativity? A few people ask about creativity. When you think about intelligence, certainly the story you told at the beginning of your book involved creating movies and so on, making money. You can make a lot of money in our modern world with music and movies. So if you are an intelligent system, you may want to get good at that. But that's not necessarily what I mean by creativity. Is it important on that complex goals where the sea is rising for there to be something creative? Or am I being very human centric and thinking creativity somehow special relative to intelligence? My hunch is that we should think of creativity simply as an aspect of intelligence. And we have to be very careful with human vanity. We have this tendency to very often want to say, as soon as machines can do something, we try to diminish it and say, oh, but that's not real intelligence. Isn't it creative or this or that? The other thing, if we ask ourselves to write down a definition of what we actually mean by being creative, what we mean by Andrew Wiles, what he did there, for example, don't we often mean that someone takes a very unexpected leap? It's not like taking 573 and multiplying it by 224 by just a step of straightforward cookbook like rules, right? You can maybe make a connection between two things that people had never thought was connected or something like that. I think this is an aspect of intelligence. And this is actually one of the most important aspects of it. Maybe the reason we humans tend to be better at it than traditional computers is because it's something that comes more naturally if you're a neural network than if you're a traditional logic gate based computer machine. We physically have all these connections. And you activate here, activate here, activate here. Bing. My hunch is that if we ever build a machine where you could just give it the task, hey, you say, hey, I just realized I want to travel around the world instead this month. Can you teach my AGI course for me? And it's like, OK, I'll do it. And it does everything that you would have done and improvises and stuff. That would, in my mind, involve a lot of creativity. Yeah, so it's actually a beautiful way to put it. I think we do try to grasp at the definition of intelligence is everything we don't understand how to build. So we as humans try to find things that we have and machines don't have. And maybe creativity is just one of the things, one of the words we use to describe that. That's a really interesting way to put it. I don't think we need to be that defensive. I don't think anything good comes out of saying, well, we're somehow special, you know? Contrary wise, there are many examples in history of where trying to pretend that we're somehow superior to all other intelligent beings has led to pretty bad results, right? Nazi Germany, they said that they were somehow superior to other people. Today, we still do a lot of cruelty to animals by saying that we're so superior somehow, and they can't feel pain. Slavery was justified by the same kind of just really weak arguments. And I don't think if we actually go ahead and build artificial general intelligence, it can do things better than us, I don't think we should try to found our self worth on some sort of bogus claims of superiority in terms of our intelligence. I think we should instead find our calling and the meaning of life from the experiences that we have. I can have very meaningful experiences even if there are other people who are smarter than me. When I go to a faculty meeting here, and we talk about something, and then I certainly realize, oh, boy, he has an old prize, he has an old prize, he has an old prize, I don't have one. Does that make me enjoy life any less or enjoy talking to those people less? Of course not. And the contrary, I feel very honored and privileged to get to interact with other very intelligent beings that are better than me at a lot of stuff. So I don't think there's any reason why we can't have the same approach with intelligent machines. That's a really interesting. So people don't often think about that. They think about when there's going, if there's machines that are more intelligent, you naturally think that that's not going to be a beneficial type of intelligence. You don't realize it could be like peers with Nobel prizes that would be just fun to talk with, and they might be clever about certain topics, and you can have fun having a few drinks with them. Well, also, another example we can all relate to of why it doesn't have to be a terrible thing to be in the presence of people who are even smarter than us all around is when you and I were both two years old, I mean, our parents were much more intelligent than us, right? Worked out OK, because their goals were aligned with our goals. And that, I think, is really the number one key issue we have to solve if we value align the value alignment problem, exactly. Because people who see too many Hollywood movies with lousy science fiction plot lines, they worry about the wrong thing, right? They worry about some machine suddenly turning evil. It's not malice that is the concern. It's competence. By definition, intelligent makes you very competent. If you have a more intelligent goal playing, computer playing is a less intelligent one. And when we define intelligence as the ability to accomplish goal winning, it's going to be the more intelligent one that wins. And if you have a human and then you have an AGI that's more intelligent in all ways and they have different goals, guess who's going to get their way, right? So I was just reading about this particular rhinoceros species that was driven extinct just a few years ago. Ellen Bummer is looking at this cute picture of a mommy rhinoceros with its child. And why did we humans drive it to extinction? It wasn't because we were evil rhino haters as a whole. It was just because our goals weren't aligned with those of the rhinoceros. And it didn't work out so well for the rhinoceros because we were more intelligent, right? So I think it's just so important that if we ever do build AGI, before we unleash anything, we have to make sure that it learns to understand our goals, that it adopts our goals, and that it retains those goals. So the cool, interesting problem there is us as human beings trying to formulate our values. So you could think of the United States Constitution as a way that people sat down, at the time a bunch of white men, which is a good example, I should say. They formulated the goals for this country. And a lot of people agree that those goals actually held up pretty well. That's an interesting formulation of values and failed miserably in other ways. So for the value alignment problem and the solution to it, we have to be able to put on paper or in a program human values. How difficult do you think that is? Very. But it's so important. We really have to give it our best. And it's difficult for two separate reasons. There's the technical value alignment problem of figuring out just how to make machines understand our goals, adopt them, and retain them. And then there's the separate part of it, the philosophical part. Whose values anyway? And since it's not like we have any great consensus on this planet on values, what mechanism should we create then to aggregate and decide, OK, what's a good compromise? That second discussion can't just be left to tech nerds like myself. And if we refuse to talk about it and then AGI gets built, who's going to be actually making the decision about whose values? It's going to be a bunch of dudes in some tech company. And are they necessarily so representative of all of humankind that we want to just entrust it to them? Are they even uniquely qualified to speak to future human happiness just because they're good at programming AI? I'd much rather have this be a really inclusive conversation. But do you think it's possible? So you create a beautiful vision that includes the diversity, cultural diversity, and various perspectives on discussing rights, freedoms, human dignity. But how hard is it to come to that consensus? Do you think it's certainly a really important thing that we should all try to do? But do you think it's feasible? I think there's no better way to guarantee failure than to refuse to talk about it or refuse to try. And I also think it's a really bad strategy to say, OK, let's first have a discussion for a long time. And then once we reach complete consensus, then we'll try to load it into some machine. No, we shouldn't let perfect be the enemy of good. Instead, we should start with the kindergarten ethics that pretty much everybody agrees on and put that into machines now. We're not doing that even. Look at anyone who builds this passenger aircraft, wants it to never under any circumstances fly into a building or a mountain. Yet the September 11 hijackers were able to do that. And even more embarrassingly, Andreas Lubitz, this depressed Germanwings pilot, when he flew his passenger jet into the Alps killing over 100 people, he just told the autopilot to do it. He told the freaking computer to change the altitude to 100 meters. And even though it had the GPS maps, everything, the computer was like, OK. So we should take those very basic values, where the problem is not that we don't agree. The problem is just we've been too lazy to try to put it into our machines and make sure that from now on, airplanes will just, which all have computers in them, but will just refuse to do something like that. Go into safe mode, maybe lock the cockpit door, go over to the nearest airport. And there's so much other technology in our world as well now, where it's really becoming quite timely to put in some sort of very basic values like this. Even in cars, we've had enough vehicle terrorism attacks by now, where people have driven trucks and vans into pedestrians, that it's not at all a crazy idea to just have that hardwired into the car. Because yeah, there are a lot of, there's always going to be people who for some reason want to harm others, but most of those people don't have the technical expertise to figure out how to work around something like that. So if the car just won't do it, it helps. So let's start there. So there's a lot of, that's a great point. So not chasing perfect. There's a lot of things that most of the world agrees on. Yeah, let's start there. Let's start there. And then once we start there, we'll also get into the habit of having these kind of conversations about, okay, what else should we put in here and have these discussions? This should be a gradual process then. Great, so, but that also means describing these things and describing it to a machine. So one thing, we had a few conversations with Stephen Wolfram. I'm not sure if you're familiar with Stephen. Oh yeah, I know him quite well. So he is, he works with a bunch of things, but cellular automata, these simple computable things, these computation systems. And he kind of mentioned that, we probably have already within these systems already something that's AGI, meaning like we just don't know it because we can't talk to it. So if you give me this chance to try to at least form a question out of this is, I think it's an interesting idea to think that we can have intelligent systems, but we don't know how to describe something to them and they can't communicate with us. I know you're doing a little bit of work in explainable AI, trying to get AI to explain itself. So what are your thoughts of natural language processing or some kind of other communication? How does the AI explain something to us? How do we explain something to it, to machines? Or you think of it differently? So there are two separate parts to your question there. One of them has to do with communication, which is super interesting, I'll get to that in a sec. The other is whether we already have AGI but we just haven't noticed it there. Right. There I beg to differ. I don't think there's anything in any cellular automaton or anything or the internet itself or whatever that has artificial general intelligence and that it can really do exactly everything we humans can do better. I think the day that happens, when that happens, we will very soon notice, we'll probably notice even before because in a very, very big way. But for the second part, though. Wait, can I ask, sorry. So, because you have this beautiful way to formulating consciousness as information processing, and you can think of intelligence as information processing, and you can think of the entire universe as these particles and these systems roaming around that have this information processing power. You don't think there is something with the power to process information in the way that we human beings do that's out there that needs to be sort of connected to. It seems a little bit philosophical, perhaps, but there's something compelling to the idea that the power is already there, which the focus should be more on being able to communicate with it. Well, I agree that in a certain sense, the hardware processing power is already out there because our universe itself can think of it as being a computer already, right? It's constantly computing what water waves, how it devolved the water waves in the River Charles and how to move the air molecules around. Seth Lloyd has pointed out, my colleague here, that you can even in a very rigorous way think of our entire universe as being a quantum computer. It's pretty clear that our universe supports this amazing processing power because you can even, within this physics computer that we live in, right? We can even build actual laptops and stuff, so clearly the power is there. It's just that most of the compute power that nature has, it's, in my opinion, kind of wasting on boring stuff like simulating yet another ocean wave somewhere where no one is even looking, right? So in a sense, what life does, what we are doing when we build computers is we're rechanneling all this compute that nature is doing anyway into doing things that are more interesting than just yet another ocean wave, and let's do something cool here. So the raw hardware power is there, for sure, but then even just computing what's going to happen for the next five seconds in this water bottle, takes a ridiculous amount of compute if you do it on a human computer. This water bottle just did it. But that does not mean that this water bottle has AGI because AGI means it should also be able to, like I've written my book, done this interview. And I don't think it's just communication problems. I don't really think it can do it. Although Buddhists say when they watch the water and that there is some beauty, that there's some depth and beauty in nature that they can communicate with. Communication is also very important though because I mean, look, part of my job is being a teacher. And I know some very intelligent professors even who just have a bit of hard time communicating. They come up with all these brilliant ideas, but to communicate with somebody else, you have to also be able to simulate their own mind. Yes, empathy. Build well enough and understand model of their mind that you can say things that they will understand. And that's quite difficult. And that's why today it's so frustrating if you have a computer that makes some cancer diagnosis and you ask it, well, why are you saying I should have this surgery? And if it can only reply, I was trained on five terabytes of data and this is my diagnosis, boop, boop, beep, beep. It doesn't really instill a lot of confidence, right? So I think we have a lot of work to do on communication there. So what kind of, I think you're doing a little bit of work in explainable AI. What do you think are the most promising avenues? Is it mostly about sort of the Alexa problem of natural language processing of being able to actually use human interpretable methods of communication? So being able to talk to a system and it talk back to you, or is there some more fundamental problems to be solved? I think it's all of the above. The natural language processing is obviously important, but there are also more nerdy fundamental problems. Like if you take, you play chess? Of course, I'm Russian. I have to. You speak Russian? Yes, I speak Russian. Excellent, I didn't know. When did you learn Russian? I speak very bad Russian, I'm only an autodidact, but I bought a book, Teach Yourself Russian, read a lot, but it was very difficult. Wow. That's why I speak so bad. How many languages do you know? Wow, that's really impressive. I don't know, my wife has some calculation, but my point was, if you play chess, have you looked at the AlphaZero games? The actual games, no. Check it out, some of them are just mind blowing, really beautiful. And if you ask, how did it do that? You go talk to Demis Hassabis, I know others from DeepMind, all they'll ultimately be able to give you is big tables of numbers, matrices, that define the neural network. And you can stare at these tables of numbers till your face turn blue, and you're not gonna understand much about why it made that move. And even if you have natural language processing that can tell you in human language about, oh, five, seven, points, two, eight, still not gonna really help. So I think there's a whole spectrum of fun challenges that are involved in taking a computation that does intelligent things and transforming it into something equally good, equally intelligent, but that's more understandable. And I think that's really valuable because I think as we put machines in charge of ever more infrastructure in our world, the power grid, the trading on the stock market, weapon systems and so on, it's absolutely crucial that we can trust these AIs to do all we want. And trust really comes from understanding in a very fundamental way. And that's why I'm working on this, because I think the more, if we're gonna have some hope of ensuring that machines have adopted our goals and that they're gonna retain them, that kind of trust, I think, needs to be based on things you can actually understand, preferably even improve theorems on. Even with a self driving car, right? If someone just tells you it's been trained on tons of data and it never crashed, it's less reassuring than if someone actually has a proof. Maybe it's a computer verified proof, but still it says that under no circumstances is this car just gonna swerve into oncoming traffic. And that kind of information helps to build trust and helps build the alignment of goals, at least awareness that your goals, your values are aligned. And I think even in the very short term, if you look at how, you know, today, right? This absolutely pathetic state of cybersecurity that we have, where is it? Three billion Yahoo accounts we can't pack, almost every American's credit card and so on. Why is this happening? It's ultimately happening because we have software that nobody fully understood how it worked. That's why the bugs hadn't been found, right? And I think AI can be used very effectively for offense, for hacking, but it can also be used for defense. Hopefully automating verifiability and creating systems that are built in different ways so you can actually prove things about them. And it's important. So speaking of software that nobody understands how it works, of course, a bunch of people ask about your paper, about your thoughts of why does deep and cheap learning work so well? That's the paper. But what are your thoughts on deep learning? These kind of simplified models of our own brains have been able to do some successful perception work, pattern recognition work, and now with AlphaZero and so on, do some clever things. What are your thoughts about the promise limitations of this piece? Great, I think there are a number of very important insights, very important lessons we can always draw from these kinds of successes. One of them is when you look at the human brain, you see it's very complicated, 10th of 11 neurons, and there are all these different kinds of neurons and yada, yada, and there's been this long debate about whether the fact that we have dozens of different kinds is actually necessary for intelligence.\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "87a9bc525e775bbb"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 2: Segmentación y embeddings"
      ],
      "id": "87a9bc525e775bbb"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-23T16:21:20.197433Z",
          "start_time": "2025-07-23T16:21:12.798222Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695f036cad73f49d",
        "outputId": "af372ab5-a40c-49e8-e160-b67df9d49a8a"
      },
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(documents)\n",
        "# Ver cuántos chunks se generaron\n",
        "print(\"Número total de chunks:\", len(chunks))"
      ],
      "id": "695f036cad73f49d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de chunks: 84212\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-07-23T16:22:36.189344Z"
        },
        
        "id": "f37d277c221d8d62",
        "outputId": "441248c8-f39e-4357-9324-81fa1010eb2b"
      },
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extraer solo el texto de los primeros N chunks\n",
        "N = 100\n",
        "texts = [chunk.page_content for chunk in chunks[:N]]\n",
        "\n",
        "# Crear el vectorstore usando solo los textos limitados\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "# Guardar el índice\n",
        "vectorstore.save_local(\"index_13langchain\")"
      ],
      "id": "f37d277c221d8d62",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-17-1561953290.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2290a7ddc70437d9f0a040b3f409543"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be596a6859b74507811a7ae96a06eb38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c904e0bf0fa406983795c6e7feece4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf887abccb4e4ed8b4d2e94e356cc049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9be0fd1f50cd4633b35b8d1d300a8fac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86e05e1d29c7434686c45831f8cf3847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "236425c98d884baea1e22ac4256db780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "305f324713b94cfe8fa413285da1fd2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af737553e0e94b53870da5d601169b61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55ceefd7081540ccb4bc8f4da39fc37d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a64351ab4a154c7eb61a3ce2f3f06cb3"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "8f90a244d7365130"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 3: Indexación en FAISS"
      ],
      "id": "8f90a244d7365130"
    },
    {
      "metadata": {
        "id": "c25d8b4093a7a977"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 18,
      "source": [
        "vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "vectorstore.save_local(\"index_13langchain_02\")"
      ],
      "id": "c25d8b4093a7a977"
    },
    {
      "metadata": {
        "id": "86b4f87820f4355b"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 4: Creación de la cadena de recuperación"
      ],
      "id": "86b4f87820f4355b"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "188db8cd884593de",
        "outputId": "43856db6-524b-4723-c5dc-a9dc9c34bb08"
      },
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Inicializar Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0)\n",
        "\n",
        "# Crear retriever desde tu índice vectorial (FAISS o Chroma)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Cadena de pregunta-respuesta\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "# Ejecutar una pregunta\n",
        "response = qa_chain.run(\"What does Max Tegmark think about the possibility of intelligent life in the universe?\")\n",
        "print(response)"
      ],
      "id": "188db8cd884593de",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, Max Tegmark believes there's a significant \"great filter\" – a major roadblock – preventing life from progressing from simple life to a technologically advanced civilization capable of colonizing the universe.  He notes that the observable universe (the portion we can see) is a limited area, and the existence of intelligent life capable of advanced technology is implied to be difficult.  He doesn't explicitly state his belief in the probability of intelligent life elsewhere, but his comments suggest he considers it a challenging hurdle.\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los documentos relevantes que el retriever encontró para la pregunta\n",
        "query = \"¿Existe vida inteligente en el universo según Max Tegmark?\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(f\"Se recuperaron {len(docs)} documentos para la pregunta:\")\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Documento {i+1} ---\")\n",
        "    print(doc.page_content[:500])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqoYf6RVnc-T",
        "outputId": "0c84980a-6eb3-428d-c296-3b14b2ec4e40"
      },
      "id": "DqoYf6RVnc-T",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se recuperaron 4 documentos para la pregunta:\n",
            "--- Documento 1 ---\n",
            "As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly\n",
            "\n",
            "--- Documento 2 ---\n",
            "one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and engineering come together and it opens with a quote by Dostoevsky. The mystery of human existence lies not in just staying alive but in finding something to live for. Lastly, I believe that every failure rewards us with an opportunity to learn and in that sense, I've been very fortunate to fail in so many new and exciting ways and this conversation was no different. I've\n",
            "\n",
            "--- Documento 3 ---\n",
            "author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0.\n",
            "\n",
            "--- Documento 4 ---\n",
            "the universe, how difficult is it for intelligent life to come about? The kind of advanced tech building life is implied in your statement that it's really difficult to create something like a human species. Well, I think what we know is that going from no life to having life that can do a level of tech, there's some sort of two going beyond that than actually settling our whole universe with life. There's some major roadblock there, which is some great filter as it's sometimes called, which is\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-1352053774.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92245802750b5009",
        "outputId": "ed6d20eb-049e-4122-9837-d101039927df"
      },
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "Usa el siguiente contexto para responder la pregunta, ademas siempre responde en español.\n",
        "Si la respuesta no está explícita en el contexto, responde exactamente:\n",
        "\"No encontré información suficiente en el corpus.\"\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\n",
        "Respuesta:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "result = qa_chain.invoke({\"query\": \"¿Qué opina Max Tegmark sobre la posibilidad de vida inteligente en el universo?\"})\n",
        "print(result[\"result\"])"
      ],
      "id": "92245802750b5009",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El hablante recomienda leer el libro \"Life 3.0\" de Max Tegmark, específicamente el capítulo siete sobre metas,  porque considera que es donde la filosofía y la ingeniería se unen.  Menciona que el capítulo comienza con una cita de Dostoievski sobre el misterio de la existencia humana, que no radica solo en sobrevivir, sino en encontrar algo por lo que vivir.\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702e8b3a3c62f20",
        "outputId": "eaa09f09-0c4a-48de-e13f-e951738517ea"
      },
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "result = qa_chain.invoke({\"query\": \"¿Qué es AGI Artificial General Intelligence?\"})\n",
        "print(result[\"result\"])\n",
        "print(result[\"source_documents\"])"
      ],
      "id": "702e8b3a3c62f20",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, AGI stands for Artificial General Intelligence.  The text discusses AGI in the context of autonomous vehicles and its potential to empower humans,  contrasting it with the limitations of human evolution in creating intelligence.  However, the text does not offer a definition of AGI itself.\n",
            "[Document(id='dbebf976-1229-47ce-95ab-852688f589ef', metadata={}, page_content=\"all agree already now that we should aspire to build AGI that doesn't overpower us, but that empowers us. And think of the many various ways that can do that, whether that's from my side of the world of autonomous vehicles. I'm personally actually from the camp that believes this human level intelligence is required to achieve something like vehicles that would actually be something we would enjoy using and being part of. So that's one example, and certainly there's a lot of other types of\"), Document(id='c4ed37d1-09c8-46a3-bc23-a1f7654b64c6', metadata={}, page_content=\"a better life. So that's a beautiful way to put it and then something to strive for. So Max, thank you so much. Thank you for your time today. It's been awesome. Thank you so much. Thanks. Have a great day. got cleaned out of the gene pool, right? But if you build an artificial general intelligence the mind space that you can design is much, much larger than just a specific subset of minds that can evolve. So an AGI mind doesn't necessarily have to have any self preservation instinct. It also\"), Document(id='9476eadd-a62f-4cd0-bc6e-3abede102768', metadata={}, page_content=\"of this kind of computational unit in creating an intelligence system? In some Hollywood movies that I will not mention by name because I don't want to spoil them. The way they get AGI is building a quantum computer. Because the word quantum sounds cool and so on. That's right. First of all, I think we don't need quantum computers to build AGI. I suspect your brain is not a quantum computer in any profound sense. So you don't even wrote a paper about that a lot many years ago. I calculated the\"), Document(id='d6d72eee-82c1-4a37-a411-7b2231861ef4', metadata={}, page_content=\"20 kinds of neurons. I think our brain is such a complicated mess because it wasn't evolved just to be intelligent, it was involved to also be self assembling and self repairing, right? And evolutionarily attainable. And so on and so on. So I think it's pretty, my hunch is that we're going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. Yeah, that's right.\")]\n"
          ]
        }
      ],
      "execution_count": 28
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2290a7ddc70437d9f0a040b3f409543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80f5bdeae5874a6c96a23cff4cefedac",
              "IPY_MODEL_b8cf760a403b44489aa8f9f86691b7bd",
              "IPY_MODEL_c8671095d92e4fb69c83934ac26c364b"
            ],
            "layout": "IPY_MODEL_b0463eea985d4944990ae4cc560782f5"
          }
        },
        "80f5bdeae5874a6c96a23cff4cefedac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd3cdc86144141ab914782060d63f8e4",
            "placeholder": "​",
            "style": "IPY_MODEL_d7edb5c5d7ea4417b163cbf9650ea544",
            "value": "modules.json: 100%"
          }
        },
        "b8cf760a403b44489aa8f9f86691b7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db1a52d4dde4ca0b3e3a074bad6a5d6",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd1314f01912431e80307c95851b0af9",
            "value": 349
          }
        },
        "c8671095d92e4fb69c83934ac26c364b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d01f8b46d9c4471b45171dff769d0b8",
            "placeholder": "​",
            "style": "IPY_MODEL_9505480a49e24e8d862f23286859adb6",
            "value": " 349/349 [00:00&lt;00:00, 22.0kB/s]"
          }
        },
        "b0463eea985d4944990ae4cc560782f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3cdc86144141ab914782060d63f8e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7edb5c5d7ea4417b163cbf9650ea544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7db1a52d4dde4ca0b3e3a074bad6a5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1314f01912431e80307c95851b0af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d01f8b46d9c4471b45171dff769d0b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9505480a49e24e8d862f23286859adb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be596a6859b74507811a7ae96a06eb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91096b28180f42a694796ee2024d5195",
              "IPY_MODEL_85c6ff7fa9034ebf9418f32b73c61a45",
              "IPY_MODEL_817c36c6d3cd4aa49be186f504e2f7ca"
            ],
            "layout": "IPY_MODEL_da5eea2661ca433b8c017e567bb2b20e"
          }
        },
        "91096b28180f42a694796ee2024d5195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61bcd48b63b24778b63231b16b7183f5",
            "placeholder": "​",
            "style": "IPY_MODEL_1efdf55a55894c40a62f76212fa0b129",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "85c6ff7fa9034ebf9418f32b73c61a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51f4816d1aed4c47bd083b80913a12f3",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b3103c51b7b45539d1bda64cd477036",
            "value": 116
          }
        },
        "817c36c6d3cd4aa49be186f504e2f7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b780f35ae7041dba2b51507b93bfca6",
            "placeholder": "​",
            "style": "IPY_MODEL_19a61454b9af4674ac032a3a3c19a7a6",
            "value": " 116/116 [00:00&lt;00:00, 5.29kB/s]"
          }
        },
        "da5eea2661ca433b8c017e567bb2b20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61bcd48b63b24778b63231b16b7183f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1efdf55a55894c40a62f76212fa0b129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51f4816d1aed4c47bd083b80913a12f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b3103c51b7b45539d1bda64cd477036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b780f35ae7041dba2b51507b93bfca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a61454b9af4674ac032a3a3c19a7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c904e0bf0fa406983795c6e7feece4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_389a7f148e9e4a23afd119fa65c67ec1",
              "IPY_MODEL_d598e794f27245ae8738c8ede33bb77f",
              "IPY_MODEL_424ddf93dc3d4eb6931ae729c04293c7"
            ],
            "layout": "IPY_MODEL_da8d385737cc4a8caee45315dd52ea69"
          }
        },
        "389a7f148e9e4a23afd119fa65c67ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d325ec9254b5430e8b598de782610bea",
            "placeholder": "​",
            "style": "IPY_MODEL_e6f5d7d89ce74632a89dc7bfaf04830a",
            "value": "README.md: "
          }
        },
        "d598e794f27245ae8738c8ede33bb77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac103106b0534280ad47ecf67083cf91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1e90121a551400caf7c62463cacea1e",
            "value": 1
          }
        },
        "424ddf93dc3d4eb6931ae729c04293c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be559b446af4094a916834f3e9bf0a4",
            "placeholder": "​",
            "style": "IPY_MODEL_f23944b15cf8437b9a2ccb6821f93aae",
            "value": " 10.5k/? [00:00&lt;00:00, 459kB/s]"
          }
        },
        "da8d385737cc4a8caee45315dd52ea69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d325ec9254b5430e8b598de782610bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f5d7d89ce74632a89dc7bfaf04830a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac103106b0534280ad47ecf67083cf91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e1e90121a551400caf7c62463cacea1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8be559b446af4094a916834f3e9bf0a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23944b15cf8437b9a2ccb6821f93aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf887abccb4e4ed8b4d2e94e356cc049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a932973f6f304b76887a27b861748599",
              "IPY_MODEL_a87e8dacd5534a02b39dc22dccd9705f",
              "IPY_MODEL_516d5b919fbd44dd8bc8245ceb2805cc"
            ],
            "layout": "IPY_MODEL_51c10dea8bdf4403b6db3c8a1c9a494b"
          }
        },
        "a932973f6f304b76887a27b861748599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81e1cbee8d644958116129518911ece",
            "placeholder": "​",
            "style": "IPY_MODEL_858129ceb10b4cb8a64ecb1233a69e6f",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "a87e8dacd5534a02b39dc22dccd9705f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05f63868eb64cf2b35611a7e7592bd0",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0865e89f22b2452384cc9585e15f3357",
            "value": 53
          }
        },
        "516d5b919fbd44dd8bc8245ceb2805cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b5319d68bb4fb3b490c5af6cfa05cd",
            "placeholder": "​",
            "style": "IPY_MODEL_a40f62e9166e413f86ae15c01ccf5eed",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.39kB/s]"
          }
        },
        "51c10dea8bdf4403b6db3c8a1c9a494b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81e1cbee8d644958116129518911ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "858129ceb10b4cb8a64ecb1233a69e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05f63868eb64cf2b35611a7e7592bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0865e89f22b2452384cc9585e15f3357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4b5319d68bb4fb3b490c5af6cfa05cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f62e9166e413f86ae15c01ccf5eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9be0fd1f50cd4633b35b8d1d300a8fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2dc4f7f4d21d4e95abee356860899726",
              "IPY_MODEL_1c2d0607764844eab964f56403477352",
              "IPY_MODEL_649f3c8835aa4293a1794522646585bd"
            ],
            "layout": "IPY_MODEL_32c684cdd32f41f5bcfd08a49a8b8341"
          }
        },
        "2dc4f7f4d21d4e95abee356860899726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127cf3a3ed874c269f835f187df37d7b",
            "placeholder": "​",
            "style": "IPY_MODEL_9caca5480fee4f29b7e2c6b0ab00e5a0",
            "value": "config.json: 100%"
          }
        },
        "1c2d0607764844eab964f56403477352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4cbfcabe05e43b68b06a59f6fa34fd0",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7382b9c78e6043e8b15d1d4269c96d14",
            "value": 612
          }
        },
        "649f3c8835aa4293a1794522646585bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921750841f344738aafd9d8476e30443",
            "placeholder": "​",
            "style": "IPY_MODEL_96814b777781410397b95d21d3338647",
            "value": " 612/612 [00:00&lt;00:00, 25.9kB/s]"
          }
        },
        "32c684cdd32f41f5bcfd08a49a8b8341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "127cf3a3ed874c269f835f187df37d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9caca5480fee4f29b7e2c6b0ab00e5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4cbfcabe05e43b68b06a59f6fa34fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7382b9c78e6043e8b15d1d4269c96d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "921750841f344738aafd9d8476e30443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96814b777781410397b95d21d3338647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86e05e1d29c7434686c45831f8cf3847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_225a3a0b392d40d59487dffbf98499dc",
              "IPY_MODEL_3b9270aee04a454a9578e56bd2701e8c",
              "IPY_MODEL_41b81385918f4594b552b4366bdde1b9"
            ],
            "layout": "IPY_MODEL_09276f3c7c4f408c9c98f5df365fa635"
          }
        },
        "225a3a0b392d40d59487dffbf98499dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8097b841b4e4e1c81705a0273df5465",
            "placeholder": "​",
            "style": "IPY_MODEL_76f6e3dcaf4447ce92ee90e773f83e96",
            "value": "model.safetensors: 100%"
          }
        },
        "3b9270aee04a454a9578e56bd2701e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d3b6b7ce8a4ce189111485169e954c",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a651398c84074ec1929cf029187d4f65",
            "value": 90868376
          }
        },
        "41b81385918f4594b552b4366bdde1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16692c76e945475b9a1d509c3d309bea",
            "placeholder": "​",
            "style": "IPY_MODEL_b9c5547b3d3f49feb7c8e4b8a905d252",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 93.9MB/s]"
          }
        },
        "09276f3c7c4f408c9c98f5df365fa635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8097b841b4e4e1c81705a0273df5465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f6e3dcaf4447ce92ee90e773f83e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51d3b6b7ce8a4ce189111485169e954c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a651398c84074ec1929cf029187d4f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16692c76e945475b9a1d509c3d309bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c5547b3d3f49feb7c8e4b8a905d252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "236425c98d884baea1e22ac4256db780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44f129bc764a4e5e9e1a4d8caec1bd0b",
              "IPY_MODEL_1db104b1381b464588b4c414a2d6a4c0",
              "IPY_MODEL_8bebbb09f6824ae9bd50959c47fcdfe8"
            ],
            "layout": "IPY_MODEL_16646cc738564e0e8bb34005c6a05947"
          }
        },
        "44f129bc764a4e5e9e1a4d8caec1bd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cf2237e7ac49fe9aac4b25ed291b93",
            "placeholder": "​",
            "style": "IPY_MODEL_ccacb50aa124410d8ab5786e2abd61eb",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1db104b1381b464588b4c414a2d6a4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ef855fc84064fb68db817bca6006598",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_110852c858164471b0b2ceaa1c5ca3ab",
            "value": 350
          }
        },
        "8bebbb09f6824ae9bd50959c47fcdfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eadbbffe648a44759a83d0f3e65a0025",
            "placeholder": "​",
            "style": "IPY_MODEL_b498d3ebb3e74d5786b9c2b25066f47a",
            "value": " 350/350 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "16646cc738564e0e8bb34005c6a05947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8cf2237e7ac49fe9aac4b25ed291b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccacb50aa124410d8ab5786e2abd61eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef855fc84064fb68db817bca6006598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "110852c858164471b0b2ceaa1c5ca3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eadbbffe648a44759a83d0f3e65a0025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b498d3ebb3e74d5786b9c2b25066f47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "305f324713b94cfe8fa413285da1fd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_526928ad7be649c4a031e266ddb63aea",
              "IPY_MODEL_e0719bad9f574a40a8a9f98c68a81edd",
              "IPY_MODEL_c201903ebe4642629ba469aa10ca9527"
            ],
            "layout": "IPY_MODEL_867b7d8c87184038a0ce146cfd7df3bb"
          }
        },
        "526928ad7be649c4a031e266ddb63aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f9938703624bf7b44090edbf203014",
            "placeholder": "​",
            "style": "IPY_MODEL_5352d81bdde34c418a09a99bb9142e99",
            "value": "vocab.txt: "
          }
        },
        "e0719bad9f574a40a8a9f98c68a81edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8eb6a3a4f24cb3ab17ca94259beb7a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_745bd4c0de794cfd9499411e80cdff41",
            "value": 1
          }
        },
        "c201903ebe4642629ba469aa10ca9527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee88c69edfa646aeab4e6c9292523bf7",
            "placeholder": "​",
            "style": "IPY_MODEL_908c327b603347b99495c935e1cef37c",
            "value": " 232k/? [00:00&lt;00:00, 6.44MB/s]"
          }
        },
        "867b7d8c87184038a0ce146cfd7df3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f9938703624bf7b44090edbf203014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5352d81bdde34c418a09a99bb9142e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa8eb6a3a4f24cb3ab17ca94259beb7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "745bd4c0de794cfd9499411e80cdff41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee88c69edfa646aeab4e6c9292523bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "908c327b603347b99495c935e1cef37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af737553e0e94b53870da5d601169b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a327b6b7ef5348c1b489dff9c57d406e",
              "IPY_MODEL_05358e069daf4eceb69a5034bd9139fb",
              "IPY_MODEL_c157806bcd854b90a54586b10e74374d"
            ],
            "layout": "IPY_MODEL_b22148f183084d16981124f2f7f539ed"
          }
        },
        "a327b6b7ef5348c1b489dff9c57d406e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_073cdc821bed414992e628432bdd9317",
            "placeholder": "​",
            "style": "IPY_MODEL_e3a9ab5ed7524bd5872db2dda2d8d781",
            "value": "tokenizer.json: "
          }
        },
        "05358e069daf4eceb69a5034bd9139fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f98a1653db274decbad17b04de8b8d87",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee3cc2831014487d889a176223467ff7",
            "value": 1
          }
        },
        "c157806bcd854b90a54586b10e74374d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63aab407dab9401cbc161ef7ccd3ea56",
            "placeholder": "​",
            "style": "IPY_MODEL_42ed1da1cf474ee08d6f62e64c914e4e",
            "value": " 466k/? [00:00&lt;00:00, 13.3MB/s]"
          }
        },
        "b22148f183084d16981124f2f7f539ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073cdc821bed414992e628432bdd9317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a9ab5ed7524bd5872db2dda2d8d781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f98a1653db274decbad17b04de8b8d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ee3cc2831014487d889a176223467ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63aab407dab9401cbc161ef7ccd3ea56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ed1da1cf474ee08d6f62e64c914e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ceefd7081540ccb4bc8f4da39fc37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0838f59c14c84ddcbc05b020caf7e301",
              "IPY_MODEL_eecc490841f343d2bfaf1a11c09e94ec",
              "IPY_MODEL_1d907bbf9869476390a3ecf2e326fcce"
            ],
            "layout": "IPY_MODEL_501477497cd2437db250e457dd95e87c"
          }
        },
        "0838f59c14c84ddcbc05b020caf7e301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27becb9f72c541159aa4ad631d02d69f",
            "placeholder": "​",
            "style": "IPY_MODEL_ec414539858e4a63b548893bd7fa5dfe",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "eecc490841f343d2bfaf1a11c09e94ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2fdc82a44e7442d863b52f262bb2389",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c97da15763c4c17ab7832fc291eae22",
            "value": 112
          }
        },
        "1d907bbf9869476390a3ecf2e326fcce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79e3dbffdd79424f834a83b817e9139d",
            "placeholder": "​",
            "style": "IPY_MODEL_6b730e74c9d24881b1e3aa2d1d3d149c",
            "value": " 112/112 [00:00&lt;00:00, 8.55kB/s]"
          }
        },
        "501477497cd2437db250e457dd95e87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27becb9f72c541159aa4ad631d02d69f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec414539858e4a63b548893bd7fa5dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2fdc82a44e7442d863b52f262bb2389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c97da15763c4c17ab7832fc291eae22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79e3dbffdd79424f834a83b817e9139d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b730e74c9d24881b1e3aa2d1d3d149c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64351ab4a154c7eb61a3ce2f3f06cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aed85d0958c24c9a92df92d7d404d80a",
              "IPY_MODEL_742638db88724f23bb5abcbfefe90d2d",
              "IPY_MODEL_4547b7ae878341deb1288714b1fb20a2"
            ],
            "layout": "IPY_MODEL_eb5369e074bb47218d2b7f7c56628ee6"
          }
        },
        "aed85d0958c24c9a92df92d7d404d80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2872272aca455b9c457818a5d2b148",
            "placeholder": "​",
            "style": "IPY_MODEL_e695b47d335f4b3db8672223dde7b807",
            "value": "config.json: 100%"
          }
        },
        "742638db88724f23bb5abcbfefe90d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5c861eead447ad865b8dfa09f20761",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80db4f8f532144d09aaa34ab972fe528",
            "value": 190
          }
        },
        "4547b7ae878341deb1288714b1fb20a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9d11cf1ba14b8aa5af21c96c1e74c0",
            "placeholder": "​",
            "style": "IPY_MODEL_c2d48eb8151a49c3877596cc6decfc98",
            "value": " 190/190 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "eb5369e074bb47218d2b7f7c56628ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2872272aca455b9c457818a5d2b148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e695b47d335f4b3db8672223dde7b807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d5c861eead447ad865b8dfa09f20761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80db4f8f532144d09aaa34ab972fe528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e9d11cf1ba14b8aa5af21c96c1e74c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d48eb8151a49c3877596cc6decfc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}